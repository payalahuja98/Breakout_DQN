{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "MP5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zxAxDhW2OsR"
      },
      "source": [
        "# Deep Q-Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRToCjBW2OsS"
      },
      "source": [
        "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViIj8Nlo2OsS",
        "outputId": "ab4160bc-aa10-493a-f47c-5938edbee4e4"
      },
      "source": [
        "!pip3 install gym pyvirtualdisplay\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Fetched 252 kB in 1s (224 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-5RO7UD2OsS",
        "outputId": "9b4f5ed9-3555-40e0-bd0c-ab2213de48b3"
      },
      "source": [
        "!pip3 install --upgrade setuptools\n",
        "!pip3 install ez_setup \n",
        "!pip3 install gym[atari] "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (51.0.0)\n",
            "Requirement already satisfied: ez_setup in /usr/local/lib/python3.6/dist-packages (0.9)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.18.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmk1bjPS2OsS"
      },
      "source": [
        "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DK6GLvh2OsS"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "import gym\n",
        "import torch\n",
        "import pylab\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from copy import deepcopy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
        "from model import DQN\n",
        "from config import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q39zvwKh2OsS"
      },
      "source": [
        "## Understanding the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1NL9Ykx2OsS"
      },
      "source": [
        "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
        "\n",
        "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwck-dp82OsS"
      },
      "source": [
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "state = env.reset()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wiHY5ZF2OsS"
      },
      "source": [
        "number_lives = find_max_lives(env)\n",
        "state_size = env.observation_space.shape\n",
        "action_size = 3 #fire, left, and right"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX9dHkWk2OsS"
      },
      "source": [
        "## Creating a DQN Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRX7F26O2OsS"
      },
      "source": [
        "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
        "\n",
        "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
        "\n",
        "__Frame__ : Number of frames processed in total.\n",
        "\n",
        "__Memory Size__ : The current size of the replay memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHR49W102OsS"
      },
      "source": [
        "double_dqn = False # set to True if using double DQN agent\n",
        "\n",
        "if double_dqn:\n",
        "    from agent_double import Agent\n",
        "else:\n",
        "    from agent import Agent\n",
        "\n",
        "agent = Agent(action_size)\n",
        "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
        "frame = 0\n",
        "memory_size = 0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DeEFu7Z2OsT"
      },
      "source": [
        "### Main Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOthVzF92OsT"
      },
      "source": [
        "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7_5zXMZ2OsT",
        "outputId": "7fb61437-4fa9-424b-859a-7677b7255171"
      },
      "source": [
        "rewards, episodes = [], []\n",
        "best_eval_reward = 0\n",
        "for e in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "    step = 0\n",
        "    d = False\n",
        "    state = env.reset()\n",
        "    next_state = state\n",
        "    life = number_lives\n",
        "\n",
        "    get_init_state(history, state)\n",
        "\n",
        "    while not done:\n",
        "        step += 1\n",
        "        frame += 1\n",
        "\n",
        "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
        "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "            action = 0\n",
        "        else:\n",
        "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "        state = next_state\n",
        "        next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "        frame_next_state = get_frame(next_state)\n",
        "        history[4, :, :] = frame_next_state\n",
        "        terminal_state = check_live(life, info['ale.lives'])\n",
        "\n",
        "        life = info['ale.lives']\n",
        "        r = np.clip(reward, -1, 1) \n",
        "        r = reward\n",
        "\n",
        "        # Store the transition in memory \n",
        "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "        # Start training after random sample generation\n",
        "        if(frame >= train_frame):\n",
        "            agent.train_policy_net(frame)\n",
        "            # Update the target network only for Double DQN only\n",
        "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
        "                agent.update_target_net()\n",
        "        score += reward\n",
        "        history[:4, :, :] = history[1:, :, :]\n",
        "            \n",
        "        if done:\n",
        "            evaluation_reward.append(score)\n",
        "            rewards.append(np.mean(evaluation_reward))\n",
        "            episodes.append(e)\n",
        "            pylab.plot(episodes, rewards, 'b')\n",
        "            pylab.xlabel('Episodes')\n",
        "            pylab.ylabel('Rewards') \n",
        "            pylab.title('Episodes vs Reward')\n",
        "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
        "            \n",
        "            # every episode, plot the play time\n",
        "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
        "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
        "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
        "\n",
        "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
        "            ### Change this save condition to whatever you prefer ###\n",
        "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
        "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
        "                best_eval_reward = np.mean(evaluation_reward)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0   score: 2.0   memory length: 200   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 1   score: 0.0   memory length: 322   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 2   score: 0.0   memory length: 444   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 0.6666666666666666\n",
            "episode: 3   score: 0.0   memory length: 567   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.5\n",
            "episode: 4   score: 2.0   memory length: 764   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 0.8\n",
            "episode: 5   score: 2.0   memory length: 945   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 6   score: 0.0   memory length: 1068   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.8571428571428571\n",
            "episode: 7   score: 4.0   memory length: 1380   epsilon: 1.0    steps: 312    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 8   score: 0.0   memory length: 1503   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1111111111111112\n",
            "episode: 9   score: 3.0   memory length: 1772   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 10   score: 0.0   memory length: 1895   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1818181818181819\n",
            "episode: 11   score: 1.0   memory length: 2065   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.1666666666666667\n",
            "episode: 12   score: 1.0   memory length: 2234   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.1538461538461537\n",
            "episode: 13   score: 0.0   memory length: 2356   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.0714285714285714\n",
            "episode: 14   score: 2.0   memory length: 2554   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.1333333333333333\n",
            "episode: 15   score: 1.0   memory length: 2707   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.125\n",
            "episode: 16   score: 2.0   memory length: 2905   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.1764705882352942\n",
            "episode: 17   score: 0.0   memory length: 3028   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1111111111111112\n",
            "episode: 18   score: 0.0   memory length: 3151   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.0526315789473684\n",
            "episode: 19   score: 1.0   memory length: 3320   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.05\n",
            "episode: 20   score: 5.0   memory length: 3650   epsilon: 1.0    steps: 330    lr: 0.0001     evaluation reward: 1.2380952380952381\n",
            "episode: 21   score: 0.0   memory length: 3773   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1818181818181819\n",
            "episode: 22   score: 2.0   memory length: 3972   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.2173913043478262\n",
            "episode: 23   score: 5.0   memory length: 4284   epsilon: 1.0    steps: 312    lr: 0.0001     evaluation reward: 1.375\n",
            "episode: 24   score: 1.0   memory length: 4452   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 25   score: 0.0   memory length: 4575   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3076923076923077\n",
            "episode: 26   score: 3.0   memory length: 4802   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.3703703703703705\n",
            "episode: 27   score: 1.0   memory length: 4971   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3571428571428572\n",
            "episode: 28   score: 0.0   memory length: 5094   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3103448275862069\n",
            "episode: 29   score: 1.0   memory length: 5262   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 30   score: 0.0   memory length: 5385   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2580645161290323\n",
            "episode: 31   score: 2.0   memory length: 5583   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.28125\n",
            "episode: 32   score: 0.0   memory length: 5705   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2424242424242424\n",
            "episode: 33   score: 0.0   memory length: 5827   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2058823529411764\n",
            "episode: 34   score: 0.0   memory length: 5950   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1714285714285715\n",
            "episode: 35   score: 3.0   memory length: 6196   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.2222222222222223\n",
            "episode: 36   score: 2.0   memory length: 6395   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.2432432432432432\n",
            "episode: 37   score: 3.0   memory length: 6640   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.2894736842105263\n",
            "episode: 38   score: 0.0   memory length: 6763   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2564102564102564\n",
            "episode: 39   score: 0.0   memory length: 6885   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.225\n",
            "episode: 40   score: 2.0   memory length: 7083   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.2439024390243902\n",
            "episode: 41   score: 1.0   memory length: 7233   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.2380952380952381\n",
            "episode: 42   score: 5.0   memory length: 7557   epsilon: 1.0    steps: 324    lr: 0.0001     evaluation reward: 1.3255813953488371\n",
            "episode: 43   score: 2.0   memory length: 7757   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.3409090909090908\n",
            "episode: 44   score: 1.0   memory length: 7927   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
            "episode: 45   score: 1.0   memory length: 8096   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.326086956521739\n",
            "episode: 46   score: 1.0   memory length: 8247   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3191489361702127\n",
            "episode: 47   score: 0.0   memory length: 8370   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2916666666666667\n",
            "episode: 48   score: 1.0   memory length: 8539   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.2857142857142858\n",
            "episode: 49   score: 1.0   memory length: 8709   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 50   score: 2.0   memory length: 8908   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.2941176470588236\n",
            "episode: 51   score: 3.0   memory length: 9134   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.3269230769230769\n",
            "episode: 52   score: 0.0   memory length: 9257   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3018867924528301\n",
            "episode: 53   score: 2.0   memory length: 9455   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3148148148148149\n",
            "episode: 54   score: 8.0   memory length: 9949   epsilon: 1.0    steps: 494    lr: 0.0001     evaluation reward: 1.4363636363636363\n",
            "episode: 55   score: 1.0   memory length: 10100   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
            "episode: 56   score: 0.0   memory length: 10223   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4035087719298245\n",
            "episode: 57   score: 1.0   memory length: 10374   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.396551724137931\n",
            "episode: 58   score: 0.0   memory length: 10496   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3728813559322033\n",
            "episode: 59   score: 2.0   memory length: 10696   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.3833333333333333\n",
            "episode: 60   score: 2.0   memory length: 10893   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.3934426229508197\n",
            "episode: 61   score: 2.0   memory length: 11090   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.403225806451613\n",
            "episode: 62   score: 0.0   memory length: 11213   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.380952380952381\n",
            "episode: 63   score: 0.0   memory length: 11335   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.359375\n",
            "episode: 64   score: 0.0   memory length: 11457   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3384615384615384\n",
            "episode: 65   score: 1.0   memory length: 11627   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
            "episode: 66   score: 3.0   memory length: 11874   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.3582089552238805\n",
            "episode: 67   score: 1.0   memory length: 12045   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.3529411764705883\n",
            "episode: 68   score: 1.0   memory length: 12214   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3478260869565217\n",
            "episode: 69   score: 0.0   memory length: 12337   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3285714285714285\n",
            "episode: 70   score: 2.0   memory length: 12535   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3380281690140845\n",
            "episode: 71   score: 1.0   memory length: 12705   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
            "episode: 72   score: 1.0   memory length: 12856   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3287671232876712\n",
            "episode: 73   score: 2.0   memory length: 13074   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.337837837837838\n",
            "episode: 74   score: 0.0   memory length: 13196   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 75   score: 0.0   memory length: 13319   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3026315789473684\n",
            "episode: 76   score: 1.0   memory length: 13470   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.2987012987012987\n",
            "episode: 77   score: 2.0   memory length: 13688   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.3076923076923077\n",
            "episode: 78   score: 1.0   memory length: 13838   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.3037974683544304\n",
            "episode: 79   score: 3.0   memory length: 14082   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.325\n",
            "episode: 80   score: 1.0   memory length: 14251   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3209876543209877\n",
            "episode: 81   score: 0.0   memory length: 14373   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3048780487804879\n",
            "episode: 82   score: 4.0   memory length: 14670   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.3373493975903614\n",
            "episode: 83   score: 5.0   memory length: 14976   epsilon: 1.0    steps: 306    lr: 0.0001     evaluation reward: 1.380952380952381\n",
            "episode: 84   score: 1.0   memory length: 15145   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3764705882352941\n",
            "episode: 85   score: 0.0   memory length: 15268   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3604651162790697\n",
            "episode: 86   score: 2.0   memory length: 15466   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.367816091954023\n",
            "episode: 87   score: 0.0   memory length: 15589   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3522727272727273\n",
            "episode: 88   score: 1.0   memory length: 15758   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.348314606741573\n",
            "episode: 89   score: 2.0   memory length: 15956   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3555555555555556\n",
            "episode: 90   score: 2.0   memory length: 16138   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.3626373626373627\n",
            "episode: 91   score: 1.0   memory length: 16289   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.358695652173913\n",
            "episode: 92   score: 0.0   memory length: 16411   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3440860215053763\n",
            "episode: 93   score: 2.0   memory length: 16627   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.351063829787234\n",
            "episode: 94   score: 0.0   memory length: 16749   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3368421052631578\n",
            "episode: 95   score: 0.0   memory length: 16872   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3229166666666667\n",
            "episode: 96   score: 0.0   memory length: 16995   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.309278350515464\n",
            "episode: 97   score: 0.0   memory length: 17118   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2959183673469388\n",
            "episode: 98   score: 2.0   memory length: 17316   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.303030303030303\n",
            "episode: 99   score: 3.0   memory length: 17543   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 100   score: 1.0   memory length: 17712   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 101   score: 2.0   memory length: 17910   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 102   score: 1.0   memory length: 18078   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 103   score: 1.0   memory length: 18249   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 104   score: 2.0   memory length: 18430   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 105   score: 0.0   memory length: 18552   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 106   score: 4.0   memory length: 18829   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 107   score: 2.0   memory length: 19050   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 108   score: 1.0   memory length: 19220   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 109   score: 1.0   memory length: 19371   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 110   score: 1.0   memory length: 19522   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 111   score: 3.0   memory length: 19788   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 112   score: 2.0   memory length: 19991   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 113   score: 3.0   memory length: 20260   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 114   score: 3.0   memory length: 20487   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 115   score: 1.0   memory length: 20637   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 116   score: 4.0   memory length: 20917   epsilon: 1.0    steps: 280    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 117   score: 1.0   memory length: 21089   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 118   score: 1.0   memory length: 21261   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 119   score: 2.0   memory length: 21459   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 120   score: 1.0   memory length: 21610   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 121   score: 0.0   memory length: 21733   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 122   score: 1.0   memory length: 21884   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 123   score: 1.0   memory length: 22035   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 124   score: 3.0   memory length: 22281   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 125   score: 0.0   memory length: 22404   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 126   score: 1.0   memory length: 22574   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 127   score: 2.0   memory length: 22791   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 128   score: 2.0   memory length: 23009   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 129   score: 0.0   memory length: 23131   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 130   score: 0.0   memory length: 23253   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 131   score: 1.0   memory length: 23422   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 132   score: 1.0   memory length: 23593   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 133   score: 1.0   memory length: 23744   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 134   score: 0.0   memory length: 23867   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 135   score: 4.0   memory length: 24147   epsilon: 1.0    steps: 280    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 136   score: 0.0   memory length: 24270   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 137   score: 1.0   memory length: 24421   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 138   score: 2.0   memory length: 24619   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 139   score: 0.0   memory length: 24742   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 140   score: 2.0   memory length: 24960   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 141   score: 3.0   memory length: 25190   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 142   score: 0.0   memory length: 25313   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 143   score: 0.0   memory length: 25435   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 144   score: 3.0   memory length: 25702   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 145   score: 2.0   memory length: 25900   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 146   score: 3.0   memory length: 26113   epsilon: 1.0    steps: 213    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 147   score: 0.0   memory length: 26236   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 148   score: 0.0   memory length: 26359   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 149   score: 0.0   memory length: 26482   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 150   score: 3.0   memory length: 26729   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 151   score: 3.0   memory length: 26976   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 152   score: 1.0   memory length: 27147   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 153   score: 3.0   memory length: 27412   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 154   score: 1.0   memory length: 27563   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 155   score: 2.0   memory length: 27778   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 156   score: 1.0   memory length: 27950   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 157   score: 2.0   memory length: 28169   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 158   score: 1.0   memory length: 28321   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 159   score: 0.0   memory length: 28443   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 160   score: 2.0   memory length: 28644   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 161   score: 0.0   memory length: 28767   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 162   score: 6.0   memory length: 29161   epsilon: 1.0    steps: 394    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 163   score: 0.0   memory length: 29284   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 164   score: 2.0   memory length: 29501   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 165   score: 1.0   memory length: 29669   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 166   score: 1.0   memory length: 29839   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 167   score: 0.0   memory length: 29962   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 168   score: 1.0   memory length: 30113   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 169   score: 3.0   memory length: 30361   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 170   score: 0.0   memory length: 30484   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 171   score: 2.0   memory length: 30699   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 172   score: 3.0   memory length: 30967   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 173   score: 1.0   memory length: 31138   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 174   score: 1.0   memory length: 31307   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 175   score: 0.0   memory length: 31430   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 176   score: 0.0   memory length: 31552   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 177   score: 4.0   memory length: 31829   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 178   score: 1.0   memory length: 31998   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 179   score: 1.0   memory length: 32167   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 180   score: 2.0   memory length: 32384   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 181   score: 3.0   memory length: 32651   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 182   score: 1.0   memory length: 32822   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 183   score: 1.0   memory length: 32991   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 184   score: 0.0   memory length: 33114   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 185   score: 0.0   memory length: 33236   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 186   score: 0.0   memory length: 33359   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 187   score: 1.0   memory length: 33510   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 188   score: 0.0   memory length: 33632   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 189   score: 2.0   memory length: 33835   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 190   score: 4.0   memory length: 34132   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 191   score: 0.0   memory length: 34254   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 192   score: 1.0   memory length: 34424   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 193   score: 1.0   memory length: 34594   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 194   score: 1.0   memory length: 34765   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 195   score: 2.0   memory length: 34982   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 196   score: 2.0   memory length: 35162   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 197   score: 2.0   memory length: 35382   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 198   score: 0.0   memory length: 35505   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 199   score: 4.0   memory length: 35779   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 200   score: 1.0   memory length: 35930   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 201   score: 2.0   memory length: 36145   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 202   score: 1.0   memory length: 36314   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 203   score: 1.0   memory length: 36484   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 204   score: 3.0   memory length: 36710   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 205   score: 2.0   memory length: 36892   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 206   score: 4.0   memory length: 37187   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 207   score: 2.0   memory length: 37384   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 208   score: 2.0   memory length: 37604   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 209   score: 3.0   memory length: 37832   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 210   score: 4.0   memory length: 38123   epsilon: 1.0    steps: 291    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 211   score: 4.0   memory length: 38440   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 212   score: 1.0   memory length: 38609   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 213   score: 4.0   memory length: 38905   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 214   score: 1.0   memory length: 39076   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 215   score: 2.0   memory length: 39296   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 216   score: 5.0   memory length: 39611   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 217   score: 2.0   memory length: 39808   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 218   score: 0.0   memory length: 39931   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 219   score: 0.0   memory length: 40054   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 220   score: 0.0   memory length: 40177   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 221   score: 0.0   memory length: 40300   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 222   score: 3.0   memory length: 40543   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 223   score: 2.0   memory length: 40762   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 224   score: 0.0   memory length: 40885   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 225   score: 5.0   memory length: 41178   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 226   score: 2.0   memory length: 41393   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 227   score: 2.0   memory length: 41610   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 228   score: 0.0   memory length: 41733   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 229   score: 2.0   memory length: 41951   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 230   score: 0.0   memory length: 42074   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 231   score: 0.0   memory length: 42197   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 232   score: 0.0   memory length: 42320   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 233   score: 1.0   memory length: 42488   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 234   score: 3.0   memory length: 42736   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 235   score: 0.0   memory length: 42859   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 236   score: 1.0   memory length: 43031   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 237   score: 1.0   memory length: 43200   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 238   score: 0.0   memory length: 43322   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 239   score: 2.0   memory length: 43503   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 240   score: 2.0   memory length: 43700   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 241   score: 0.0   memory length: 43823   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 242   score: 3.0   memory length: 44053   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 243   score: 2.0   memory length: 44271   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 244   score: 0.0   memory length: 44394   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 245   score: 2.0   memory length: 44574   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 246   score: 3.0   memory length: 44841   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 247   score: 0.0   memory length: 44964   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 248   score: 1.0   memory length: 45133   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 249   score: 3.0   memory length: 45348   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 250   score: 4.0   memory length: 45662   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 251   score: 3.0   memory length: 45911   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 252   score: 1.0   memory length: 46083   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 253   score: 0.0   memory length: 46206   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 254   score: 2.0   memory length: 46404   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 255   score: 0.0   memory length: 46526   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 256   score: 0.0   memory length: 46649   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 257   score: 1.0   memory length: 46821   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 258   score: 4.0   memory length: 47077   epsilon: 1.0    steps: 256    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 259   score: 2.0   memory length: 47292   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 260   score: 2.0   memory length: 47489   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 261   score: 3.0   memory length: 47753   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 262   score: 0.0   memory length: 47875   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 263   score: 2.0   memory length: 48073   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 264   score: 0.0   memory length: 48196   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 265   score: 0.0   memory length: 48319   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 266   score: 1.0   memory length: 48488   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 267   score: 3.0   memory length: 48735   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 268   score: 1.0   memory length: 48886   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 269   score: 2.0   memory length: 49083   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 270   score: 1.0   memory length: 49234   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 271   score: 1.0   memory length: 49404   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 272   score: 0.0   memory length: 49527   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 273   score: 3.0   memory length: 49774   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 274   score: 3.0   memory length: 50039   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 275   score: 1.0   memory length: 50208   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 276   score: 3.0   memory length: 50459   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 277   score: 0.0   memory length: 50581   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 278   score: 2.0   memory length: 50778   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 279   score: 0.0   memory length: 50901   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 280   score: 0.0   memory length: 51024   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 281   score: 1.0   memory length: 51193   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 282   score: 2.0   memory length: 51390   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 283   score: 1.0   memory length: 51541   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 284   score: 2.0   memory length: 51723   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 285   score: 4.0   memory length: 52017   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 286   score: 0.0   memory length: 52140   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 287   score: 1.0   memory length: 52291   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 288   score: 2.0   memory length: 52509   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 289   score: 2.0   memory length: 52727   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 290   score: 2.0   memory length: 52925   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 291   score: 2.0   memory length: 53126   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 292   score: 3.0   memory length: 53373   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 293   score: 0.0   memory length: 53496   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 294   score: 2.0   memory length: 53693   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 295   score: 3.0   memory length: 53960   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 296   score: 3.0   memory length: 54190   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 297   score: 1.0   memory length: 54359   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 298   score: 3.0   memory length: 54606   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 299   score: 3.0   memory length: 54833   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 300   score: 2.0   memory length: 55030   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 301   score: 0.0   memory length: 55153   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 302   score: 4.0   memory length: 55450   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 303   score: 1.0   memory length: 55601   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 304   score: 2.0   memory length: 55819   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 305   score: 1.0   memory length: 55987   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 306   score: 1.0   memory length: 56159   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 307   score: 1.0   memory length: 56328   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 308   score: 1.0   memory length: 56497   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 309   score: 3.0   memory length: 56728   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 310   score: 0.0   memory length: 56851   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 311   score: 1.0   memory length: 57020   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 312   score: 3.0   memory length: 57247   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 313   score: 2.0   memory length: 57445   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 314   score: 0.0   memory length: 57568   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 315   score: 4.0   memory length: 57844   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 316   score: 1.0   memory length: 58013   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 317   score: 0.0   memory length: 58136   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 318   score: 3.0   memory length: 58361   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 319   score: 3.0   memory length: 58625   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 320   score: 1.0   memory length: 58793   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 321   score: 2.0   memory length: 58991   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 322   score: 2.0   memory length: 59189   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 323   score: 1.0   memory length: 59361   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 324   score: 0.0   memory length: 59484   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 325   score: 0.0   memory length: 59606   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 326   score: 1.0   memory length: 59776   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 327   score: 1.0   memory length: 59945   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 328   score: 2.0   memory length: 60162   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 329   score: 3.0   memory length: 60392   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 330   score: 0.0   memory length: 60515   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 331   score: 3.0   memory length: 60761   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 332   score: 0.0   memory length: 60884   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 333   score: 2.0   memory length: 61083   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 334   score: 3.0   memory length: 61354   epsilon: 1.0    steps: 271    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 335   score: 1.0   memory length: 61504   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 336   score: 3.0   memory length: 61730   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 337   score: 1.0   memory length: 61899   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 338   score: 3.0   memory length: 62144   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 339   score: 1.0   memory length: 62312   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 340   score: 2.0   memory length: 62509   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 341   score: 2.0   memory length: 62707   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 342   score: 2.0   memory length: 62886   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 343   score: 2.0   memory length: 63106   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 344   score: 0.0   memory length: 63229   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 345   score: 1.0   memory length: 63400   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 346   score: 1.0   memory length: 63569   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 347   score: 1.0   memory length: 63739   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 348   score: 0.0   memory length: 63861   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 349   score: 1.0   memory length: 64011   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 350   score: 0.0   memory length: 64133   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 351   score: 2.0   memory length: 64350   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 352   score: 2.0   memory length: 64547   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 353   score: 0.0   memory length: 64670   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 354   score: 2.0   memory length: 64868   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 355   score: 0.0   memory length: 64991   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 356   score: 1.0   memory length: 65141   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 357   score: 3.0   memory length: 65385   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 358   score: 2.0   memory length: 65582   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 359   score: 1.0   memory length: 65733   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 360   score: 4.0   memory length: 66020   epsilon: 1.0    steps: 287    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 361   score: 1.0   memory length: 66189   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 362   score: 0.0   memory length: 66312   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 363   score: 0.0   memory length: 66435   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 364   score: 1.0   memory length: 66603   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 365   score: 3.0   memory length: 66874   epsilon: 1.0    steps: 271    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 366   score: 0.0   memory length: 66997   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 367   score: 1.0   memory length: 67166   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 368   score: 1.0   memory length: 67336   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 369   score: 2.0   memory length: 67535   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 370   score: 0.0   memory length: 67657   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 371   score: 3.0   memory length: 67903   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 372   score: 4.0   memory length: 68179   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 373   score: 2.0   memory length: 68377   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 374   score: 3.0   memory length: 68625   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 375   score: 2.0   memory length: 68807   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 376   score: 2.0   memory length: 69005   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 377   score: 1.0   memory length: 69174   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 378   score: 1.0   memory length: 69346   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 379   score: 0.0   memory length: 69468   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 380   score: 0.0   memory length: 69591   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 381   score: 1.0   memory length: 69762   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 382   score: 2.0   memory length: 69960   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 383   score: 0.0   memory length: 70083   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 384   score: 0.0   memory length: 70206   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 385   score: 4.0   memory length: 70467   epsilon: 1.0    steps: 261    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 386   score: 4.0   memory length: 70763   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 387   score: 2.0   memory length: 70961   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 388   score: 0.0   memory length: 71084   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 389   score: 1.0   memory length: 71253   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 390   score: 0.0   memory length: 71375   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 391   score: 2.0   memory length: 71575   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 392   score: 2.0   memory length: 71772   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 393   score: 0.0   memory length: 71895   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 394   score: 0.0   memory length: 72017   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 395   score: 0.0   memory length: 72140   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 396   score: 0.0   memory length: 72263   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 397   score: 3.0   memory length: 72490   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 398   score: 6.0   memory length: 72867   epsilon: 1.0    steps: 377    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 399   score: 2.0   memory length: 73065   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 400   score: 1.0   memory length: 73215   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 401   score: 0.0   memory length: 73338   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 402   score: 3.0   memory length: 73586   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 403   score: 0.0   memory length: 73709   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 404   score: 2.0   memory length: 73906   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 405   score: 3.0   memory length: 74157   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 406   score: 0.0   memory length: 74280   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 407   score: 1.0   memory length: 74431   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 408   score: 2.0   memory length: 74629   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 409   score: 5.0   memory length: 74930   epsilon: 1.0    steps: 301    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 410   score: 4.0   memory length: 75247   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 411   score: 0.0   memory length: 75370   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 412   score: 1.0   memory length: 75521   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 413   score: 2.0   memory length: 75719   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 414   score: 1.0   memory length: 75890   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 415   score: 0.0   memory length: 76013   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 416   score: 0.0   memory length: 76136   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 417   score: 0.0   memory length: 76259   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 418   score: 0.0   memory length: 76381   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 419   score: 1.0   memory length: 76549   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 420   score: 1.0   memory length: 76717   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 421   score: 2.0   memory length: 76898   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 422   score: 0.0   memory length: 77021   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 423   score: 0.0   memory length: 77144   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 424   score: 0.0   memory length: 77267   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 425   score: 3.0   memory length: 77493   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 426   score: 0.0   memory length: 77616   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 427   score: 1.0   memory length: 77767   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 428   score: 2.0   memory length: 77964   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 429   score: 2.0   memory length: 78162   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 430   score: 2.0   memory length: 78343   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 431   score: 2.0   memory length: 78562   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 432   score: 3.0   memory length: 78830   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 433   score: 0.0   memory length: 78953   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 434   score: 2.0   memory length: 79153   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 435   score: 0.0   memory length: 79276   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 436   score: 2.0   memory length: 79473   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 437   score: 2.0   memory length: 79670   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 438   score: 1.0   memory length: 79842   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 439   score: 2.0   memory length: 80061   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 440   score: 3.0   memory length: 80328   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 441   score: 0.0   memory length: 80451   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 442   score: 0.0   memory length: 80574   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 443   score: 4.0   memory length: 80872   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 444   score: 1.0   memory length: 81041   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 445   score: 1.0   memory length: 81210   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 446   score: 1.0   memory length: 81379   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 447   score: 2.0   memory length: 81596   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 448   score: 0.0   memory length: 81718   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 449   score: 0.0   memory length: 81841   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 450   score: 0.0   memory length: 81963   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 451   score: 0.0   memory length: 82086   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 452   score: 3.0   memory length: 82314   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 453   score: 0.0   memory length: 82437   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 454   score: 2.0   memory length: 82637   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 455   score: 0.0   memory length: 82760   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 456   score: 0.0   memory length: 82883   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 457   score: 1.0   memory length: 83034   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 458   score: 0.0   memory length: 83156   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 459   score: 2.0   memory length: 83374   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 460   score: 0.0   memory length: 83496   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 461   score: 0.0   memory length: 83619   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 462   score: 1.0   memory length: 83790   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 463   score: 4.0   memory length: 84086   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 464   score: 2.0   memory length: 84266   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 465   score: 2.0   memory length: 84464   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 466   score: 0.0   memory length: 84587   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 467   score: 2.0   memory length: 84804   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 468   score: 2.0   memory length: 84983   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 469   score: 3.0   memory length: 85229   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 470   score: 4.0   memory length: 85501   epsilon: 1.0    steps: 272    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 471   score: 0.0   memory length: 85624   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 472   score: 3.0   memory length: 85853   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 473   score: 0.0   memory length: 85976   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 474   score: 2.0   memory length: 86194   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 475   score: 0.0   memory length: 86317   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 476   score: 1.0   memory length: 86468   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 477   score: 1.0   memory length: 86639   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 478   score: 1.0   memory length: 86808   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 479   score: 0.0   memory length: 86931   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 480   score: 0.0   memory length: 87054   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 481   score: 2.0   memory length: 87272   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 482   score: 0.0   memory length: 87395   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 483   score: 1.0   memory length: 87546   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 484   score: 1.0   memory length: 87716   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 485   score: 0.0   memory length: 87839   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 486   score: 1.0   memory length: 87990   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 487   score: 1.0   memory length: 88158   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 488   score: 4.0   memory length: 88432   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 489   score: 2.0   memory length: 88630   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 490   score: 3.0   memory length: 88897   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 491   score: 3.0   memory length: 89122   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 492   score: 3.0   memory length: 89390   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 493   score: 4.0   memory length: 89680   epsilon: 1.0    steps: 290    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 494   score: 2.0   memory length: 89880   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 495   score: 3.0   memory length: 90107   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 496   score: 2.0   memory length: 90305   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 497   score: 0.0   memory length: 90428   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 498   score: 3.0   memory length: 90674   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 499   score: 1.0   memory length: 90824   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 500   score: 2.0   memory length: 91022   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 501   score: 4.0   memory length: 91296   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 502   score: 3.0   memory length: 91544   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 503   score: 0.0   memory length: 91667   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 504   score: 2.0   memory length: 91865   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 505   score: 1.0   memory length: 92016   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 506   score: 1.0   memory length: 92167   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 507   score: 3.0   memory length: 92430   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 508   score: 0.0   memory length: 92553   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 509   score: 4.0   memory length: 92867   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 510   score: 1.0   memory length: 93038   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 511   score: 2.0   memory length: 93238   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 512   score: 3.0   memory length: 93505   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 513   score: 1.0   memory length: 93677   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 514   score: 1.0   memory length: 93847   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 515   score: 3.0   memory length: 94091   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 516   score: 0.0   memory length: 94214   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 517   score: 0.0   memory length: 94337   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 518   score: 1.0   memory length: 94488   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 519   score: 3.0   memory length: 94732   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 520   score: 2.0   memory length: 94950   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 521   score: 6.0   memory length: 95324   epsilon: 1.0    steps: 374    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 522   score: 1.0   memory length: 95493   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 523   score: 0.0   memory length: 95616   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 524   score: 1.0   memory length: 95766   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 525   score: 2.0   memory length: 95985   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 526   score: 0.0   memory length: 96107   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 527   score: 0.0   memory length: 96230   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 528   score: 2.0   memory length: 96428   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 529   score: 1.0   memory length: 96579   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 530   score: 1.0   memory length: 96751   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 531   score: 1.0   memory length: 96901   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 532   score: 0.0   memory length: 97024   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 533   score: 0.0   memory length: 97147   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 534   score: 0.0   memory length: 97269   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 535   score: 2.0   memory length: 97466   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 536   score: 0.0   memory length: 97588   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 537   score: 2.0   memory length: 97786   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 538   score: 4.0   memory length: 98066   epsilon: 1.0    steps: 280    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 539   score: 0.0   memory length: 98189   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 540   score: 3.0   memory length: 98436   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 541   score: 1.0   memory length: 98604   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 542   score: 3.0   memory length: 98834   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 543   score: 0.0   memory length: 98957   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 544   score: 3.0   memory length: 99184   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 545   score: 2.0   memory length: 99402   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 546   score: 0.0   memory length: 99524   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 547   score: 0.0   memory length: 99646   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 548   score: 1.0   memory length: 99815   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/agent.py:95: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 1, 32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = F.smooth_l1_loss(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode: 549   score: 4.0   memory length: 100127   epsilon: 0.9997465600000055    steps: 312    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 550   score: 8.0   memory length: 100457   epsilon: 0.9990931600000197    steps: 330    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 551   score: 1.0   memory length: 100609   epsilon: 0.9987922000000262    steps: 152    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 552   score: 2.0   memory length: 100806   epsilon: 0.9984021400000347    steps: 197    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 553   score: 2.0   memory length: 101008   epsilon: 0.9980021800000434    steps: 202    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 554   score: 1.0   memory length: 101178   epsilon: 0.9976655800000507    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 555   score: 2.0   memory length: 101376   epsilon: 0.9972735400000592    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 556   score: 2.0   memory length: 101591   epsilon: 0.9968478400000684    steps: 215    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 557   score: 2.0   memory length: 101788   epsilon: 0.9964577800000769    steps: 197    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 558   score: 0.0   memory length: 101910   epsilon: 0.9962162200000821    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 559   score: 2.0   memory length: 102127   epsilon: 0.9957865600000915    steps: 217    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 560   score: 1.0   memory length: 102297   epsilon: 0.9954499600000988    steps: 170    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 561   score: 5.0   memory length: 102643   epsilon: 0.9947648800001136    steps: 346    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 562   score: 0.0   memory length: 102765   epsilon: 0.9945233200001189    steps: 122    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 563   score: 2.0   memory length: 102986   epsilon: 0.9940857400001284    steps: 221    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 564   score: 0.0   memory length: 103108   epsilon: 0.9938441800001336    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 565   score: 0.0   memory length: 103231   epsilon: 0.9936006400001389    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 566   score: 0.0   memory length: 103354   epsilon: 0.9933571000001442    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 567   score: 3.0   memory length: 103580   epsilon: 0.9929096200001539    steps: 226    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 568   score: 2.0   memory length: 103798   epsilon: 0.9924779800001633    steps: 218    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 569   score: 1.0   memory length: 103967   epsilon: 0.9921433600001706    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 570   score: 1.0   memory length: 104118   epsilon: 0.991844380000177    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 571   score: 1.0   memory length: 104268   epsilon: 0.9915473800001835    steps: 150    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 572   score: 3.0   memory length: 104493   epsilon: 0.9911018800001932    steps: 225    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 573   score: 2.0   memory length: 104691   epsilon: 0.9907098400002017    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 574   score: 7.0   memory length: 104975   epsilon: 0.9901475200002139    steps: 284    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 575   score: 0.0   memory length: 105097   epsilon: 0.9899059600002191    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 576   score: 4.0   memory length: 105390   epsilon: 0.9893258200002317    steps: 293    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 577   score: 1.0   memory length: 105559   epsilon: 0.988991200000239    steps: 169    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 578   score: 1.0   memory length: 105728   epsilon: 0.9886565800002463    steps: 169    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 579   score: 3.0   memory length: 105977   epsilon: 0.988163560000257    steps: 249    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 580   score: 2.0   memory length: 106175   epsilon: 0.9877715200002655    steps: 198    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 581   score: 0.0   memory length: 106298   epsilon: 0.9875279800002708    steps: 123    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 582   score: 1.0   memory length: 106467   epsilon: 0.987193360000278    steps: 169    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 583   score: 0.0   memory length: 106589   epsilon: 0.9869518000002833    steps: 122    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 584   score: 3.0   memory length: 106815   epsilon: 0.986504320000293    steps: 226    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 585   score: 1.0   memory length: 106984   epsilon: 0.9861697000003002    steps: 169    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 586   score: 1.0   memory length: 107154   epsilon: 0.9858331000003075    steps: 170    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 587   score: 0.0   memory length: 107277   epsilon: 0.9855895600003128    steps: 123    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 588   score: 0.0   memory length: 107400   epsilon: 0.9853460200003181    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 589   score: 1.0   memory length: 107551   epsilon: 0.9850470400003246    steps: 151    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 590   score: 3.0   memory length: 107800   epsilon: 0.9845540200003353    steps: 249    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 591   score: 2.0   memory length: 107980   epsilon: 0.984197620000343    steps: 180    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 592   score: 3.0   memory length: 108249   epsilon: 0.9836650000003546    steps: 269    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 593   score: 2.0   memory length: 108467   epsilon: 0.983233360000364    steps: 218    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 594   score: 0.0   memory length: 108589   epsilon: 0.9829918000003692    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 595   score: 1.0   memory length: 108739   epsilon: 0.9826948000003757    steps: 150    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 596   score: 0.0   memory length: 108861   epsilon: 0.9824532400003809    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 597   score: 1.0   memory length: 109013   epsilon: 0.9821522800003875    steps: 152    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 598   score: 1.0   memory length: 109184   epsilon: 0.9818137000003948    steps: 171    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 599   score: 3.0   memory length: 109450   epsilon: 0.9812870200004062    steps: 266    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 600   score: 1.0   memory length: 109601   epsilon: 0.9809880400004127    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 601   score: 0.0   memory length: 109724   epsilon: 0.980744500000418    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 602   score: 0.0   memory length: 109847   epsilon: 0.9805009600004233    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 603   score: 0.0   memory length: 109970   epsilon: 0.9802574200004286    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 604   score: 2.0   memory length: 110167   epsilon: 0.9798673600004371    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 605   score: 1.0   memory length: 110337   epsilon: 0.9795307600004444    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 606   score: 1.0   memory length: 110506   epsilon: 0.9791961400004516    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 607   score: 2.0   memory length: 110686   epsilon: 0.9788397400004594    steps: 180    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 608   score: 0.0   memory length: 110809   epsilon: 0.9785962000004647    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 609   score: 0.0   memory length: 110931   epsilon: 0.9783546400004699    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 610   score: 1.0   memory length: 111100   epsilon: 0.9780200200004772    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 611   score: 2.0   memory length: 111298   epsilon: 0.9776279800004857    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 612   score: 2.0   memory length: 111478   epsilon: 0.9772715800004934    steps: 180    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 613   score: 0.0   memory length: 111600   epsilon: 0.9770300200004987    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 614   score: 1.0   memory length: 111768   epsilon: 0.9766973800005059    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 615   score: 1.0   memory length: 111919   epsilon: 0.9763984000005124    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 616   score: 2.0   memory length: 112117   epsilon: 0.9760063600005209    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 617   score: 2.0   memory length: 112315   epsilon: 0.9756143200005294    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 618   score: 0.0   memory length: 112437   epsilon: 0.9753727600005346    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 619   score: 1.0   memory length: 112608   epsilon: 0.975034180000542    steps: 171    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 620   score: 2.0   memory length: 112807   epsilon: 0.9746401600005505    steps: 199    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 621   score: 4.0   memory length: 113079   epsilon: 0.9741016000005622    steps: 272    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 622   score: 1.0   memory length: 113251   epsilon: 0.9737610400005696    steps: 172    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 623   score: 2.0   memory length: 113449   epsilon: 0.9733690000005781    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 624   score: 2.0   memory length: 113649   epsilon: 0.9729730000005867    steps: 200    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 625   score: 4.0   memory length: 113965   epsilon: 0.9723473200006003    steps: 316    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 626   score: 2.0   memory length: 114182   epsilon: 0.9719176600006096    steps: 217    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 627   score: 3.0   memory length: 114426   epsilon: 0.9714345400006201    steps: 244    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 628   score: 2.0   memory length: 114624   epsilon: 0.9710425000006286    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 629   score: 0.0   memory length: 114746   epsilon: 0.9708009400006339    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 630   score: 1.0   memory length: 114915   epsilon: 0.9704663200006411    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 631   score: 3.0   memory length: 115142   epsilon: 0.9700168600006509    steps: 227    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 632   score: 1.0   memory length: 115310   epsilon: 0.9696842200006581    steps: 168    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 633   score: 2.0   memory length: 115528   epsilon: 0.9692525800006675    steps: 218    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 634   score: 3.0   memory length: 115795   epsilon: 0.968723920000679    steps: 267    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 635   score: 4.0   memory length: 116054   epsilon: 0.9682111000006901    steps: 259    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 636   score: 1.0   memory length: 116226   epsilon: 0.9678705400006975    steps: 172    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 637   score: 2.0   memory length: 116423   epsilon: 0.967480480000706    steps: 197    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 638   score: 1.0   memory length: 116594   epsilon: 0.9671419000007133    steps: 171    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 639   score: 0.0   memory length: 116717   epsilon: 0.9668983600007186    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 640   score: 3.0   memory length: 116946   epsilon: 0.9664449400007284    steps: 229    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 641   score: 0.0   memory length: 117069   epsilon: 0.9662014000007337    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 642   score: 1.0   memory length: 117237   epsilon: 0.965868760000741    steps: 168    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 643   score: 2.0   memory length: 117435   epsilon: 0.9654767200007495    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 644   score: 1.0   memory length: 117604   epsilon: 0.9651421000007567    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 645   score: 2.0   memory length: 117801   epsilon: 0.9647520400007652    steps: 197    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 646   score: 0.0   memory length: 117923   epsilon: 0.9645104800007704    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 647   score: 0.0   memory length: 118046   epsilon: 0.9642669400007757    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 648   score: 1.0   memory length: 118198   epsilon: 0.9639659800007823    steps: 152    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 649   score: 2.0   memory length: 118397   epsilon: 0.9635719600007908    steps: 199    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 650   score: 0.0   memory length: 118520   epsilon: 0.9633284200007961    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 651   score: 0.0   memory length: 118643   epsilon: 0.9630848800008014    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 652   score: 0.0   memory length: 118765   epsilon: 0.9628433200008066    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 653   score: 0.0   memory length: 118888   epsilon: 0.9625997800008119    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 654   score: 3.0   memory length: 119134   epsilon: 0.9621127000008225    steps: 246    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 655   score: 0.0   memory length: 119257   epsilon: 0.9618691600008278    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 656   score: 0.0   memory length: 119380   epsilon: 0.9616256200008331    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 657   score: 0.0   memory length: 119502   epsilon: 0.9613840600008383    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 658   score: 0.0   memory length: 119624   epsilon: 0.9611425000008436    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 659   score: 2.0   memory length: 119839   epsilon: 0.9607168000008528    steps: 215    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 660   score: 0.0   memory length: 119962   epsilon: 0.9604732600008581    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 661   score: 1.0   memory length: 120132   epsilon: 0.9601366600008654    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 662   score: 3.0   memory length: 120395   epsilon: 0.9596159200008767    steps: 263    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 663   score: 2.0   memory length: 120610   epsilon: 0.9591902200008859    steps: 215    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 664   score: 2.0   memory length: 120809   epsilon: 0.9587962000008945    steps: 199    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 665   score: 0.0   memory length: 120931   epsilon: 0.9585546400008997    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 666   score: 0.0   memory length: 121054   epsilon: 0.958311100000905    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 667   score: 2.0   memory length: 121256   epsilon: 0.9579111400009137    steps: 202    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 668   score: 0.0   memory length: 121379   epsilon: 0.957667600000919    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 669   score: 1.0   memory length: 121530   epsilon: 0.9573686200009255    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 670   score: 0.0   memory length: 121653   epsilon: 0.9571250800009308    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 671   score: 0.0   memory length: 121776   epsilon: 0.9568815400009361    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 672   score: 2.0   memory length: 121976   epsilon: 0.9564855400009447    steps: 200    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 673   score: 0.0   memory length: 122099   epsilon: 0.9562420000009499    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 674   score: 0.0   memory length: 122222   epsilon: 0.9559984600009552    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 675   score: 0.0   memory length: 122344   epsilon: 0.9557569000009605    steps: 122    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 676   score: 4.0   memory length: 122622   epsilon: 0.9552064600009724    steps: 278    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 677   score: 2.0   memory length: 122839   epsilon: 0.9547768000009818    steps: 217    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 678   score: 2.0   memory length: 123037   epsilon: 0.9543847600009903    steps: 198    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 679   score: 0.0   memory length: 123159   epsilon: 0.9541432000009955    steps: 122    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 680   score: 1.0   memory length: 123327   epsilon: 0.9538105600010027    steps: 168    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 681   score: 0.0   memory length: 123450   epsilon: 0.953567020001008    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 682   score: 0.0   memory length: 123573   epsilon: 0.9533234800010133    steps: 123    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 683   score: 0.0   memory length: 123696   epsilon: 0.9530799400010186    steps: 123    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 684   score: 2.0   memory length: 123894   epsilon: 0.9526879000010271    steps: 198    lr: 0.0001     evaluation reward: 1.19\n",
            "episode: 685   score: 0.0   memory length: 124017   epsilon: 0.9524443600010324    steps: 123    lr: 0.0001     evaluation reward: 1.18\n",
            "episode: 686   score: 0.0   memory length: 124139   epsilon: 0.9522028000010376    steps: 122    lr: 0.0001     evaluation reward: 1.17\n",
            "episode: 687   score: 2.0   memory length: 124339   epsilon: 0.9518068000010462    steps: 200    lr: 0.0001     evaluation reward: 1.19\n",
            "episode: 688   score: 3.0   memory length: 124586   epsilon: 0.9513177400010568    steps: 247    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 689   score: 4.0   memory length: 124901   epsilon: 0.9506940400010704    steps: 315    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 690   score: 0.0   memory length: 125024   epsilon: 0.9504505000010757    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 691   score: 3.0   memory length: 125291   epsilon: 0.9499218400010871    steps: 267    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 692   score: 3.0   memory length: 125519   epsilon: 0.949470400001097    steps: 228    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 693   score: 1.0   memory length: 125670   epsilon: 0.9491714200011034    steps: 151    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 694   score: 1.0   memory length: 125820   epsilon: 0.9488744200011099    steps: 150    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 695   score: 3.0   memory length: 126091   epsilon: 0.9483378400011215    steps: 271    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 696   score: 2.0   memory length: 126289   epsilon: 0.94794580000113    steps: 198    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 697   score: 2.0   memory length: 126470   epsilon: 0.9475874200011378    steps: 181    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 698   score: 1.0   memory length: 126620   epsilon: 0.9472904200011443    steps: 150    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 699   score: 1.0   memory length: 126771   epsilon: 0.9469914400011508    steps: 151    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 700   score: 1.0   memory length: 126921   epsilon: 0.9466944400011572    steps: 150    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 701   score: 5.0   memory length: 127245   epsilon: 0.9460529200011711    steps: 324    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 702   score: 1.0   memory length: 127415   epsilon: 0.9457163200011784    steps: 170    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 703   score: 2.0   memory length: 127632   epsilon: 0.9452866600011878    steps: 217    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 704   score: 1.0   memory length: 127801   epsilon: 0.944952040001195    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 705   score: 0.0   memory length: 127923   epsilon: 0.9447104800012003    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 706   score: 1.0   memory length: 128094   epsilon: 0.9443719000012076    steps: 171    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 707   score: 1.0   memory length: 128265   epsilon: 0.944033320001215    steps: 171    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 708   score: 1.0   memory length: 128435   epsilon: 0.9436967200012223    steps: 170    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 709   score: 3.0   memory length: 128678   epsilon: 0.9432155800012327    steps: 243    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 710   score: 1.0   memory length: 128829   epsilon: 0.9429166000012392    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 711   score: 2.0   memory length: 129013   epsilon: 0.9425522800012471    steps: 184    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 712   score: 1.0   memory length: 129182   epsilon: 0.9422176600012544    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 713   score: 5.0   memory length: 129545   epsilon: 0.94149892000127    steps: 363    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 714   score: 2.0   memory length: 129761   epsilon: 0.9410712400012793    steps: 216    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 715   score: 1.0   memory length: 129930   epsilon: 0.9407366200012865    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 716   score: 2.0   memory length: 130127   epsilon: 0.940346560001295    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 717   score: 1.0   memory length: 130278   epsilon: 0.9400475800013015    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 718   score: 3.0   memory length: 130526   epsilon: 0.9395565400013122    steps: 248    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 719   score: 0.0   memory length: 130648   epsilon: 0.9393149800013174    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 720   score: 2.0   memory length: 130829   epsilon: 0.9389566000013252    steps: 181    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 721   score: 0.0   memory length: 130952   epsilon: 0.9387130600013305    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 722   score: 0.0   memory length: 131074   epsilon: 0.9384715000013357    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 723   score: 2.0   memory length: 131272   epsilon: 0.9380794600013442    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 724   score: 6.0   memory length: 131618   epsilon: 0.9373943800013591    steps: 346    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 725   score: 2.0   memory length: 131816   epsilon: 0.9370023400013676    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 726   score: 0.0   memory length: 131939   epsilon: 0.9367588000013729    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 727   score: 0.0   memory length: 132062   epsilon: 0.9365152600013782    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 728   score: 0.0   memory length: 132184   epsilon: 0.9362737000013834    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 729   score: 1.0   memory length: 132354   epsilon: 0.9359371000013907    steps: 170    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 730   score: 1.0   memory length: 132525   epsilon: 0.9355985200013981    steps: 171    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 731   score: 3.0   memory length: 132788   epsilon: 0.9350777800014094    steps: 263    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 732   score: 0.0   memory length: 132911   epsilon: 0.9348342400014147    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 733   score: 3.0   memory length: 133156   epsilon: 0.9343491400014252    steps: 245    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 734   score: 1.0   memory length: 133326   epsilon: 0.9340125400014325    steps: 170    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 735   score: 3.0   memory length: 133576   epsilon: 0.9335175400014433    steps: 250    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 736   score: 0.0   memory length: 133699   epsilon: 0.9332740000014486    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 737   score: 2.0   memory length: 133898   epsilon: 0.9328799800014571    steps: 199    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 738   score: 4.0   memory length: 134173   epsilon: 0.9323354800014689    steps: 275    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 739   score: 0.0   memory length: 134296   epsilon: 0.9320919400014742    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 740   score: 0.0   memory length: 134418   epsilon: 0.9318503800014795    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 741   score: 0.0   memory length: 134540   epsilon: 0.9316088200014847    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 742   score: 0.0   memory length: 134662   epsilon: 0.93136726000149    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 743   score: 2.0   memory length: 134879   epsilon: 0.9309376000014993    steps: 217    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 744   score: 1.0   memory length: 135049   epsilon: 0.9306010000015066    steps: 170    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 745   score: 2.0   memory length: 135247   epsilon: 0.9302089600015151    steps: 198    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 746   score: 1.0   memory length: 135418   epsilon: 0.9298703800015224    steps: 171    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 747   score: 0.0   memory length: 135540   epsilon: 0.9296288200015277    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 748   score: 3.0   memory length: 135766   epsilon: 0.9291813400015374    steps: 226    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 749   score: 0.0   memory length: 135888   epsilon: 0.9289397800015426    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 750   score: 1.0   memory length: 136057   epsilon: 0.9286051600015499    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 751   score: 1.0   memory length: 136225   epsilon: 0.9282725200015571    steps: 168    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 752   score: 2.0   memory length: 136440   epsilon: 0.9278468200015664    steps: 215    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 753   score: 1.0   memory length: 136610   epsilon: 0.9275102200015737    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 754   score: 2.0   memory length: 136790   epsilon: 0.9271538200015814    steps: 180    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 755   score: 3.0   memory length: 137035   epsilon: 0.926668720001592    steps: 245    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 756   score: 1.0   memory length: 137204   epsilon: 0.9263341000015992    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 757   score: 1.0   memory length: 137354   epsilon: 0.9260371000016057    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 758   score: 5.0   memory length: 137698   epsilon: 0.9253559800016204    steps: 344    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 759   score: 3.0   memory length: 137945   epsilon: 0.9248669200016311    steps: 247    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 760   score: 1.0   memory length: 138096   epsilon: 0.9245679400016376    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 761   score: 2.0   memory length: 138296   epsilon: 0.9241719400016462    steps: 200    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 762   score: 0.0   memory length: 138419   epsilon: 0.9239284000016514    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 763   score: 0.0   memory length: 138542   epsilon: 0.9236848600016567    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 764   score: 2.0   memory length: 138761   epsilon: 0.9232512400016661    steps: 219    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 765   score: 2.0   memory length: 138959   epsilon: 0.9228592000016747    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 766   score: 2.0   memory length: 139157   epsilon: 0.9224671600016832    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 767   score: 3.0   memory length: 139403   epsilon: 0.9219800800016937    steps: 246    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 768   score: 1.0   memory length: 139574   epsilon: 0.9216415000017011    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 769   score: 2.0   memory length: 139791   epsilon: 0.9212118400017104    steps: 217    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 770   score: 1.0   memory length: 139942   epsilon: 0.9209128600017169    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 771   score: 0.0   memory length: 140064   epsilon: 0.9206713000017221    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 772   score: 2.0   memory length: 140265   epsilon: 0.9202733200017308    steps: 201    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 773   score: 1.0   memory length: 140416   epsilon: 0.9199743400017373    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 774   score: 5.0   memory length: 140759   epsilon: 0.919295200001752    steps: 343    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 775   score: 0.0   memory length: 140882   epsilon: 0.9190516600017573    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 776   score: 1.0   memory length: 141051   epsilon: 0.9187170400017646    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 777   score: 2.0   memory length: 141249   epsilon: 0.9183250000017731    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 778   score: 1.0   memory length: 141418   epsilon: 0.9179903800017803    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 779   score: 4.0   memory length: 141714   epsilon: 0.9174043000017931    steps: 296    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 780   score: 0.0   memory length: 141837   epsilon: 0.9171607600017984    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 781   score: 3.0   memory length: 142065   epsilon: 0.9167093200018082    steps: 228    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 782   score: 0.0   memory length: 142187   epsilon: 0.9164677600018134    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 783   score: 1.0   memory length: 142356   epsilon: 0.9161331400018207    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 784   score: 4.0   memory length: 142650   epsilon: 0.9155510200018333    steps: 294    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 785   score: 2.0   memory length: 142867   epsilon: 0.9151213600018426    steps: 217    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 786   score: 3.0   memory length: 143112   epsilon: 0.9146362600018532    steps: 245    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 787   score: 3.0   memory length: 143358   epsilon: 0.9141491800018637    steps: 246    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 788   score: 2.0   memory length: 143540   epsilon: 0.9137888200018716    steps: 182    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 789   score: 5.0   memory length: 143845   epsilon: 0.9131849200018847    steps: 305    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 790   score: 0.0   memory length: 143968   epsilon: 0.91294138000189    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 791   score: 3.0   memory length: 144215   epsilon: 0.9124523200019006    steps: 247    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 792   score: 2.0   memory length: 144413   epsilon: 0.9120602800019091    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 793   score: 2.0   memory length: 144628   epsilon: 0.9116345800019183    steps: 215    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 794   score: 1.0   memory length: 144797   epsilon: 0.9112999600019256    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 795   score: 0.0   memory length: 144920   epsilon: 0.9110564200019309    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 796   score: 1.0   memory length: 145088   epsilon: 0.9107237800019381    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 797   score: 3.0   memory length: 145334   epsilon: 0.9102367000019487    steps: 246    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 798   score: 5.0   memory length: 145658   epsilon: 0.9095951800019626    steps: 324    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 799   score: 0.0   memory length: 145780   epsilon: 0.9093536200019678    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 800   score: 0.0   memory length: 145903   epsilon: 0.9091100800019731    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 801   score: 1.0   memory length: 146072   epsilon: 0.9087754600019804    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 802   score: 1.0   memory length: 146242   epsilon: 0.9084388600019877    steps: 170    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 803   score: 0.0   memory length: 146365   epsilon: 0.908195320001993    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 804   score: 4.0   memory length: 146666   epsilon: 0.9075993400020059    steps: 301    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 805   score: 2.0   memory length: 146847   epsilon: 0.9072409600020137    steps: 181    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 806   score: 3.0   memory length: 147113   epsilon: 0.9067142800020251    steps: 266    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 807   score: 2.0   memory length: 147311   epsilon: 0.9063222400020337    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 808   score: 3.0   memory length: 147540   epsilon: 0.9058688200020435    steps: 229    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 809   score: 0.0   memory length: 147663   epsilon: 0.9056252800020488    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 810   score: 3.0   memory length: 147906   epsilon: 0.9051441400020592    steps: 243    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 811   score: 0.0   memory length: 148029   epsilon: 0.9049006000020645    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 812   score: 2.0   memory length: 148246   epsilon: 0.9044709400020738    steps: 217    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 813   score: 1.0   memory length: 148414   epsilon: 0.9041383000020811    steps: 168    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 814   score: 1.0   memory length: 148585   epsilon: 0.9037997200020884    steps: 171    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 815   score: 0.0   memory length: 148708   epsilon: 0.9035561800020937    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 816   score: 2.0   memory length: 148906   epsilon: 0.9031641400021022    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 817   score: 1.0   memory length: 149075   epsilon: 0.9028295200021095    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 818   score: 2.0   memory length: 149273   epsilon: 0.902437480002118    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 819   score: 3.0   memory length: 149517   epsilon: 0.9019543600021285    steps: 244    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 820   score: 4.0   memory length: 149814   epsilon: 0.9013663000021412    steps: 297    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 821   score: 1.0   memory length: 149984   epsilon: 0.9010297000021485    steps: 170    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 822   score: 1.0   memory length: 150153   epsilon: 0.9006950800021558    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 823   score: 1.0   memory length: 150304   epsilon: 0.9003961000021623    steps: 151    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 824   score: 1.0   memory length: 150455   epsilon: 0.9000971200021688    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 825   score: 1.0   memory length: 150605   epsilon: 0.8998001200021752    steps: 150    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 826   score: 1.0   memory length: 150774   epsilon: 0.8994655000021825    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 827   score: 1.0   memory length: 150943   epsilon: 0.8991308800021898    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 828   score: 0.0   memory length: 151065   epsilon: 0.898889320002195    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 829   score: 0.0   memory length: 151187   epsilon: 0.8986477600022003    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 830   score: 1.0   memory length: 151356   epsilon: 0.8983131400022075    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 831   score: 3.0   memory length: 151601   epsilon: 0.897828040002218    steps: 245    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 832   score: 0.0   memory length: 151724   epsilon: 0.8975845000022233    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 833   score: 1.0   memory length: 151893   epsilon: 0.8972498800022306    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 834   score: 1.0   memory length: 152063   epsilon: 0.8969132800022379    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 835   score: 1.0   memory length: 152231   epsilon: 0.8965806400022451    steps: 168    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 836   score: 1.0   memory length: 152400   epsilon: 0.8962460200022524    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 837   score: 2.0   memory length: 152619   epsilon: 0.8958124000022618    steps: 219    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 838   score: 0.0   memory length: 152741   epsilon: 0.895570840002267    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 839   score: 2.0   memory length: 152959   epsilon: 0.8951392000022764    steps: 218    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 840   score: 0.0   memory length: 153082   epsilon: 0.8948956600022817    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 841   score: 2.0   memory length: 153283   epsilon: 0.8944976800022904    steps: 201    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 842   score: 2.0   memory length: 153482   epsilon: 0.8941036600022989    steps: 199    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 843   score: 0.0   memory length: 153605   epsilon: 0.8938601200023042    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 844   score: 0.0   memory length: 153728   epsilon: 0.8936165800023095    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 845   score: 2.0   memory length: 153946   epsilon: 0.8931849400023189    steps: 218    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 846   score: 3.0   memory length: 154173   epsilon: 0.8927354800023286    steps: 227    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 847   score: 0.0   memory length: 154296   epsilon: 0.8924919400023339    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 848   score: 1.0   memory length: 154464   epsilon: 0.8921593000023411    steps: 168    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 849   score: 2.0   memory length: 154644   epsilon: 0.8918029000023489    steps: 180    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 850   score: 0.0   memory length: 154766   epsilon: 0.8915613400023541    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 851   score: 2.0   memory length: 154984   epsilon: 0.8911297000023635    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 852   score: 0.0   memory length: 155106   epsilon: 0.8908881400023687    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 853   score: 2.0   memory length: 155304   epsilon: 0.8904961000023772    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 854   score: 0.0   memory length: 155427   epsilon: 0.8902525600023825    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 855   score: 0.0   memory length: 155550   epsilon: 0.8900090200023878    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 856   score: 2.0   memory length: 155771   epsilon: 0.8895714400023973    steps: 221    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 857   score: 0.0   memory length: 155893   epsilon: 0.8893298800024025    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 858   score: 0.0   memory length: 156016   epsilon: 0.8890863400024078    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 859   score: 0.0   memory length: 156138   epsilon: 0.8888447800024131    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 860   score: 0.0   memory length: 156261   epsilon: 0.8886012400024184    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 861   score: 0.0   memory length: 156384   epsilon: 0.8883577000024236    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 862   score: 0.0   memory length: 156507   epsilon: 0.8881141600024289    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 863   score: 2.0   memory length: 156687   epsilon: 0.8877577600024367    steps: 180    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 864   score: 0.0   memory length: 156809   epsilon: 0.8875162000024419    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 865   score: 1.0   memory length: 156977   epsilon: 0.8871835600024491    steps: 168    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 866   score: 3.0   memory length: 157202   epsilon: 0.8867380600024588    steps: 225    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 867   score: 0.0   memory length: 157325   epsilon: 0.8864945200024641    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 868   score: 3.0   memory length: 157572   epsilon: 0.8860054600024747    steps: 247    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 869   score: 2.0   memory length: 157790   epsilon: 0.8855738200024841    steps: 218    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 870   score: 3.0   memory length: 158015   epsilon: 0.8851283200024938    steps: 225    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 871   score: 0.0   memory length: 158138   epsilon: 0.884884780002499    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 872   score: 2.0   memory length: 158336   epsilon: 0.8844927400025075    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 873   score: 1.0   memory length: 158505   epsilon: 0.8841581200025148    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 874   score: 2.0   memory length: 158722   epsilon: 0.8837284600025241    steps: 217    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 875   score: 5.0   memory length: 159049   epsilon: 0.8830810000025382    steps: 327    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 876   score: 3.0   memory length: 159316   epsilon: 0.8825523400025497    steps: 267    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 877   score: 0.0   memory length: 159438   epsilon: 0.8823107800025549    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 878   score: 2.0   memory length: 159654   epsilon: 0.8818831000025642    steps: 216    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 879   score: 0.0   memory length: 159777   epsilon: 0.8816395600025695    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 880   score: 1.0   memory length: 159928   epsilon: 0.881340580002576    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 881   score: 0.0   memory length: 160050   epsilon: 0.8810990200025812    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 882   score: 4.0   memory length: 160345   epsilon: 0.8805149200025939    steps: 295    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 883   score: 1.0   memory length: 160516   epsilon: 0.8801763400026013    steps: 171    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 884   score: 2.0   memory length: 160713   epsilon: 0.8797862800026097    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 885   score: 2.0   memory length: 160933   epsilon: 0.8793506800026192    steps: 220    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 886   score: 3.0   memory length: 161178   epsilon: 0.8788655800026297    steps: 245    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 887   score: 0.0   memory length: 161300   epsilon: 0.878624020002635    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 888   score: 2.0   memory length: 161498   epsilon: 0.8782319800026435    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 889   score: 1.0   memory length: 161666   epsilon: 0.8778993400026507    steps: 168    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 890   score: 4.0   memory length: 161943   epsilon: 0.8773508800026626    steps: 277    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 891   score: 1.0   memory length: 162093   epsilon: 0.877053880002669    steps: 150    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 892   score: 2.0   memory length: 162291   epsilon: 0.8766618400026776    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 893   score: 2.0   memory length: 162507   epsilon: 0.8762341600026868    steps: 216    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 894   score: 1.0   memory length: 162658   epsilon: 0.8759351800026933    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 895   score: 0.0   memory length: 162781   epsilon: 0.8756916400026986    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 896   score: 0.0   memory length: 162903   epsilon: 0.8754500800027039    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 897   score: 0.0   memory length: 163026   epsilon: 0.8752065400027091    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 898   score: 0.0   memory length: 163148   epsilon: 0.8749649800027144    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 899   score: 0.0   memory length: 163270   epsilon: 0.8747234200027196    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 900   score: 3.0   memory length: 163515   epsilon: 0.8742383200027302    steps: 245    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 901   score: 5.0   memory length: 163841   epsilon: 0.8735928400027442    steps: 326    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 902   score: 2.0   memory length: 164059   epsilon: 0.8731612000027535    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 903   score: 1.0   memory length: 164227   epsilon: 0.8728285600027608    steps: 168    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 904   score: 1.0   memory length: 164397   epsilon: 0.8724919600027681    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 905   score: 3.0   memory length: 164643   epsilon: 0.8720048800027786    steps: 246    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 906   score: 1.0   memory length: 164794   epsilon: 0.8717059000027851    steps: 151    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 907   score: 1.0   memory length: 164947   epsilon: 0.8714029600027917    steps: 153    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 908   score: 1.0   memory length: 165117   epsilon: 0.871066360002799    steps: 170    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 909   score: 4.0   memory length: 165393   epsilon: 0.8705198800028109    steps: 276    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 910   score: 2.0   memory length: 165591   epsilon: 0.8701278400028194    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 911   score: 0.0   memory length: 165714   epsilon: 0.8698843000028247    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 912   score: 0.0   memory length: 165836   epsilon: 0.8696427400028299    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 913   score: 3.0   memory length: 166063   epsilon: 0.8691932800028397    steps: 227    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 914   score: 1.0   memory length: 166231   epsilon: 0.8688606400028469    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 915   score: 0.0   memory length: 166354   epsilon: 0.8686171000028522    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 916   score: 2.0   memory length: 166554   epsilon: 0.8682211000028608    steps: 200    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 917   score: 3.0   memory length: 166780   epsilon: 0.8677736200028705    steps: 226    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 918   score: 0.0   memory length: 166903   epsilon: 0.8675300800028758    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 919   score: 0.0   memory length: 167026   epsilon: 0.8672865400028811    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 920   score: 2.0   memory length: 167223   epsilon: 0.8668964800028895    steps: 197    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 921   score: 3.0   memory length: 167468   epsilon: 0.8664113800029001    steps: 245    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 922   score: 0.0   memory length: 167591   epsilon: 0.8661678400029054    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 923   score: 3.0   memory length: 167835   epsilon: 0.8656847200029159    steps: 244    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 924   score: 0.0   memory length: 167958   epsilon: 0.8654411800029211    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 925   score: 2.0   memory length: 168175   epsilon: 0.8650115200029305    steps: 217    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 926   score: 2.0   memory length: 168373   epsilon: 0.864619480002939    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 927   score: 0.0   memory length: 168496   epsilon: 0.8643759400029443    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 928   score: 1.0   memory length: 168665   epsilon: 0.8640413200029515    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 929   score: 2.0   memory length: 168883   epsilon: 0.8636096800029609    steps: 218    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 930   score: 0.0   memory length: 169006   epsilon: 0.8633661400029662    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 931   score: 1.0   memory length: 169176   epsilon: 0.8630295400029735    steps: 170    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 932   score: 1.0   memory length: 169327   epsilon: 0.86273056000298    steps: 151    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 933   score: 2.0   memory length: 169525   epsilon: 0.8623385200029885    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 934   score: 2.0   memory length: 169744   epsilon: 0.8619049000029979    steps: 219    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 935   score: 2.0   memory length: 169960   epsilon: 0.8614772200030072    steps: 216    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 936   score: 0.0   memory length: 170083   epsilon: 0.8612336800030125    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 937   score: 1.0   memory length: 170234   epsilon: 0.860934700003019    steps: 151    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 938   score: 1.0   memory length: 170385   epsilon: 0.8606357200030255    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 939   score: 2.0   memory length: 170583   epsilon: 0.860243680003034    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 940   score: 3.0   memory length: 170829   epsilon: 0.8597566000030445    steps: 246    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 941   score: 0.0   memory length: 170952   epsilon: 0.8595130600030498    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 942   score: 0.0   memory length: 171075   epsilon: 0.8592695200030551    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 943   score: 0.0   memory length: 171198   epsilon: 0.8590259800030604    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 944   score: 1.0   memory length: 171369   epsilon: 0.8586874000030678    steps: 171    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 945   score: 2.0   memory length: 171586   epsilon: 0.8582577400030771    steps: 217    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 946   score: 0.0   memory length: 171709   epsilon: 0.8580142000030824    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 947   score: 0.0   memory length: 171832   epsilon: 0.8577706600030877    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 948   score: 2.0   memory length: 172050   epsilon: 0.857339020003097    steps: 218    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 949   score: 1.0   memory length: 172201   epsilon: 0.8570400400031035    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 950   score: 2.0   memory length: 172420   epsilon: 0.8566064200031129    steps: 219    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 951   score: 1.0   memory length: 172572   epsilon: 0.8563054600031195    steps: 152    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 952   score: 2.0   memory length: 172788   epsilon: 0.8558777800031288    steps: 216    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 953   score: 2.0   memory length: 173004   epsilon: 0.855450100003138    steps: 216    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 954   score: 2.0   memory length: 173222   epsilon: 0.8550184600031474    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 955   score: 1.0   memory length: 173373   epsilon: 0.8547194800031539    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 956   score: 0.0   memory length: 173496   epsilon: 0.8544759400031592    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 957   score: 2.0   memory length: 173715   epsilon: 0.8540423200031686    steps: 219    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 958   score: 1.0   memory length: 173865   epsilon: 0.853745320003175    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 959   score: 1.0   memory length: 174037   epsilon: 0.8534047600031824    steps: 172    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 960   score: 0.0   memory length: 174159   epsilon: 0.8531632000031877    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 961   score: 3.0   memory length: 174423   epsilon: 0.852640480003199    steps: 264    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 962   score: 1.0   memory length: 174591   epsilon: 0.8523078400032063    steps: 168    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 963   score: 1.0   memory length: 174742   epsilon: 0.8520088600032127    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 964   score: 0.0   memory length: 174864   epsilon: 0.851767300003218    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 965   score: 0.0   memory length: 174987   epsilon: 0.8515237600032233    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 966   score: 1.0   memory length: 175137   epsilon: 0.8512267600032297    steps: 150    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 967   score: 0.0   memory length: 175260   epsilon: 0.850983220003235    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 968   score: 2.0   memory length: 175478   epsilon: 0.8505515800032444    steps: 218    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 969   score: 0.0   memory length: 175601   epsilon: 0.8503080400032497    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 970   score: 0.0   memory length: 175723   epsilon: 0.8500664800032549    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 971   score: 0.0   memory length: 175846   epsilon: 0.8498229400032602    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 972   score: 0.0   memory length: 175968   epsilon: 0.8495813800032654    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 973   score: 1.0   memory length: 176119   epsilon: 0.8492824000032719    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 974   score: 1.0   memory length: 176287   epsilon: 0.8489497600032792    steps: 168    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 975   score: 0.0   memory length: 176409   epsilon: 0.8487082000032844    steps: 122    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 976   score: 0.0   memory length: 176532   epsilon: 0.8484646600032897    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 977   score: 0.0   memory length: 176655   epsilon: 0.848221120003295    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 978   score: 1.0   memory length: 176827   epsilon: 0.8478805600033024    steps: 172    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 979   score: 0.0   memory length: 176949   epsilon: 0.8476390000033076    steps: 122    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 980   score: 2.0   memory length: 177166   epsilon: 0.8472093400033169    steps: 217    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 981   score: 0.0   memory length: 177289   epsilon: 0.8469658000033222    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 982   score: 2.0   memory length: 177507   epsilon: 0.8465341600033316    steps: 218    lr: 0.0001     evaluation reward: 1.19\n",
            "episode: 983   score: 1.0   memory length: 177676   epsilon: 0.8461995400033389    steps: 169    lr: 0.0001     evaluation reward: 1.19\n",
            "episode: 984   score: 0.0   memory length: 177799   epsilon: 0.8459560000033441    steps: 123    lr: 0.0001     evaluation reward: 1.17\n",
            "episode: 985   score: 1.0   memory length: 177969   epsilon: 0.8456194000033515    steps: 170    lr: 0.0001     evaluation reward: 1.16\n",
            "episode: 986   score: 0.0   memory length: 178092   epsilon: 0.8453758600033567    steps: 123    lr: 0.0001     evaluation reward: 1.13\n",
            "episode: 987   score: 1.0   memory length: 178264   epsilon: 0.8450353000033641    steps: 172    lr: 0.0001     evaluation reward: 1.14\n",
            "episode: 988   score: 5.0   memory length: 178560   epsilon: 0.8444492200033769    steps: 296    lr: 0.0001     evaluation reward: 1.17\n",
            "episode: 989   score: 0.0   memory length: 178683   epsilon: 0.8442056800033821    steps: 123    lr: 0.0001     evaluation reward: 1.16\n",
            "episode: 990   score: 3.0   memory length: 178929   epsilon: 0.8437186000033927    steps: 246    lr: 0.0001     evaluation reward: 1.15\n",
            "episode: 991   score: 2.0   memory length: 179126   epsilon: 0.8433285400034012    steps: 197    lr: 0.0001     evaluation reward: 1.16\n",
            "episode: 992   score: 2.0   memory length: 179324   epsilon: 0.8429365000034097    steps: 198    lr: 0.0001     evaluation reward: 1.16\n",
            "episode: 993   score: 2.0   memory length: 179522   epsilon: 0.8425444600034182    steps: 198    lr: 0.0001     evaluation reward: 1.16\n",
            "episode: 994   score: 3.0   memory length: 179769   epsilon: 0.8420554000034288    steps: 247    lr: 0.0001     evaluation reward: 1.18\n",
            "episode: 995   score: 2.0   memory length: 179967   epsilon: 0.8416633600034373    steps: 198    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 996   score: 0.0   memory length: 180089   epsilon: 0.8414218000034426    steps: 122    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 997   score: 4.0   memory length: 180349   epsilon: 0.8409070000034538    steps: 260    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 998   score: 1.0   memory length: 180519   epsilon: 0.8405704000034611    steps: 170    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 999   score: 3.0   memory length: 180786   epsilon: 0.8400417400034725    steps: 267    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1000   score: 3.0   memory length: 181033   epsilon: 0.8395526800034832    steps: 247    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1001   score: 0.0   memory length: 181156   epsilon: 0.8393091400034884    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 1002   score: 2.0   memory length: 181374   epsilon: 0.8388775000034978    steps: 218    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 1003   score: 1.0   memory length: 181524   epsilon: 0.8385805000035043    steps: 150    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 1004   score: 0.0   memory length: 181646   epsilon: 0.8383389400035095    steps: 122    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 1005   score: 1.0   memory length: 181797   epsilon: 0.838039960003516    steps: 151    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 1006   score: 0.0   memory length: 181920   epsilon: 0.8377964200035213    steps: 123    lr: 0.0001     evaluation reward: 1.19\n",
            "episode: 1007   score: 3.0   memory length: 182167   epsilon: 0.8373073600035319    steps: 247    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 1008   score: 1.0   memory length: 182336   epsilon: 0.8369727400035392    steps: 169    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 1009   score: 1.0   memory length: 182507   epsilon: 0.8366341600035465    steps: 171    lr: 0.0001     evaluation reward: 1.18\n",
            "episode: 1010   score: 3.0   memory length: 182754   epsilon: 0.8361451000035571    steps: 247    lr: 0.0001     evaluation reward: 1.19\n",
            "episode: 1011   score: 1.0   memory length: 182923   epsilon: 0.8358104800035644    steps: 169    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 1012   score: 1.0   memory length: 183093   epsilon: 0.8354738800035717    steps: 170    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 1013   score: 1.0   memory length: 183265   epsilon: 0.8351333200035791    steps: 172    lr: 0.0001     evaluation reward: 1.19\n",
            "episode: 1014   score: 2.0   memory length: 183482   epsilon: 0.8347036600035884    steps: 217    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 1015   score: 3.0   memory length: 183713   epsilon: 0.8342462800035984    steps: 231    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 1016   score: 4.0   memory length: 184009   epsilon: 0.8336602000036111    steps: 296    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 1017   score: 2.0   memory length: 184207   epsilon: 0.8332681600036196    steps: 198    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 1018   score: 3.0   memory length: 184417   epsilon: 0.8328523600036286    steps: 210    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 1019   score: 2.0   memory length: 184614   epsilon: 0.8324623000036371    steps: 197    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1020   score: 1.0   memory length: 184782   epsilon: 0.8321296600036443    steps: 168    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1021   score: 5.0   memory length: 185074   epsilon: 0.8315515000036569    steps: 292    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1022   score: 0.0   memory length: 185197   epsilon: 0.8313079600036621    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1023   score: 2.0   memory length: 185416   epsilon: 0.8308743400036716    steps: 219    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1024   score: 0.0   memory length: 185538   epsilon: 0.8306327800036768    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1025   score: 2.0   memory length: 185736   epsilon: 0.8302407400036853    steps: 198    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1026   score: 1.0   memory length: 185887   epsilon: 0.8299417600036918    steps: 151    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1027   score: 0.0   memory length: 186010   epsilon: 0.8296982200036971    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1028   score: 2.0   memory length: 186229   epsilon: 0.8292646000037065    steps: 219    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1029   score: 2.0   memory length: 186427   epsilon: 0.828872560003715    steps: 198    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1030   score: 4.0   memory length: 186722   epsilon: 0.8282884600037277    steps: 295    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1031   score: 3.0   memory length: 186949   epsilon: 0.8278390000037374    steps: 227    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1032   score: 0.0   memory length: 187072   epsilon: 0.8275954600037427    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1033   score: 1.0   memory length: 187242   epsilon: 0.82725886000375    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1034   score: 2.0   memory length: 187440   epsilon: 0.8268668200037586    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1035   score: 3.0   memory length: 187688   epsilon: 0.8263757800037692    steps: 248    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1036   score: 7.0   memory length: 188113   epsilon: 0.8255342800037875    steps: 425    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 1037   score: 0.0   memory length: 188236   epsilon: 0.8252907400037928    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1038   score: 3.0   memory length: 188465   epsilon: 0.8248373200038026    steps: 229    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1039   score: 0.0   memory length: 188587   epsilon: 0.8245957600038079    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1040   score: 1.0   memory length: 188738   epsilon: 0.8242967800038143    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1041   score: 2.0   memory length: 188957   epsilon: 0.8238631600038238    steps: 219    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1042   score: 4.0   memory length: 189275   epsilon: 0.8232335200038374    steps: 318    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1043   score: 2.0   memory length: 189495   epsilon: 0.8227979200038469    steps: 220    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1044   score: 2.0   memory length: 189713   epsilon: 0.8223662800038563    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1045   score: 2.0   memory length: 189911   epsilon: 0.8219742400038648    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1046   score: 3.0   memory length: 190177   epsilon: 0.8214475600038762    steps: 266    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1047   score: 1.0   memory length: 190328   epsilon: 0.8211485800038827    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1048   score: 1.0   memory length: 190498   epsilon: 0.82081198000389    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1049   score: 0.0   memory length: 190620   epsilon: 0.8205704200038952    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1050   score: 2.0   memory length: 190818   epsilon: 0.8201783800039038    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1051   score: 1.0   memory length: 190969   epsilon: 0.8198794000039102    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1052   score: 1.0   memory length: 191119   epsilon: 0.8195824000039167    steps: 150    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1053   score: 0.0   memory length: 191242   epsilon: 0.819338860003922    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1054   score: 2.0   memory length: 191457   epsilon: 0.8189131600039312    steps: 215    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1055   score: 1.0   memory length: 191626   epsilon: 0.8185785400039385    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1056   score: 0.0   memory length: 191748   epsilon: 0.8183369800039437    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1057   score: 1.0   memory length: 191899   epsilon: 0.8180380000039502    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1058   score: 0.0   memory length: 192022   epsilon: 0.8177944600039555    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1059   score: 0.0   memory length: 192145   epsilon: 0.8175509200039608    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1060   score: 1.0   memory length: 192296   epsilon: 0.8172519400039673    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1061   score: 1.0   memory length: 192467   epsilon: 0.8169133600039746    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1062   score: 1.0   memory length: 192636   epsilon: 0.8165787400039819    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1063   score: 4.0   memory length: 192912   epsilon: 0.8160322600039938    steps: 276    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1064   score: 0.0   memory length: 193035   epsilon: 0.815788720003999    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1065   score: 0.0   memory length: 193158   epsilon: 0.8155451800040043    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1066   score: 0.0   memory length: 193281   epsilon: 0.8153016400040096    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1067   score: 4.0   memory length: 193552   epsilon: 0.8147650600040213    steps: 271    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1068   score: 2.0   memory length: 193750   epsilon: 0.8143730200040298    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1069   score: 1.0   memory length: 193919   epsilon: 0.814038400004037    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1070   score: 0.0   memory length: 194042   epsilon: 0.8137948600040423    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1071   score: 3.0   memory length: 194307   epsilon: 0.8132701600040537    steps: 265    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1072   score: 1.0   memory length: 194475   epsilon: 0.8129375200040609    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1073   score: 2.0   memory length: 194695   epsilon: 0.8125019200040704    steps: 220    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1074   score: 1.0   memory length: 194867   epsilon: 0.8121613600040778    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1075   score: 2.0   memory length: 195082   epsilon: 0.811735660004087    steps: 215    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1076   score: 2.0   memory length: 195300   epsilon: 0.8113040200040964    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1077   score: 0.0   memory length: 195423   epsilon: 0.8110604800041017    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1078   score: 2.0   memory length: 195621   epsilon: 0.8106684400041102    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1079   score: 1.0   memory length: 195790   epsilon: 0.8103338200041175    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1080   score: 1.0   memory length: 195960   epsilon: 0.8099972200041248    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1081   score: 5.0   memory length: 196262   epsilon: 0.8093992600041378    steps: 302    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 1082   score: 6.0   memory length: 196636   epsilon: 0.8086587400041538    steps: 374    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 1083   score: 0.0   memory length: 196759   epsilon: 0.8084152000041591    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 1084   score: 0.0   memory length: 196882   epsilon: 0.8081716600041644    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 1085   score: 1.0   memory length: 197033   epsilon: 0.8078726800041709    steps: 151    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 1086   score: 2.0   memory length: 197251   epsilon: 0.8074410400041803    steps: 218    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 1087   score: 2.0   memory length: 197469   epsilon: 0.8070094000041896    steps: 218    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 1088   score: 1.0   memory length: 197640   epsilon: 0.806670820004197    steps: 171    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 1089   score: 0.0   memory length: 197763   epsilon: 0.8064272800042023    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 1090   score: 1.0   memory length: 197935   epsilon: 0.8060867200042097    steps: 172    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 1091   score: 1.0   memory length: 198087   epsilon: 0.8057857600042162    steps: 152    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 1092   score: 0.0   memory length: 198209   epsilon: 0.8055442000042214    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 1093   score: 3.0   memory length: 198455   epsilon: 0.805057120004232    steps: 246    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 1094   score: 1.0   memory length: 198627   epsilon: 0.8047165600042394    steps: 172    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1095   score: 0.0   memory length: 198749   epsilon: 0.8044750000042447    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1096   score: 2.0   memory length: 198970   epsilon: 0.8040374200042542    steps: 221    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1097   score: 4.0   memory length: 199280   epsilon: 0.8034236200042675    steps: 310    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1098   score: 0.0   memory length: 199403   epsilon: 0.8031800800042728    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1099   score: 0.0   memory length: 199525   epsilon: 0.802938520004278    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1100   score: 4.0   memory length: 199820   epsilon: 0.8023544200042907    steps: 295    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 1101   score: 3.0   memory length: 200064   epsilon: 0.8018713000043012    steps: 244    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1102   score: 2.0   memory length: 200262   epsilon: 0.8014792600043097    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1103   score: 0.0   memory length: 200384   epsilon: 0.8012377000043149    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1104   score: 2.0   memory length: 200600   epsilon: 0.8008100200043242    steps: 216    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 1105   score: 0.0   memory length: 200723   epsilon: 0.8005664800043295    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1106   score: 0.0   memory length: 200845   epsilon: 0.8003249200043347    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1107   score: 2.0   memory length: 201042   epsilon: 0.7999348600043432    steps: 197    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1108   score: 1.0   memory length: 201211   epsilon: 0.7996002400043505    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1109   score: 0.0   memory length: 201334   epsilon: 0.7993567000043558    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1110   score: 3.0   memory length: 201602   epsilon: 0.7988260600043673    steps: 268    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1111   score: 0.0   memory length: 201724   epsilon: 0.7985845000043725    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 1112   score: 2.0   memory length: 201921   epsilon: 0.798194440004381    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1113   score: 0.0   memory length: 202043   epsilon: 0.7979528800043862    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 1114   score: 1.0   memory length: 202211   epsilon: 0.7976202400043935    steps: 168    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1115   score: 8.0   memory length: 202620   epsilon: 0.796810420004411    steps: 409    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 1116   score: 0.0   memory length: 202743   epsilon: 0.7965668800044163    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 1117   score: 1.0   memory length: 202915   epsilon: 0.7962263200044237    steps: 172    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1118   score: 1.0   memory length: 203066   epsilon: 0.7959273400044302    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1119   score: 2.0   memory length: 203263   epsilon: 0.7955372800044387    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1120   score: 3.0   memory length: 203528   epsilon: 0.7950125800044501    steps: 265    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1121   score: 2.0   memory length: 203726   epsilon: 0.7946205400044586    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1122   score: 2.0   memory length: 203944   epsilon: 0.794188900004468    steps: 218    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1123   score: 5.0   memory length: 204266   epsilon: 0.7935513400044818    steps: 322    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1124   score: 1.0   memory length: 204416   epsilon: 0.7932543400044882    steps: 150    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1125   score: 3.0   memory length: 204663   epsilon: 0.7927652800044989    steps: 247    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1126   score: 0.0   memory length: 204786   epsilon: 0.7925217400045041    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1127   score: 5.0   memory length: 205129   epsilon: 0.7918426000045189    steps: 343    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 1128   score: 1.0   memory length: 205300   epsilon: 0.7915040200045262    steps: 171    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 1129   score: 0.0   memory length: 205423   epsilon: 0.7912604800045315    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 1130   score: 2.0   memory length: 205642   epsilon: 0.7908268600045409    steps: 219    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1131   score: 2.0   memory length: 205839   epsilon: 0.7904368000045494    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1132   score: 0.0   memory length: 205962   epsilon: 0.7901932600045547    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1133   score: 2.0   memory length: 206144   epsilon: 0.7898329000045625    steps: 182    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1134   score: 1.0   memory length: 206313   epsilon: 0.7894982800045698    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1135   score: 3.0   memory length: 206576   epsilon: 0.7889775400045811    steps: 263    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1136   score: 1.0   memory length: 206727   epsilon: 0.7886785600045876    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1137   score: 1.0   memory length: 206897   epsilon: 0.7883419600045949    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1138   score: 0.0   memory length: 207020   epsilon: 0.7880984200046002    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1139   score: 2.0   memory length: 207200   epsilon: 0.7877420200046079    steps: 180    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1140   score: 2.0   memory length: 207420   epsilon: 0.7873064200046174    steps: 220    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1141   score: 3.0   memory length: 207668   epsilon: 0.786815380004628    steps: 248    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1142   score: 1.0   memory length: 207840   epsilon: 0.7864748200046354    steps: 172    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1143   score: 3.0   memory length: 208085   epsilon: 0.785989720004646    steps: 245    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1144   score: 0.0   memory length: 208208   epsilon: 0.7857461800046512    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1145   score: 1.0   memory length: 208358   epsilon: 0.7854491800046577    steps: 150    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1146   score: 1.0   memory length: 208509   epsilon: 0.7851502000046642    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1147   score: 2.0   memory length: 208707   epsilon: 0.7847581600046727    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1148   score: 3.0   memory length: 208935   epsilon: 0.7843067200046825    steps: 228    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1149   score: 3.0   memory length: 209162   epsilon: 0.7838572600046922    steps: 227    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1150   score: 1.0   memory length: 209333   epsilon: 0.7835186800046996    steps: 171    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1151   score: 1.0   memory length: 209484   epsilon: 0.7832197000047061    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1152   score: 4.0   memory length: 209742   epsilon: 0.7827088600047172    steps: 258    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1153   score: 3.0   memory length: 209989   epsilon: 0.7822198000047278    steps: 247    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1154   score: 0.0   memory length: 210111   epsilon: 0.781978240004733    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1155   score: 2.0   memory length: 210328   epsilon: 0.7815485800047424    steps: 217    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 1156   score: 1.0   memory length: 210500   epsilon: 0.7812080200047498    steps: 172    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1157   score: 1.0   memory length: 210651   epsilon: 0.7809090400047562    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1158   score: 2.0   memory length: 210871   epsilon: 0.7804734400047657    steps: 220    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1159   score: 2.0   memory length: 211068   epsilon: 0.7800833800047742    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 1160   score: 0.0   memory length: 211190   epsilon: 0.7798418200047794    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 1161   score: 1.0   memory length: 211362   epsilon: 0.7795012600047868    steps: 172    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 1162   score: 0.0   memory length: 211485   epsilon: 0.7792577200047921    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1163   score: 0.0   memory length: 211607   epsilon: 0.7790161600047973    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1164   score: 2.0   memory length: 211828   epsilon: 0.7785785800048068    steps: 221    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1165   score: 2.0   memory length: 212026   epsilon: 0.7781865400048154    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1166   score: 4.0   memory length: 212319   epsilon: 0.777606400004828    steps: 293    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 1167   score: 0.0   memory length: 212442   epsilon: 0.7773628600048332    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1168   score: 3.0   memory length: 212689   epsilon: 0.7768738000048439    steps: 247    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 1169   score: 0.0   memory length: 212812   epsilon: 0.7766302600048491    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1170   score: 2.0   memory length: 213009   epsilon: 0.7762402000048576    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 1171   score: 1.0   memory length: 213159   epsilon: 0.775943200004864    steps: 150    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1172   score: 1.0   memory length: 213331   epsilon: 0.7756026400048714    steps: 172    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1173   score: 3.0   memory length: 213559   epsilon: 0.7751512000048812    steps: 228    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 1174   score: 0.0   memory length: 213682   epsilon: 0.7749076600048865    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1175   score: 3.0   memory length: 213930   epsilon: 0.7744166200048972    steps: 248    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 1176   score: 1.0   memory length: 214098   epsilon: 0.7740839800049044    steps: 168    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1177   score: 2.0   memory length: 214295   epsilon: 0.7736939200049129    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 1178   score: 2.0   memory length: 214492   epsilon: 0.7733038600049214    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 1179   score: 1.0   memory length: 214661   epsilon: 0.7729692400049286    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 1180   score: 2.0   memory length: 214878   epsilon: 0.7725395800049379    steps: 217    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 1181   score: 1.0   memory length: 215029   epsilon: 0.7722406000049444    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1182   score: 2.0   memory length: 215250   epsilon: 0.7718030200049539    steps: 221    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1183   score: 2.0   memory length: 215448   epsilon: 0.7714109800049624    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 1184   score: 3.0   memory length: 215676   epsilon: 0.7709595400049722    steps: 228    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1185   score: 1.0   memory length: 215827   epsilon: 0.7706605600049787    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1186   score: 2.0   memory length: 216027   epsilon: 0.7702645600049873    steps: 200    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1187   score: 2.0   memory length: 216225   epsilon: 0.7698725200049958    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1188   score: 1.0   memory length: 216376   epsilon: 0.7695735400050023    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1189   score: 0.0   memory length: 216498   epsilon: 0.7693319800050076    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1190   score: 0.0   memory length: 216620   epsilon: 0.7690904200050128    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1191   score: 0.0   memory length: 216742   epsilon: 0.7688488600050181    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1192   score: 1.0   memory length: 216913   epsilon: 0.7685102800050254    steps: 171    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1193   score: 3.0   memory length: 217179   epsilon: 0.7679836000050368    steps: 266    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1194   score: 2.0   memory length: 217377   epsilon: 0.7675915600050454    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1195   score: 2.0   memory length: 217595   epsilon: 0.7671599200050547    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 1196   score: 3.0   memory length: 217827   epsilon: 0.7667005600050647    steps: 232    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 1197   score: 2.0   memory length: 218048   epsilon: 0.7662629800050742    steps: 221    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 1198   score: 5.0   memory length: 218393   epsilon: 0.765579880005089    steps: 345    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 1199   score: 1.0   memory length: 218544   epsilon: 0.7652809000050955    steps: 151    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 1200   score: 2.0   memory length: 218744   epsilon: 0.7648849000051041    steps: 200    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 1201   score: 2.0   memory length: 218941   epsilon: 0.7644948400051126    steps: 197    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 1202   score: 0.0   memory length: 219064   epsilon: 0.7642513000051179    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 1203   score: 0.0   memory length: 219186   epsilon: 0.7640097400051231    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 1204   score: 1.0   memory length: 219336   epsilon: 0.7637127400051296    steps: 150    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 1205   score: 2.0   memory length: 219555   epsilon: 0.763279120005139    steps: 219    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 1206   score: 2.0   memory length: 219773   epsilon: 0.7628474800051483    steps: 218    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 1207   score: 2.0   memory length: 219993   epsilon: 0.7624118800051578    steps: 220    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 1208   score: 2.0   memory length: 220191   epsilon: 0.7620198400051663    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 1209   score: 0.0   memory length: 220314   epsilon: 0.7617763000051716    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 1210   score: 2.0   memory length: 220515   epsilon: 0.7613783200051802    steps: 201    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 1211   score: 0.0   memory length: 220637   epsilon: 0.7611367600051855    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 1212   score: 3.0   memory length: 220882   epsilon: 0.760651660005196    steps: 245    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 1213   score: 0.0   memory length: 221005   epsilon: 0.7604081200052013    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 1214   score: 0.0   memory length: 221128   epsilon: 0.7601645800052066    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 1215   score: 1.0   memory length: 221297   epsilon: 0.7598299600052139    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1216   score: 2.0   memory length: 221500   epsilon: 0.7594280200052226    steps: 203    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1217   score: 1.0   memory length: 221670   epsilon: 0.7590914200052299    steps: 170    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1218   score: 1.0   memory length: 221842   epsilon: 0.7587508600052373    steps: 172    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1219   score: 2.0   memory length: 222039   epsilon: 0.7583608000052457    steps: 197    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1220   score: 2.0   memory length: 222237   epsilon: 0.7579687600052543    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1221   score: 6.0   memory length: 222579   epsilon: 0.757291600005269    steps: 342    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 1222   score: 2.0   memory length: 222777   epsilon: 0.7568995600052775    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 1223   score: 2.0   memory length: 222975   epsilon: 0.756507520005286    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1224   score: 3.0   memory length: 223222   epsilon: 0.7560184600052966    steps: 247    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 1225   score: 0.0   memory length: 223345   epsilon: 0.7557749200053019    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1226   score: 0.0   memory length: 223468   epsilon: 0.7555313800053072    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1227   score: 1.0   memory length: 223640   epsilon: 0.7551908200053146    steps: 172    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1228   score: 1.0   memory length: 223809   epsilon: 0.7548562000053218    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1229   score: 0.0   memory length: 223931   epsilon: 0.7546146400053271    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1230   score: 4.0   memory length: 224193   epsilon: 0.7540958800053383    steps: 262    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 1231   score: 0.0   memory length: 224315   epsilon: 0.7538543200053436    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1232   score: 1.0   memory length: 224485   epsilon: 0.7535177200053509    steps: 170    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1233   score: 2.0   memory length: 224704   epsilon: 0.7530841000053603    steps: 219    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1234   score: 0.0   memory length: 224827   epsilon: 0.7528405600053656    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1235   score: 2.0   memory length: 225043   epsilon: 0.7524128800053749    steps: 216    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1236   score: 0.0   memory length: 225165   epsilon: 0.7521713200053801    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1237   score: 1.0   memory length: 225315   epsilon: 0.7518743200053866    steps: 150    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1238   score: 1.0   memory length: 225486   epsilon: 0.7515357400053939    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1239   score: 4.0   memory length: 225759   epsilon: 0.7509952000054056    steps: 273    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1240   score: 0.0   memory length: 225882   epsilon: 0.7507516600054109    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1241   score: 3.0   memory length: 226131   epsilon: 0.7502586400054216    steps: 249    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1242   score: 3.0   memory length: 226361   epsilon: 0.7498032400054315    steps: 230    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1243   score: 1.0   memory length: 226512   epsilon: 0.749504260005438    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1244   score: 4.0   memory length: 226789   epsilon: 0.7489558000054499    steps: 277    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1245   score: 1.0   memory length: 226959   epsilon: 0.7486192000054572    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1246   score: 3.0   memory length: 227208   epsilon: 0.7481261800054679    steps: 249    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1247   score: 1.0   memory length: 227377   epsilon: 0.7477915600054752    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1248   score: 1.0   memory length: 227547   epsilon: 0.7474549600054825    steps: 170    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 1249   score: 0.0   memory length: 227670   epsilon: 0.7472114200054878    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1250   score: 2.0   memory length: 227867   epsilon: 0.7468213600054963    steps: 197    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1251   score: 2.0   memory length: 228084   epsilon: 0.7463917000055056    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1252   score: 4.0   memory length: 228360   epsilon: 0.7458452200055175    steps: 276    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1253   score: 3.0   memory length: 228606   epsilon: 0.745358140005528    steps: 246    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1254   score: 0.0   memory length: 228729   epsilon: 0.7451146000055333    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1255   score: 1.0   memory length: 228898   epsilon: 0.7447799800055406    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1256   score: 0.0   memory length: 229021   epsilon: 0.7445364400055459    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1257   score: 2.0   memory length: 229219   epsilon: 0.7441444000055544    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1258   score: 1.0   memory length: 229388   epsilon: 0.7438097800055616    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1259   score: 2.0   memory length: 229585   epsilon: 0.7434197200055701    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1260   score: 0.0   memory length: 229708   epsilon: 0.7431761800055754    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1261   score: 3.0   memory length: 229941   epsilon: 0.7427148400055854    steps: 233    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1262   score: 3.0   memory length: 230190   epsilon: 0.7422218200055961    steps: 249    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1263   score: 0.0   memory length: 230313   epsilon: 0.7419782800056014    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1264   score: 2.0   memory length: 230530   epsilon: 0.7415486200056107    steps: 217    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1265   score: 1.0   memory length: 230699   epsilon: 0.741214000005618    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1266   score: 3.0   memory length: 230967   epsilon: 0.7406833600056295    steps: 268    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 1267   score: 1.0   memory length: 231137   epsilon: 0.7403467600056368    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1268   score: 0.0   memory length: 231259   epsilon: 0.7401052000056421    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1269   score: 2.0   memory length: 231477   epsilon: 0.7396735600056514    steps: 218    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 1270   score: 1.0   memory length: 231629   epsilon: 0.739372600005658    steps: 152    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1271   score: 1.0   memory length: 231781   epsilon: 0.7390716400056645    steps: 152    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1272   score: 0.0   memory length: 231904   epsilon: 0.7388281000056698    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1273   score: 2.0   memory length: 232122   epsilon: 0.7383964600056792    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1274   score: 1.0   memory length: 232291   epsilon: 0.7380618400056864    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1275   score: 1.0   memory length: 232461   epsilon: 0.7377252400056937    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1276   score: 2.0   memory length: 232679   epsilon: 0.7372936000057031    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1277   score: 0.0   memory length: 232802   epsilon: 0.7370500600057084    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1278   score: 2.0   memory length: 232999   epsilon: 0.7366600000057169    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1279   score: 0.0   memory length: 233121   epsilon: 0.7364184400057221    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1280   score: 2.0   memory length: 233319   epsilon: 0.7360264000057306    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1281   score: 0.0   memory length: 233442   epsilon: 0.7357828600057359    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1282   score: 1.0   memory length: 233612   epsilon: 0.7354462600057432    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1283   score: 0.0   memory length: 233734   epsilon: 0.7352047000057484    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1284   score: 2.0   memory length: 233931   epsilon: 0.7348146400057569    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1285   score: 1.0   memory length: 234099   epsilon: 0.7344820000057641    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1286   score: 1.0   memory length: 234267   epsilon: 0.7341493600057714    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1287   score: 2.0   memory length: 234483   epsilon: 0.7337216800057806    steps: 216    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1288   score: 3.0   memory length: 234747   epsilon: 0.733198960005792    steps: 264    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1289   score: 2.0   memory length: 234964   epsilon: 0.7327693000058013    steps: 217    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1290   score: 5.0   memory length: 235309   epsilon: 0.7320862000058161    steps: 345    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1291   score: 0.0   memory length: 235432   epsilon: 0.7318426600058214    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1292   score: 1.0   memory length: 235602   epsilon: 0.7315060600058287    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1293   score: 0.0   memory length: 235725   epsilon: 0.731262520005834    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1294   score: 4.0   memory length: 236003   epsilon: 0.730712080005846    steps: 278    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1295   score: 1.0   memory length: 236171   epsilon: 0.7303794400058532    steps: 168    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1296   score: 4.0   memory length: 236467   epsilon: 0.7297933600058659    steps: 296    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1297   score: 0.0   memory length: 236590   epsilon: 0.7295498200058712    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1298   score: 2.0   memory length: 236787   epsilon: 0.7291597600058797    steps: 197    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1299   score: 2.0   memory length: 237004   epsilon: 0.728730100005889    steps: 217    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1300   score: 3.0   memory length: 237251   epsilon: 0.7282410400058996    steps: 247    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1301   score: 0.0   memory length: 237373   epsilon: 0.7279994800059049    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1302   score: 2.0   memory length: 237570   epsilon: 0.7276094200059133    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1303   score: 1.0   memory length: 237720   epsilon: 0.7273124200059198    steps: 150    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1304   score: 3.0   memory length: 237968   epsilon: 0.7268213800059304    steps: 248    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1305   score: 3.0   memory length: 238214   epsilon: 0.726334300005941    steps: 246    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1306   score: 3.0   memory length: 238462   epsilon: 0.7258432600059517    steps: 248    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1307   score: 0.0   memory length: 238584   epsilon: 0.7256017000059569    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1308   score: 1.0   memory length: 238752   epsilon: 0.7252690600059641    steps: 168    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1309   score: 1.0   memory length: 238920   epsilon: 0.7249364200059714    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1310   score: 0.0   memory length: 239042   epsilon: 0.7246948600059766    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1311   score: 2.0   memory length: 239240   epsilon: 0.7243028200059851    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1312   score: 0.0   memory length: 239363   epsilon: 0.7240592800059904    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1313   score: 0.0   memory length: 239486   epsilon: 0.7238157400059957    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1314   score: 1.0   memory length: 239655   epsilon: 0.723481120006003    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1315   score: 1.0   memory length: 239806   epsilon: 0.7231821400060094    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1316   score: 1.0   memory length: 239977   epsilon: 0.7228435600060168    steps: 171    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1317   score: 1.0   memory length: 240147   epsilon: 0.7225069600060241    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1318   score: 3.0   memory length: 240373   epsilon: 0.7220594800060338    steps: 226    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1319   score: 2.0   memory length: 240571   epsilon: 0.7216674400060423    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1320   score: 0.0   memory length: 240694   epsilon: 0.7214239000060476    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1321   score: 1.0   memory length: 240844   epsilon: 0.7211269000060541    steps: 150    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1322   score: 0.0   memory length: 240967   epsilon: 0.7208833600060593    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1323   score: 3.0   memory length: 241211   epsilon: 0.7204002400060698    steps: 244    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1324   score: 2.0   memory length: 241429   epsilon: 0.7199686000060792    steps: 218    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1325   score: 3.0   memory length: 241659   epsilon: 0.7195132000060891    steps: 230    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1326   score: 0.0   memory length: 241781   epsilon: 0.7192716400060943    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1327   score: 0.0   memory length: 241904   epsilon: 0.7190281000060996    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1328   score: 0.0   memory length: 242027   epsilon: 0.7187845600061049    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1329   score: 2.0   memory length: 242245   epsilon: 0.7183529200061143    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1330   score: 3.0   memory length: 242491   epsilon: 0.7178658400061249    steps: 246    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1331   score: 1.0   memory length: 242660   epsilon: 0.7175312200061321    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1332   score: 1.0   memory length: 242830   epsilon: 0.7171946200061394    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1333   score: 3.0   memory length: 243057   epsilon: 0.7167451600061492    steps: 227    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1334   score: 0.0   memory length: 243179   epsilon: 0.7165036000061544    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1335   score: 1.0   memory length: 243349   epsilon: 0.7161670000061617    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1336   score: 1.0   memory length: 243499   epsilon: 0.7158700000061682    steps: 150    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1337   score: 0.0   memory length: 243622   epsilon: 0.7156264600061735    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1338   score: 0.0   memory length: 243744   epsilon: 0.7153849000061787    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1339   score: 0.0   memory length: 243867   epsilon: 0.715141360006184    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 1340   score: 1.0   memory length: 244036   epsilon: 0.7148067400061913    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1341   score: 0.0   memory length: 244159   epsilon: 0.7145632000061966    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 1342   score: 0.0   memory length: 244281   epsilon: 0.7143216400062018    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1343   score: 3.0   memory length: 244529   epsilon: 0.7138306000062125    steps: 248    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1344   score: 1.0   memory length: 244698   epsilon: 0.7134959800062197    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1345   score: 2.0   memory length: 244896   epsilon: 0.7131039400062282    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1346   score: 0.0   memory length: 245018   epsilon: 0.7128623800062335    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1347   score: 2.0   memory length: 245216   epsilon: 0.712470340006242    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1348   score: 3.0   memory length: 245483   epsilon: 0.7119416800062535    steps: 267    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1349   score: 2.0   memory length: 245680   epsilon: 0.7115516200062619    steps: 197    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1350   score: 0.0   memory length: 245803   epsilon: 0.7113080800062672    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1351   score: 0.0   memory length: 245926   epsilon: 0.7110645400062725    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1352   score: 2.0   memory length: 246144   epsilon: 0.7106329000062819    steps: 218    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1353   score: 0.0   memory length: 246267   epsilon: 0.7103893600062872    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1354   score: 1.0   memory length: 246437   epsilon: 0.7100527600062945    steps: 170    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1355   score: 2.0   memory length: 246638   epsilon: 0.7096547800063031    steps: 201    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1356   score: 0.0   memory length: 246761   epsilon: 0.7094112400063084    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1357   score: 3.0   memory length: 246989   epsilon: 0.7089598000063182    steps: 228    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1358   score: 1.0   memory length: 247159   epsilon: 0.7086232000063255    steps: 170    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1359   score: 4.0   memory length: 247435   epsilon: 0.7080767200063374    steps: 276    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1360   score: 2.0   memory length: 247633   epsilon: 0.7076846800063459    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1361   score: 1.0   memory length: 247802   epsilon: 0.7073500600063531    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1362   score: 1.0   memory length: 247973   epsilon: 0.7070114800063605    steps: 171    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1363   score: 1.0   memory length: 248124   epsilon: 0.706712500006367    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1364   score: 4.0   memory length: 248398   epsilon: 0.7061699800063788    steps: 274    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1365   score: 2.0   memory length: 248601   epsilon: 0.7057680400063875    steps: 203    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1366   score: 3.0   memory length: 248847   epsilon: 0.7052809600063981    steps: 246    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1367   score: 2.0   memory length: 249063   epsilon: 0.7048532800064073    steps: 216    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1368   score: 0.0   memory length: 249186   epsilon: 0.7046097400064126    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1369   score: 2.0   memory length: 249386   epsilon: 0.7042137400064212    steps: 200    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1370   score: 2.0   memory length: 249584   epsilon: 0.7038217000064297    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1371   score: 3.0   memory length: 249811   epsilon: 0.7033722400064395    steps: 227    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1372   score: 3.0   memory length: 250039   epsilon: 0.7029208000064493    steps: 228    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1373   score: 1.0   memory length: 250207   epsilon: 0.7025881600064565    steps: 168    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1374   score: 2.0   memory length: 250424   epsilon: 0.7021585000064658    steps: 217    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1375   score: 1.0   memory length: 250575   epsilon: 0.7018595200064723    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1376   score: 2.0   memory length: 250794   epsilon: 0.7014259000064818    steps: 219    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1377   score: 1.0   memory length: 250965   epsilon: 0.7010873200064891    steps: 171    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1378   score: 1.0   memory length: 251115   epsilon: 0.7007903200064955    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1379   score: 1.0   memory length: 251284   epsilon: 0.7004557000065028    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1380   score: 4.0   memory length: 251578   epsilon: 0.6998735800065154    steps: 294    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1381   score: 2.0   memory length: 251776   epsilon: 0.699481540006524    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1382   score: 2.0   memory length: 251973   epsilon: 0.6990914800065324    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1383   score: 0.0   memory length: 252096   epsilon: 0.6988479400065377    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1384   score: 0.0   memory length: 252219   epsilon: 0.698604400006543    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1385   score: 3.0   memory length: 252445   epsilon: 0.6981569200065527    steps: 226    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1386   score: 1.0   memory length: 252615   epsilon: 0.69782032000656    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1387   score: 1.0   memory length: 252766   epsilon: 0.6975213400065665    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1388   score: 4.0   memory length: 253078   epsilon: 0.6969035800065799    steps: 312    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1389   score: 0.0   memory length: 253201   epsilon: 0.6966600400065852    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1390   score: 0.0   memory length: 253324   epsilon: 0.6964165000065905    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1391   score: 0.0   memory length: 253447   epsilon: 0.6961729600065958    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1392   score: 2.0   memory length: 253666   epsilon: 0.6957393400066052    steps: 219    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1393   score: 0.0   memory length: 253789   epsilon: 0.6954958000066105    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1394   score: 0.0   memory length: 253911   epsilon: 0.6952542400066157    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 1395   score: 0.0   memory length: 254034   epsilon: 0.695010700006621    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1396   score: 2.0   memory length: 254215   epsilon: 0.6946523200066288    steps: 181    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1397   score: 2.0   memory length: 254434   epsilon: 0.6942187000066382    steps: 219    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1398   score: 0.0   memory length: 254556   epsilon: 0.6939771400066435    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1399   score: 3.0   memory length: 254822   epsilon: 0.6934504600066549    steps: 266    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1400   score: 1.0   memory length: 254973   epsilon: 0.6931514800066614    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1401   score: 2.0   memory length: 255173   epsilon: 0.69275548000667    steps: 200    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1402   score: 3.0   memory length: 255421   epsilon: 0.6922644400066806    steps: 248    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1403   score: 2.0   memory length: 255622   epsilon: 0.6918664600066893    steps: 201    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 1404   score: 0.0   memory length: 255745   epsilon: 0.6916229200066946    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1405   score: 1.0   memory length: 255914   epsilon: 0.6912883000067018    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1406   score: 3.0   memory length: 256159   epsilon: 0.6908032000067124    steps: 245    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1407   score: 3.0   memory length: 256386   epsilon: 0.6903537400067221    steps: 227    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1408   score: 2.0   memory length: 256584   epsilon: 0.6899617000067306    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1409   score: 1.0   memory length: 256753   epsilon: 0.6896270800067379    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1410   score: 2.0   memory length: 256951   epsilon: 0.6892350400067464    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1411   score: 0.0   memory length: 257073   epsilon: 0.6889934800067516    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1412   score: 2.0   memory length: 257270   epsilon: 0.6886034200067601    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1413   score: 0.0   memory length: 257392   epsilon: 0.6883618600067654    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1414   score: 1.0   memory length: 257542   epsilon: 0.6880648600067718    steps: 150    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1415   score: 3.0   memory length: 257789   epsilon: 0.6875758000067824    steps: 247    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1416   score: 2.0   memory length: 258006   epsilon: 0.6871461400067918    steps: 217    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1417   score: 3.0   memory length: 258267   epsilon: 0.686629360006803    steps: 261    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1418   score: 5.0   memory length: 258585   epsilon: 0.6859997200068166    steps: 318    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1419   score: 1.0   memory length: 258736   epsilon: 0.6857007400068231    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1420   score: 1.0   memory length: 258886   epsilon: 0.6854037400068296    steps: 150    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1421   score: 2.0   memory length: 259103   epsilon: 0.6849740800068389    steps: 217    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1422   score: 1.0   memory length: 259272   epsilon: 0.6846394600068462    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1423   score: 1.0   memory length: 259422   epsilon: 0.6843424600068526    steps: 150    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1424   score: 0.0   memory length: 259545   epsilon: 0.6840989200068579    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1425   score: 0.0   memory length: 259667   epsilon: 0.6838573600068631    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1426   score: 1.0   memory length: 259839   epsilon: 0.6835168000068705    steps: 172    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1427   score: 1.0   memory length: 260007   epsilon: 0.6831841600068778    steps: 168    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1428   score: 1.0   memory length: 260158   epsilon: 0.6828851800068843    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1429   score: 1.0   memory length: 260329   epsilon: 0.6825466000068916    steps: 171    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1430   score: 0.0   memory length: 260452   epsilon: 0.6823030600068969    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 1431   score: 1.0   memory length: 260621   epsilon: 0.6819684400069042    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 1432   score: 2.0   memory length: 260838   epsilon: 0.6815387800069135    steps: 217    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1433   score: 2.0   memory length: 261058   epsilon: 0.6811031800069229    steps: 220    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 1434   score: 2.0   memory length: 261278   epsilon: 0.6806675800069324    steps: 220    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1435   score: 3.0   memory length: 261526   epsilon: 0.680176540006943    steps: 248    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1436   score: 3.0   memory length: 261752   epsilon: 0.6797290600069528    steps: 226    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1437   score: 2.0   memory length: 261950   epsilon: 0.6793370200069613    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1438   score: 3.0   memory length: 262196   epsilon: 0.6788499400069719    steps: 246    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1439   score: 2.0   memory length: 262378   epsilon: 0.6784895800069797    steps: 182    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1440   score: 0.0   memory length: 262501   epsilon: 0.678246040006985    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1441   score: 2.0   memory length: 262699   epsilon: 0.6778540000069935    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1442   score: 7.0   memory length: 262964   epsilon: 0.6773293000070049    steps: 265    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 1443   score: 1.0   memory length: 263133   epsilon: 0.6769946800070121    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1444   score: 1.0   memory length: 263302   epsilon: 0.6766600600070194    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1445   score: 0.0   memory length: 263424   epsilon: 0.6764185000070246    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1446   score: 1.0   memory length: 263575   epsilon: 0.6761195200070311    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1447   score: 1.0   memory length: 263744   epsilon: 0.6757849000070384    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1448   score: 1.0   memory length: 263912   epsilon: 0.6754522600070456    steps: 168    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1449   score: 1.0   memory length: 264083   epsilon: 0.675113680007053    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1450   score: 1.0   memory length: 264234   epsilon: 0.6748147000070595    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1451   score: 0.0   memory length: 264357   epsilon: 0.6745711600070647    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1452   score: 4.0   memory length: 264649   epsilon: 0.6739930000070773    steps: 292    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1453   score: 2.0   memory length: 264830   epsilon: 0.6736346200070851    steps: 181    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1454   score: 1.0   memory length: 265000   epsilon: 0.6732980200070924    steps: 170    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1455   score: 0.0   memory length: 265123   epsilon: 0.6730544800070977    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1456   score: 3.0   memory length: 265369   epsilon: 0.6725674000071082    steps: 246    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 1457   score: 1.0   memory length: 265538   epsilon: 0.6722327800071155    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 1458   score: 2.0   memory length: 265738   epsilon: 0.6718367800071241    steps: 200    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 1459   score: 0.0   memory length: 265861   epsilon: 0.6715932400071294    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1460   score: 2.0   memory length: 266061   epsilon: 0.671197240007138    steps: 200    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1461   score: 1.0   memory length: 266230   epsilon: 0.6708626200071452    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1462   score: 0.0   memory length: 266352   epsilon: 0.6706210600071505    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1463   score: 0.0   memory length: 266475   epsilon: 0.6703775200071558    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1464   score: 0.0   memory length: 266597   epsilon: 0.670135960007161    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1465   score: 1.0   memory length: 266748   epsilon: 0.6698369800071675    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1466   score: 1.0   memory length: 266919   epsilon: 0.6694984000071749    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1467   score: 0.0   memory length: 267042   epsilon: 0.6692548600071802    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1468   score: 3.0   memory length: 267291   epsilon: 0.6687618400071909    steps: 249    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1469   score: 0.0   memory length: 267414   epsilon: 0.6685183000071961    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1470   score: 2.0   memory length: 267612   epsilon: 0.6681262600072047    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1471   score: 1.0   memory length: 267763   epsilon: 0.6678272800072111    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1472   score: 0.0   memory length: 267886   epsilon: 0.6675837400072164    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 1473   score: 0.0   memory length: 268008   epsilon: 0.6673421800072217    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1474   score: 0.0   memory length: 268131   epsilon: 0.667098640007227    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1475   score: 1.0   memory length: 268283   epsilon: 0.6667976800072335    steps: 152    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1476   score: 2.0   memory length: 268483   epsilon: 0.6664016800072421    steps: 200    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1477   score: 2.0   memory length: 268699   epsilon: 0.6659740000072514    steps: 216    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 1478   score: 3.0   memory length: 268946   epsilon: 0.665484940007262    steps: 247    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 1479   score: 0.0   memory length: 269069   epsilon: 0.6652414000072673    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1480   score: 0.0   memory length: 269192   epsilon: 0.6649978600072726    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1481   score: 2.0   memory length: 269408   epsilon: 0.6645701800072819    steps: 216    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1482   score: 1.0   memory length: 269559   epsilon: 0.6642712000072883    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1483   score: 1.0   memory length: 269730   epsilon: 0.6639326200072957    steps: 171    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1484   score: 0.0   memory length: 269853   epsilon: 0.663689080007301    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1485   score: 1.0   memory length: 270025   epsilon: 0.6633485200073084    steps: 172    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1486   score: 1.0   memory length: 270176   epsilon: 0.6630495400073149    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1487   score: 1.0   memory length: 270326   epsilon: 0.6627525400073213    steps: 150    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1488   score: 0.0   memory length: 270449   epsilon: 0.6625090000073266    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1489   score: 2.0   memory length: 270647   epsilon: 0.6621169600073351    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1490   score: 1.0   memory length: 270816   epsilon: 0.6617823400073424    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1491   score: 0.0   memory length: 270939   epsilon: 0.6615388000073477    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1492   score: 1.0   memory length: 271109   epsilon: 0.661202200007355    steps: 170    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1493   score: 0.0   memory length: 271231   epsilon: 0.6609606400073602    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1494   score: 0.0   memory length: 271354   epsilon: 0.6607171000073655    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1495   score: 2.0   memory length: 271570   epsilon: 0.6602894200073748    steps: 216    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1496   score: 1.0   memory length: 271721   epsilon: 0.6599904400073813    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1497   score: 1.0   memory length: 271873   epsilon: 0.6596894800073878    steps: 152    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1498   score: 2.0   memory length: 272071   epsilon: 0.6592974400073963    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1499   score: 0.0   memory length: 272194   epsilon: 0.6590539000074016    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1500   score: 0.0   memory length: 272316   epsilon: 0.6588123400074068    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1501   score: 1.0   memory length: 272466   epsilon: 0.6585153400074133    steps: 150    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1502   score: 2.0   memory length: 272667   epsilon: 0.6581173600074219    steps: 201    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1503   score: 3.0   memory length: 272914   epsilon: 0.6576283000074326    steps: 247    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1504   score: 1.0   memory length: 273065   epsilon: 0.657329320007439    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1505   score: 0.0   memory length: 273187   epsilon: 0.6570877600074443    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1506   score: 1.0   memory length: 273338   epsilon: 0.6567887800074508    steps: 151    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 1507   score: 0.0   memory length: 273461   epsilon: 0.6565452400074561    steps: 123    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 1508   score: 3.0   memory length: 273727   epsilon: 0.6560185600074675    steps: 266    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 1509   score: 1.0   memory length: 273896   epsilon: 0.6556839400074748    steps: 169    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 1510   score: 4.0   memory length: 274190   epsilon: 0.6551018200074874    steps: 294    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 1511   score: 4.0   memory length: 274464   epsilon: 0.6545593000074992    steps: 274    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1512   score: 0.0   memory length: 274587   epsilon: 0.6543157600075045    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1513   score: 2.0   memory length: 274784   epsilon: 0.6539257000075129    steps: 197    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1514   score: 2.0   memory length: 274981   epsilon: 0.6535356400075214    steps: 197    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1515   score: 1.0   memory length: 275149   epsilon: 0.6532030000075286    steps: 168    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1516   score: 0.0   memory length: 275271   epsilon: 0.6529614400075339    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1517   score: 2.0   memory length: 275469   epsilon: 0.6525694000075424    steps: 198    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 1518   score: 0.0   memory length: 275592   epsilon: 0.6523258600075477    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 1519   score: 1.0   memory length: 275761   epsilon: 0.6519912400075549    steps: 169    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 1520   score: 1.0   memory length: 275912   epsilon: 0.6516922600075614    steps: 151    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 1521   score: 5.0   memory length: 276215   epsilon: 0.6510923200075744    steps: 303    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 1522   score: 0.0   memory length: 276338   epsilon: 0.6508487800075797    steps: 123    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 1523   score: 4.0   memory length: 276616   epsilon: 0.6502983400075917    steps: 278    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 1524   score: 0.0   memory length: 276738   epsilon: 0.6500567800075969    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 1525   score: 1.0   memory length: 276906   epsilon: 0.6497241400076041    steps: 168    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1526   score: 0.0   memory length: 277029   epsilon: 0.6494806000076094    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 1527   score: 3.0   memory length: 277297   epsilon: 0.648949960007621    steps: 268    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1528   score: 0.0   memory length: 277420   epsilon: 0.6487064200076262    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1529   score: 1.0   memory length: 277588   epsilon: 0.6483737800076335    steps: 168    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1530   score: 4.0   memory length: 277900   epsilon: 0.6477560200076469    steps: 312    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1531   score: 2.0   memory length: 278118   epsilon: 0.6473243800076562    steps: 218    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1532   score: 1.0   memory length: 278289   epsilon: 0.6469858000076636    steps: 171    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1533   score: 1.0   memory length: 278458   epsilon: 0.6466511800076709    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1534   score: 2.0   memory length: 278676   epsilon: 0.6462195400076802    steps: 218    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1535   score: 5.0   memory length: 278995   epsilon: 0.6455879200076939    steps: 319    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1536   score: 3.0   memory length: 279244   epsilon: 0.6450949000077046    steps: 249    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1537   score: 2.0   memory length: 279462   epsilon: 0.644663260007714    steps: 218    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1538   score: 0.0   memory length: 279585   epsilon: 0.6444197200077193    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1539   score: 2.0   memory length: 279785   epsilon: 0.6440237200077279    steps: 200    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1540   score: 2.0   memory length: 280002   epsilon: 0.6435940600077372    steps: 217    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1541   score: 0.0   memory length: 280125   epsilon: 0.6433505200077425    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1542   score: 6.0   memory length: 280480   epsilon: 0.6426476200077578    steps: 355    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1543   score: 1.0   memory length: 280649   epsilon: 0.642313000007765    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1544   score: 0.0   memory length: 280772   epsilon: 0.6420694600077703    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1545   score: 1.0   memory length: 280944   epsilon: 0.6417289000077777    steps: 172    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1546   score: 1.0   memory length: 281094   epsilon: 0.6414319000077842    steps: 150    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1547   score: 2.0   memory length: 281292   epsilon: 0.6410398600077927    steps: 198    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1548   score: 1.0   memory length: 281443   epsilon: 0.6407408800077992    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1549   score: 1.0   memory length: 281612   epsilon: 0.6404062600078064    steps: 169    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1550   score: 0.0   memory length: 281735   epsilon: 0.6401627200078117    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1551   score: 2.0   memory length: 281932   epsilon: 0.6397726600078202    steps: 197    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1552   score: 2.0   memory length: 282147   epsilon: 0.6393469600078294    steps: 215    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1553   score: 0.0   memory length: 282270   epsilon: 0.6391034200078347    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 1554   score: 2.0   memory length: 282488   epsilon: 0.6386717800078441    steps: 218    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1555   score: 1.0   memory length: 282657   epsilon: 0.6383371600078513    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1556   score: 3.0   memory length: 282924   epsilon: 0.6378085000078628    steps: 267    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1557   score: 0.0   memory length: 283047   epsilon: 0.6375649600078681    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1558   score: 0.0   memory length: 283169   epsilon: 0.6373234000078734    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 1559   score: 2.0   memory length: 283371   epsilon: 0.636923440007882    steps: 202    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1560   score: 2.0   memory length: 283589   epsilon: 0.6364918000078914    steps: 218    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1561   score: 1.0   memory length: 283740   epsilon: 0.6361928200078979    steps: 151    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1562   score: 0.0   memory length: 283863   epsilon: 0.6359492800079032    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1563   score: 1.0   memory length: 284031   epsilon: 0.6356166400079104    steps: 168    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1564   score: 0.0   memory length: 284154   epsilon: 0.6353731000079157    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1565   score: 2.0   memory length: 284355   epsilon: 0.6349751200079243    steps: 201    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1566   score: 3.0   memory length: 284583   epsilon: 0.6345236800079341    steps: 228    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1567   score: 2.0   memory length: 284763   epsilon: 0.6341672800079419    steps: 180    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1568   score: 1.0   memory length: 284932   epsilon: 0.6338326600079491    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1569   score: 0.0   memory length: 285055   epsilon: 0.6335891200079544    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1570   score: 2.0   memory length: 285270   epsilon: 0.6331634200079637    steps: 215    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1571   score: 1.0   memory length: 285421   epsilon: 0.6328644400079702    steps: 151    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1572   score: 1.0   memory length: 285593   epsilon: 0.6325238800079775    steps: 172    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1573   score: 1.0   memory length: 285743   epsilon: 0.632226880007984    steps: 150    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1574   score: 1.0   memory length: 285911   epsilon: 0.6318942400079912    steps: 168    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1575   score: 1.0   memory length: 286080   epsilon: 0.6315596200079985    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1576   score: 1.0   memory length: 286251   epsilon: 0.6312210400080058    steps: 171    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1577   score: 0.0   memory length: 286374   epsilon: 0.6309775000080111    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1578   score: 1.0   memory length: 286543   epsilon: 0.6306428800080184    steps: 169    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1579   score: 0.0   memory length: 286666   epsilon: 0.6303993400080237    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1580   score: 1.0   memory length: 286834   epsilon: 0.6300667000080309    steps: 168    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1581   score: 1.0   memory length: 287005   epsilon: 0.6297281200080382    steps: 171    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1582   score: 1.0   memory length: 287173   epsilon: 0.6293954800080455    steps: 168    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1583   score: 0.0   memory length: 287296   epsilon: 0.6291519400080507    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1584   score: 1.0   memory length: 287447   epsilon: 0.6288529600080572    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1585   score: 2.0   memory length: 287645   epsilon: 0.6284609200080657    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1586   score: 1.0   memory length: 287813   epsilon: 0.628128280008073    steps: 168    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1587   score: 1.0   memory length: 287964   epsilon: 0.6278293000080795    steps: 151    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1588   score: 1.0   memory length: 288116   epsilon: 0.627528340008086    steps: 152    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1589   score: 2.0   memory length: 288298   epsilon: 0.6271679800080938    steps: 182    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1590   score: 3.0   memory length: 288511   epsilon: 0.626746240008103    steps: 213    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1591   score: 1.0   memory length: 288680   epsilon: 0.6264116200081102    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1592   score: 2.0   memory length: 288878   epsilon: 0.6260195800081187    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1593   score: 2.0   memory length: 289076   epsilon: 0.6256275400081273    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1594   score: 0.0   memory length: 289199   epsilon: 0.6253840000081325    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1595   score: 1.0   memory length: 289350   epsilon: 0.625085020008139    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1596   score: 4.0   memory length: 289647   epsilon: 0.6244969600081518    steps: 297    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1597   score: 0.0   memory length: 289770   epsilon: 0.6242534200081571    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 1598   score: 1.0   memory length: 289941   epsilon: 0.6239148400081644    steps: 171    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1599   score: 2.0   memory length: 290156   epsilon: 0.6234891400081737    steps: 215    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1600   score: 1.0   memory length: 290307   epsilon: 0.6231901600081802    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 1601   score: 0.0   memory length: 290430   epsilon: 0.6229466200081855    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1602   score: 4.0   memory length: 290723   epsilon: 0.622366480008198    steps: 293    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1603   score: 2.0   memory length: 290902   epsilon: 0.6220120600082057    steps: 179    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 1604   score: 0.0   memory length: 291025   epsilon: 0.621768520008211    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1605   score: 2.0   memory length: 291223   epsilon: 0.6213764800082195    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1606   score: 1.0   memory length: 291393   epsilon: 0.6210398800082269    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1607   score: 2.0   memory length: 291613   epsilon: 0.6206042800082363    steps: 220    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1608   score: 2.0   memory length: 291797   epsilon: 0.6202399600082442    steps: 184    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1609   score: 0.0   memory length: 291920   epsilon: 0.6199964200082495    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1610   score: 1.0   memory length: 292088   epsilon: 0.6196637800082567    steps: 168    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 1611   score: 3.0   memory length: 292333   epsilon: 0.6191786800082673    steps: 245    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1612   score: 2.0   memory length: 292533   epsilon: 0.6187826800082759    steps: 200    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1613   score: 0.0   memory length: 292656   epsilon: 0.6185391400082811    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1614   score: 1.0   memory length: 292825   epsilon: 0.6182045200082884    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1615   score: 1.0   memory length: 292993   epsilon: 0.6178718800082956    steps: 168    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1616   score: 1.0   memory length: 293161   epsilon: 0.6175392400083028    steps: 168    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1617   score: 6.0   memory length: 293527   epsilon: 0.6168145600083186    steps: 366    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1618   score: 0.0   memory length: 293650   epsilon: 0.6165710200083239    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1619   score: 4.0   memory length: 293906   epsilon: 0.6160641400083349    steps: 256    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1620   score: 0.0   memory length: 294028   epsilon: 0.6158225800083401    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1621   score: 1.0   memory length: 294197   epsilon: 0.6154879600083474    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1622   score: 1.0   memory length: 294347   epsilon: 0.6151909600083538    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 1623   score: 0.0   memory length: 294470   epsilon: 0.6149474200083591    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1624   score: 2.0   memory length: 294667   epsilon: 0.6145573600083676    steps: 197    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 1625   score: 2.0   memory length: 294864   epsilon: 0.614167300008376    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1626   score: 2.0   memory length: 295061   epsilon: 0.6137772400083845    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1627   score: 1.0   memory length: 295230   epsilon: 0.6134426200083918    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1628   score: 2.0   memory length: 295428   epsilon: 0.6130505800084003    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1629   score: 1.0   memory length: 295599   epsilon: 0.6127120000084076    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1630   score: 1.0   memory length: 295768   epsilon: 0.6123773800084149    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 1631   score: 8.0   memory length: 296087   epsilon: 0.6117457600084286    steps: 319    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1632   score: 0.0   memory length: 296210   epsilon: 0.6115022200084339    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1633   score: 1.0   memory length: 296378   epsilon: 0.6111695800084411    steps: 168    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1634   score: 1.0   memory length: 296529   epsilon: 0.6108706000084476    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1635   score: 0.0   memory length: 296652   epsilon: 0.6106270600084529    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1636   score: 0.0   memory length: 296775   epsilon: 0.6103835200084582    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1637   score: 3.0   memory length: 297000   epsilon: 0.6099380200084679    steps: 225    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1638   score: 2.0   memory length: 297180   epsilon: 0.6095816200084756    steps: 180    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1639   score: 1.0   memory length: 297330   epsilon: 0.609284620008482    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1640   score: 0.0   memory length: 297452   epsilon: 0.6090430600084873    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1641   score: 1.0   memory length: 297603   epsilon: 0.6087440800084938    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1642   score: 3.0   memory length: 297829   epsilon: 0.6082966000085035    steps: 226    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1643   score: 2.0   memory length: 298048   epsilon: 0.6078629800085129    steps: 219    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1644   score: 3.0   memory length: 298295   epsilon: 0.6073739200085235    steps: 247    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1645   score: 0.0   memory length: 298418   epsilon: 0.6071303800085288    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1646   score: 0.0   memory length: 298541   epsilon: 0.6068868400085341    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1647   score: 0.0   memory length: 298663   epsilon: 0.6066452800085393    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1648   score: 0.0   memory length: 298785   epsilon: 0.6064037200085446    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1649   score: 3.0   memory length: 299031   epsilon: 0.6059166400085552    steps: 246    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1650   score: 1.0   memory length: 299182   epsilon: 0.6056176600085617    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1651   score: 1.0   memory length: 299333   epsilon: 0.6053186800085681    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1652   score: 0.0   memory length: 299455   epsilon: 0.6050771200085734    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1653   score: 0.0   memory length: 299578   epsilon: 0.6048335800085787    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1654   score: 0.0   memory length: 299701   epsilon: 0.604590040008584    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1655   score: 1.0   memory length: 299851   epsilon: 0.6042930400085904    steps: 150    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1656   score: 3.0   memory length: 300099   epsilon: 0.6038020000086011    steps: 248    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1657   score: 0.0   memory length: 300222   epsilon: 0.6035584600086064    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1658   score: 1.0   memory length: 300394   epsilon: 0.6032179000086137    steps: 172    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1659   score: 0.0   memory length: 300517   epsilon: 0.602974360008619    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1660   score: 2.0   memory length: 300737   epsilon: 0.6025387600086285    steps: 220    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 1661   score: 0.0   memory length: 300860   epsilon: 0.6022952200086338    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1662   score: 0.0   memory length: 300982   epsilon: 0.602053660008639    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1663   score: 1.0   memory length: 301154   epsilon: 0.6017131000086464    steps: 172    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1664   score: 0.0   memory length: 301276   epsilon: 0.6014715400086517    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1665   score: 0.0   memory length: 301399   epsilon: 0.601228000008657    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 1666   score: 3.0   memory length: 301646   epsilon: 0.6007389400086676    steps: 247    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 1667   score: 3.0   memory length: 301916   epsilon: 0.6002043400086792    steps: 270    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 1668   score: 0.0   memory length: 302038   epsilon: 0.5999627800086844    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 1669   score: 2.0   memory length: 302256   epsilon: 0.5995311400086938    steps: 218    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1670   score: 1.0   memory length: 302407   epsilon: 0.5992321600087003    steps: 151    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 1671   score: 1.0   memory length: 302578   epsilon: 0.5988935800087076    steps: 171    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 1672   score: 1.0   memory length: 302746   epsilon: 0.5985609400087148    steps: 168    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 1673   score: 2.0   memory length: 302944   epsilon: 0.5981689000087234    steps: 198    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1674   score: 0.0   memory length: 303066   epsilon: 0.5979273400087286    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 1675   score: 1.0   memory length: 303217   epsilon: 0.5976283600087351    steps: 151    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 1676   score: 4.0   memory length: 303533   epsilon: 0.5970026800087487    steps: 316    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1677   score: 1.0   memory length: 303703   epsilon: 0.596666080008756    steps: 170    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1678   score: 1.0   memory length: 303854   epsilon: 0.5963671000087625    steps: 151    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1679   score: 0.0   memory length: 303977   epsilon: 0.5961235600087678    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1680   score: 1.0   memory length: 304146   epsilon: 0.595788940008775    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1681   score: 0.0   memory length: 304269   epsilon: 0.5955454000087803    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1682   score: 1.0   memory length: 304419   epsilon: 0.5952484000087868    steps: 150    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1683   score: 1.0   memory length: 304589   epsilon: 0.5949118000087941    steps: 170    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1684   score: 2.0   memory length: 304807   epsilon: 0.5944801600088034    steps: 218    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1685   score: 1.0   memory length: 304958   epsilon: 0.5941811800088099    steps: 151    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1686   score: 2.0   memory length: 305176   epsilon: 0.5937495400088193    steps: 218    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1687   score: 1.0   memory length: 305326   epsilon: 0.5934525400088257    steps: 150    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1688   score: 2.0   memory length: 305542   epsilon: 0.593024860008835    steps: 216    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1689   score: 2.0   memory length: 305739   epsilon: 0.5926348000088435    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1690   score: 7.0   memory length: 306175   epsilon: 0.5917715200088622    steps: 436    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1691   score: 0.0   memory length: 306298   epsilon: 0.5915279800088675    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1692   score: 1.0   memory length: 306467   epsilon: 0.5911933600088748    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1693   score: 1.0   memory length: 306636   epsilon: 0.590858740008882    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1694   score: 1.0   memory length: 306805   epsilon: 0.5905241200088893    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1695   score: 1.0   memory length: 306977   epsilon: 0.5901835600088967    steps: 172    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1696   score: 1.0   memory length: 307127   epsilon: 0.5898865600089032    steps: 150    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1697   score: 1.0   memory length: 307278   epsilon: 0.5895875800089097    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1698   score: 1.0   memory length: 307446   epsilon: 0.5892549400089169    steps: 168    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1699   score: 0.0   memory length: 307568   epsilon: 0.5890133800089221    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1700   score: 0.0   memory length: 307690   epsilon: 0.5887718200089274    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1701   score: 0.0   memory length: 307812   epsilon: 0.5885302600089326    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 1702   score: 0.0   memory length: 307934   epsilon: 0.5882887000089378    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 1703   score: 0.0   memory length: 308057   epsilon: 0.5880451600089431    steps: 123    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 1704   score: 1.0   memory length: 308228   epsilon: 0.5877065800089505    steps: 171    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 1705   score: 2.0   memory length: 308446   epsilon: 0.5872749400089599    steps: 218    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 1706   score: 7.0   memory length: 308713   epsilon: 0.5867462800089713    steps: 267    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1707   score: 4.0   memory length: 309024   epsilon: 0.5861305000089847    steps: 311    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1708   score: 3.0   memory length: 309253   epsilon: 0.5856770800089945    steps: 229    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1709   score: 2.0   memory length: 309450   epsilon: 0.585287020009003    steps: 197    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1710   score: 0.0   memory length: 309573   epsilon: 0.5850434800090083    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1711   score: 3.0   memory length: 309840   epsilon: 0.5845148200090198    steps: 267    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1712   score: 0.0   memory length: 309963   epsilon: 0.5842712800090251    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1713   score: 3.0   memory length: 310192   epsilon: 0.5838178600090349    steps: 229    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1714   score: 0.0   memory length: 310314   epsilon: 0.5835763000090401    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1715   score: 2.0   memory length: 310512   epsilon: 0.5831842600090487    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1716   score: 3.0   memory length: 310763   epsilon: 0.5826872800090594    steps: 251    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1717   score: 0.0   memory length: 310886   epsilon: 0.5824437400090647    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1718   score: 2.0   memory length: 311066   epsilon: 0.5820873400090725    steps: 180    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1719   score: 3.0   memory length: 311331   epsilon: 0.5815626400090839    steps: 265    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1720   score: 0.0   memory length: 311453   epsilon: 0.5813210800090891    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1721   score: 4.0   memory length: 311744   epsilon: 0.5807449000091016    steps: 291    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1722   score: 0.0   memory length: 311867   epsilon: 0.5805013600091069    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1723   score: 2.0   memory length: 312083   epsilon: 0.5800736800091162    steps: 216    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1724   score: 3.0   memory length: 312353   epsilon: 0.5795390800091278    steps: 270    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1725   score: 1.0   memory length: 312521   epsilon: 0.579206440009135    steps: 168    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1726   score: 2.0   memory length: 312736   epsilon: 0.5787807400091443    steps: 215    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1727   score: 0.0   memory length: 312858   epsilon: 0.5785391800091495    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1728   score: 0.0   memory length: 312981   epsilon: 0.5782956400091548    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1729   score: 1.0   memory length: 313132   epsilon: 0.5779966600091613    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1730   score: 2.0   memory length: 313330   epsilon: 0.5776046200091698    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1731   score: 1.0   memory length: 313500   epsilon: 0.5772680200091771    steps: 170    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 1732   score: 3.0   memory length: 313746   epsilon: 0.5767809400091877    steps: 246    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1733   score: 1.0   memory length: 313915   epsilon: 0.5764463200091949    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1734   score: 4.0   memory length: 314229   epsilon: 0.5758246000092084    steps: 314    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1735   score: 1.0   memory length: 314379   epsilon: 0.5755276000092149    steps: 150    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1736   score: 0.0   memory length: 314502   epsilon: 0.5752840600092202    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1737   score: 0.0   memory length: 314625   epsilon: 0.5750405200092255    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1738   score: 2.0   memory length: 314822   epsilon: 0.5746504600092339    steps: 197    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1739   score: 0.0   memory length: 314944   epsilon: 0.5744089000092392    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1740   score: 3.0   memory length: 315191   epsilon: 0.5739198400092498    steps: 247    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1741   score: 3.0   memory length: 315417   epsilon: 0.5734723600092595    steps: 226    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 1742   score: 0.0   memory length: 315540   epsilon: 0.5732288200092648    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 1743   score: 1.0   memory length: 315711   epsilon: 0.5728902400092721    steps: 171    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 1744   score: 2.0   memory length: 315909   epsilon: 0.5724982000092806    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 1745   score: 3.0   memory length: 316172   epsilon: 0.571977460009292    steps: 263    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 1746   score: 1.0   memory length: 316344   epsilon: 0.5716369000092993    steps: 172    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1747   score: 0.0   memory length: 316466   epsilon: 0.5713953400093046    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 1748   score: 2.0   memory length: 316664   epsilon: 0.5710033000093131    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1749   score: 5.0   memory length: 317011   epsilon: 0.570316240009328    steps: 347    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 1750   score: 0.0   memory length: 317133   epsilon: 0.5700746800093333    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1751   score: 0.0   memory length: 317256   epsilon: 0.5698311400093385    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 1752   score: 3.0   memory length: 317501   epsilon: 0.5693460400093491    steps: 245    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1753   score: 4.0   memory length: 317787   epsilon: 0.5687797600093614    steps: 286    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1754   score: 2.0   memory length: 317971   epsilon: 0.5684154400093693    steps: 184    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1755   score: 2.0   memory length: 318189   epsilon: 0.5679838000093786    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1756   score: 1.0   memory length: 318357   epsilon: 0.5676511600093859    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1757   score: 2.0   memory length: 318575   epsilon: 0.5672195200093952    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1758   score: 0.0   memory length: 318698   epsilon: 0.5669759800094005    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1759   score: 0.0   memory length: 318821   epsilon: 0.5667324400094058    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1760   score: 3.0   memory length: 319069   epsilon: 0.5662414000094165    steps: 248    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1761   score: 2.0   memory length: 319268   epsilon: 0.565847380009425    steps: 199    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1762   score: 2.0   memory length: 319487   epsilon: 0.5654137600094344    steps: 219    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1763   score: 1.0   memory length: 319657   epsilon: 0.5650771600094417    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1764   score: 0.0   memory length: 319779   epsilon: 0.564835600009447    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1765   score: 0.0   memory length: 319902   epsilon: 0.5645920600094523    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1766   score: 0.0   memory length: 320024   epsilon: 0.5643505000094575    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1767   score: 2.0   memory length: 320247   epsilon: 0.5639089600094671    steps: 223    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1768   score: 0.0   memory length: 320370   epsilon: 0.5636654200094724    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1769   score: 2.0   memory length: 320587   epsilon: 0.5632357600094817    steps: 217    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1770   score: 2.0   memory length: 320805   epsilon: 0.5628041200094911    steps: 218    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1771   score: 0.0   memory length: 320928   epsilon: 0.5625605800094964    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1772   score: 0.0   memory length: 321051   epsilon: 0.5623170400095017    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1773   score: 1.0   memory length: 321203   epsilon: 0.5620160800095082    steps: 152    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1774   score: 2.0   memory length: 321421   epsilon: 0.5615844400095176    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1775   score: 0.0   memory length: 321544   epsilon: 0.5613409000095229    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1776   score: 1.0   memory length: 321713   epsilon: 0.5610062800095301    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1777   score: 0.0   memory length: 321836   epsilon: 0.5607627400095354    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1778   score: 3.0   memory length: 322100   epsilon: 0.5602400200095468    steps: 264    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1779   score: 2.0   memory length: 322317   epsilon: 0.5598103600095561    steps: 217    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1780   score: 4.0   memory length: 322604   epsilon: 0.5592421000095684    steps: 287    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1781   score: 2.0   memory length: 322803   epsilon: 0.558848080009577    steps: 199    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1782   score: 3.0   memory length: 323034   epsilon: 0.5583907000095869    steps: 231    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1783   score: 1.0   memory length: 323184   epsilon: 0.5580937000095934    steps: 150    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1784   score: 0.0   memory length: 323307   epsilon: 0.5578501600095986    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1785   score: 2.0   memory length: 323524   epsilon: 0.557420500009608    steps: 217    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1786   score: 0.0   memory length: 323647   epsilon: 0.5571769600096133    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1787   score: 1.0   memory length: 323797   epsilon: 0.5568799600096197    steps: 150    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1788   score: 2.0   memory length: 323977   epsilon: 0.5565235600096274    steps: 180    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1789   score: 2.0   memory length: 324200   epsilon: 0.556082020009637    steps: 223    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1790   score: 0.0   memory length: 324323   epsilon: 0.5558384800096423    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1791   score: 2.0   memory length: 324520   epsilon: 0.5554484200096508    steps: 197    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1792   score: 2.0   memory length: 324717   epsilon: 0.5550583600096592    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1793   score: 1.0   memory length: 324886   epsilon: 0.5547237400096665    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1794   score: 1.0   memory length: 325058   epsilon: 0.5543831800096739    steps: 172    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1795   score: 2.0   memory length: 325258   epsilon: 0.5539871800096825    steps: 200    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1796   score: 0.0   memory length: 325381   epsilon: 0.5537436400096878    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1797   score: 1.0   memory length: 325550   epsilon: 0.553409020009695    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1798   score: 0.0   memory length: 325673   epsilon: 0.5531654800097003    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1799   score: 2.0   memory length: 325871   epsilon: 0.5527734400097088    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1800   score: 2.0   memory length: 326094   epsilon: 0.5523319000097184    steps: 223    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1801   score: 1.0   memory length: 326245   epsilon: 0.5520329200097249    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1802   score: 2.0   memory length: 326465   epsilon: 0.5515973200097344    steps: 220    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1803   score: 0.0   memory length: 326587   epsilon: 0.5513557600097396    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1804   score: 2.0   memory length: 326803   epsilon: 0.5509280800097489    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1805   score: 1.0   memory length: 326971   epsilon: 0.5505954400097561    steps: 168    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1806   score: 1.0   memory length: 327139   epsilon: 0.5502628000097634    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1807   score: 2.0   memory length: 327357   epsilon: 0.5498311600097727    steps: 218    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1808   score: 0.0   memory length: 327480   epsilon: 0.549587620009778    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 1809   score: 0.0   memory length: 327603   epsilon: 0.5493440800097833    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 1810   score: 1.0   memory length: 327771   epsilon: 0.5490114400097905    steps: 168    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1811   score: 1.0   memory length: 327940   epsilon: 0.5486768200097978    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1812   score: 3.0   memory length: 328185   epsilon: 0.5481917200098083    steps: 245    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 1813   score: 7.0   memory length: 328586   epsilon: 0.5473977400098256    steps: 401    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1814   score: 5.0   memory length: 328916   epsilon: 0.5467443400098397    steps: 330    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1815   score: 2.0   memory length: 329135   epsilon: 0.5463107200098491    steps: 219    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1816   score: 0.0   memory length: 329258   epsilon: 0.5460671800098544    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1817   score: 1.0   memory length: 329428   epsilon: 0.5457305800098617    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1818   score: 4.0   memory length: 329745   epsilon: 0.5451029200098754    steps: 317    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1819   score: 1.0   memory length: 329916   epsilon: 0.5447643400098827    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1820   score: 1.0   memory length: 330085   epsilon: 0.54442972000989    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1821   score: 0.0   memory length: 330208   epsilon: 0.5441861800098953    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1822   score: 1.0   memory length: 330377   epsilon: 0.5438515600099025    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1823   score: 0.0   memory length: 330500   epsilon: 0.5436080200099078    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1824   score: 2.0   memory length: 330719   epsilon: 0.5431744000099172    steps: 219    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 1825   score: 2.0   memory length: 330917   epsilon: 0.5427823600099257    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1826   score: 2.0   memory length: 331117   epsilon: 0.5423863600099343    steps: 200    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1827   score: 3.0   memory length: 331363   epsilon: 0.5418992800099449    steps: 246    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1828   score: 1.0   memory length: 331531   epsilon: 0.5415666400099521    steps: 168    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1829   score: 1.0   memory length: 331702   epsilon: 0.5412280600099595    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1830   score: 3.0   memory length: 331928   epsilon: 0.5407805800099692    steps: 226    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1831   score: 3.0   memory length: 332173   epsilon: 0.5402954800099797    steps: 245    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1832   score: 1.0   memory length: 332341   epsilon: 0.539962840009987    steps: 168    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1833   score: 2.0   memory length: 332538   epsilon: 0.5395727800099954    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1834   score: 2.0   memory length: 332759   epsilon: 0.5391352000100049    steps: 221    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1835   score: 3.0   memory length: 333010   epsilon: 0.5386382200100157    steps: 251    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1836   score: 1.0   memory length: 333162   epsilon: 0.5383372600100222    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1837   score: 1.0   memory length: 333331   epsilon: 0.5380026400100295    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1838   score: 1.0   memory length: 333499   epsilon: 0.5376700000100367    steps: 168    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1839   score: 2.0   memory length: 333678   epsilon: 0.5373155800100444    steps: 179    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1840   score: 0.0   memory length: 333801   epsilon: 0.5370720400100497    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1841   score: 2.0   memory length: 333999   epsilon: 0.5366800000100582    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1842   score: 0.0   memory length: 334122   epsilon: 0.5364364600100635    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1843   score: 0.0   memory length: 334245   epsilon: 0.5361929200100688    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1844   score: 1.0   memory length: 334397   epsilon: 0.5358919600100753    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1845   score: 1.0   memory length: 334569   epsilon: 0.5355514000100827    steps: 172    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1846   score: 0.0   memory length: 334692   epsilon: 0.535307860010088    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1847   score: 1.0   memory length: 334862   epsilon: 0.5349712600100953    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1848   score: 2.0   memory length: 335060   epsilon: 0.5345792200101038    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1849   score: 2.0   memory length: 335239   epsilon: 0.5342248000101115    steps: 179    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1850   score: 0.0   memory length: 335362   epsilon: 0.5339812600101168    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1851   score: 2.0   memory length: 335578   epsilon: 0.5335535800101261    steps: 216    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1852   score: 3.0   memory length: 335845   epsilon: 0.5330249200101376    steps: 267    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1853   score: 0.0   memory length: 335968   epsilon: 0.5327813800101429    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1854   score: 1.0   memory length: 336120   epsilon: 0.5324804200101494    steps: 152    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 1855   score: 1.0   memory length: 336289   epsilon: 0.5321458000101567    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 1856   score: 3.0   memory length: 336515   epsilon: 0.5316983200101664    steps: 226    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 1857   score: 1.0   memory length: 336666   epsilon: 0.5313993400101729    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 1858   score: 3.0   memory length: 336912   epsilon: 0.5309122600101834    steps: 246    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 1859   score: 2.0   memory length: 337114   epsilon: 0.5305123000101921    steps: 202    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1860   score: 7.0   memory length: 337520   epsilon: 0.5297084200102096    steps: 406    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1861   score: 3.0   memory length: 337766   epsilon: 0.5292213400102201    steps: 246    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1862   score: 1.0   memory length: 337935   epsilon: 0.5288867200102274    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1863   score: 0.0   memory length: 338058   epsilon: 0.5286431800102327    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1864   score: 1.0   memory length: 338227   epsilon: 0.52830856001024    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1865   score: 1.0   memory length: 338378   epsilon: 0.5280095800102464    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1866   score: 0.0   memory length: 338501   epsilon: 0.5277660400102517    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1867   score: 1.0   memory length: 338672   epsilon: 0.5274274600102591    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1868   score: 1.0   memory length: 338823   epsilon: 0.5271284800102656    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1869   score: 0.0   memory length: 338946   epsilon: 0.5268849400102709    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1870   score: 0.0   memory length: 339069   epsilon: 0.5266414000102762    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1871   score: 1.0   memory length: 339220   epsilon: 0.5263424200102826    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1872   score: 1.0   memory length: 339391   epsilon: 0.52600384001029    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1873   score: 4.0   memory length: 339634   epsilon: 0.5255227000103004    steps: 243    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1874   score: 0.0   memory length: 339757   epsilon: 0.5252791600103057    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1875   score: 0.0   memory length: 339880   epsilon: 0.525035620010311    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1876   score: 2.0   memory length: 340063   epsilon: 0.5246732800103189    steps: 183    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1877   score: 0.0   memory length: 340185   epsilon: 0.5244317200103241    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1878   score: 2.0   memory length: 340383   epsilon: 0.5240396800103326    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1879   score: 1.0   memory length: 340533   epsilon: 0.5237426800103391    steps: 150    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1880   score: 3.0   memory length: 340780   epsilon: 0.5232536200103497    steps: 247    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1881   score: 2.0   memory length: 340983   epsilon: 0.5228516800103584    steps: 203    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1882   score: 2.0   memory length: 341164   epsilon: 0.5224933000103662    steps: 181    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1883   score: 0.0   memory length: 341287   epsilon: 0.5222497600103715    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1884   score: 3.0   memory length: 341554   epsilon: 0.521721100010383    steps: 267    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1885   score: 2.0   memory length: 341773   epsilon: 0.5212874800103924    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1886   score: 1.0   memory length: 341924   epsilon: 0.5209885000103989    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1887   score: 0.0   memory length: 342047   epsilon: 0.5207449600104042    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1888   score: 3.0   memory length: 342294   epsilon: 0.5202559000104148    steps: 247    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1889   score: 1.0   memory length: 342464   epsilon: 0.5199193000104221    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1890   score: 2.0   memory length: 342661   epsilon: 0.5195292400104305    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1891   score: 2.0   memory length: 342859   epsilon: 0.5191372000104391    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1892   score: 5.0   memory length: 343204   epsilon: 0.5184541000104539    steps: 345    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1893   score: 1.0   memory length: 343373   epsilon: 0.5181194800104612    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1894   score: 2.0   memory length: 343571   epsilon: 0.5177274400104697    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1895   score: 4.0   memory length: 343846   epsilon: 0.5171829400104815    steps: 275    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1896   score: 0.0   memory length: 343969   epsilon: 0.5169394000104868    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1897   score: 1.0   memory length: 344120   epsilon: 0.5166404200104933    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1898   score: 0.0   memory length: 344243   epsilon: 0.5163968800104985    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1899   score: 0.0   memory length: 344365   epsilon: 0.5161553200105038    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1900   score: 0.0   memory length: 344488   epsilon: 0.5159117800105091    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1901   score: 0.0   memory length: 344611   epsilon: 0.5156682400105144    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1902   score: 0.0   memory length: 344734   epsilon: 0.5154247000105197    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1903   score: 3.0   memory length: 344980   epsilon: 0.5149376200105302    steps: 246    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1904   score: 0.0   memory length: 345102   epsilon: 0.5146960600105355    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1905   score: 0.0   memory length: 345224   epsilon: 0.5144545000105407    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1906   score: 3.0   memory length: 345493   epsilon: 0.5139218800105523    steps: 269    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1907   score: 3.0   memory length: 345737   epsilon: 0.5134387600105628    steps: 244    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1908   score: 2.0   memory length: 345953   epsilon: 0.513011080010572    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1909   score: 1.0   memory length: 346104   epsilon: 0.5127121000105785    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1910   score: 3.0   memory length: 346350   epsilon: 0.5122250200105891    steps: 246    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1911   score: 3.0   memory length: 346576   epsilon: 0.5117775400105988    steps: 226    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1912   score: 3.0   memory length: 346819   epsilon: 0.5112964000106093    steps: 243    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1913   score: 2.0   memory length: 346999   epsilon: 0.510940000010617    steps: 180    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1914   score: 1.0   memory length: 347168   epsilon: 0.5106053800106243    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1915   score: 1.0   memory length: 347319   epsilon: 0.5103064000106308    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1916   score: 1.0   memory length: 347487   epsilon: 0.509973760010638    steps: 168    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1917   score: 0.0   memory length: 347610   epsilon: 0.5097302200106433    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1918   score: 2.0   memory length: 347808   epsilon: 0.5093381800106518    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1919   score: 0.0   memory length: 347930   epsilon: 0.509096620010657    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1920   score: 0.0   memory length: 348052   epsilon: 0.5088550600106623    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1921   score: 2.0   memory length: 348269   epsilon: 0.5084254000106716    steps: 217    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1922   score: 2.0   memory length: 348470   epsilon: 0.5080274200106802    steps: 201    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1923   score: 2.0   memory length: 348650   epsilon: 0.507671020010688    steps: 180    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1924   score: 3.0   memory length: 348882   epsilon: 0.507211660010698    steps: 232    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1925   score: 3.0   memory length: 349129   epsilon: 0.5067226000107086    steps: 247    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1926   score: 2.0   memory length: 349327   epsilon: 0.5063305600107171    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1927   score: 2.0   memory length: 349527   epsilon: 0.5059345600107257    steps: 200    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1928   score: 2.0   memory length: 349724   epsilon: 0.5055445000107341    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1929   score: 0.0   memory length: 349847   epsilon: 0.5053009600107394    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1930   score: 1.0   memory length: 349998   epsilon: 0.5050019800107459    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1931   score: 1.0   memory length: 350167   epsilon: 0.5046673600107532    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1932   score: 4.0   memory length: 350463   epsilon: 0.5040812800107659    steps: 296    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1933   score: 2.0   memory length: 350679   epsilon: 0.5036536000107752    steps: 216    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1934   score: 1.0   memory length: 350848   epsilon: 0.5033189800107825    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1935   score: 0.0   memory length: 350971   epsilon: 0.5030754400107877    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1936   score: 0.0   memory length: 351094   epsilon: 0.502831900010793    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1937   score: 1.0   memory length: 351245   epsilon: 0.5025329200107995    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1938   score: 2.0   memory length: 351429   epsilon: 0.5021686000108074    steps: 184    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1939   score: 1.0   memory length: 351601   epsilon: 0.5018280400108148    steps: 172    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 1940   score: 1.0   memory length: 351752   epsilon: 0.5015290600108213    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1941   score: 2.0   memory length: 351969   epsilon: 0.5010994000108306    steps: 217    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1942   score: 2.0   memory length: 352188   epsilon: 0.5006657800108401    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1943   score: 0.0   memory length: 352311   epsilon: 0.5004222400108453    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1944   score: 2.0   memory length: 352528   epsilon: 0.49999258001085445    steps: 217    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1945   score: 2.0   memory length: 352709   epsilon: 0.4996342000108522    steps: 181    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1946   score: 1.0   memory length: 352878   epsilon: 0.49929958001085006    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1947   score: 2.0   memory length: 353096   epsilon: 0.49886794001084733    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1948   score: 3.0   memory length: 353341   epsilon: 0.49838284001084426    steps: 245    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1949   score: 1.0   memory length: 353510   epsilon: 0.49804822001084215    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1950   score: 2.0   memory length: 353708   epsilon: 0.49765618001083967    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1951   score: 0.0   memory length: 353831   epsilon: 0.4974126400108381    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1952   score: 0.0   memory length: 353954   epsilon: 0.4971691000108366    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 1953   score: 3.0   memory length: 354220   epsilon: 0.49664242001083325    steps: 266    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1954   score: 2.0   memory length: 354418   epsilon: 0.49625038001083077    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1955   score: 2.0   memory length: 354616   epsilon: 0.4958583400108283    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1956   score: 4.0   memory length: 354912   epsilon: 0.4952722600108246    steps: 296    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1957   score: 2.0   memory length: 355129   epsilon: 0.49484260001082186    steps: 217    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1958   score: 2.0   memory length: 355326   epsilon: 0.4944525400108194    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1959   score: 1.0   memory length: 355495   epsilon: 0.4941179200108173    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1960   score: 0.0   memory length: 355618   epsilon: 0.49387438001081574    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 1961   score: 2.0   memory length: 355816   epsilon: 0.49348234001081326    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 1962   score: 3.0   memory length: 356062   epsilon: 0.4929952600108102    steps: 246    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1963   score: 2.0   memory length: 356245   epsilon: 0.4926329200108079    steps: 183    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 1964   score: 3.0   memory length: 356474   epsilon: 0.492179500010805    steps: 229    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1965   score: 1.0   memory length: 356625   epsilon: 0.4918805200108031    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1966   score: 2.0   memory length: 356823   epsilon: 0.49148848001080064    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1967   score: 0.0   memory length: 356945   epsilon: 0.4912469200107991    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1968   score: 0.0   memory length: 357068   epsilon: 0.4910033800107976    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1969   score: 2.0   memory length: 357265   epsilon: 0.4906133200107951    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1970   score: 0.0   memory length: 357387   epsilon: 0.4903717600107936    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1971   score: 2.0   memory length: 357603   epsilon: 0.48994408001079087    steps: 216    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1972   score: 1.0   memory length: 357771   epsilon: 0.48961144001078877    steps: 168    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1973   score: 3.0   memory length: 357999   epsilon: 0.4891600000107859    steps: 228    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1974   score: 0.0   memory length: 358121   epsilon: 0.4889184400107844    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1975   score: 2.0   memory length: 358319   epsilon: 0.4885264000107819    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1976   score: 3.0   memory length: 358584   epsilon: 0.4880017000107786    steps: 265    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1977   score: 2.0   memory length: 358782   epsilon: 0.4876096600107761    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 1978   score: 0.0   memory length: 358905   epsilon: 0.48736612001077456    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1979   score: 2.0   memory length: 359123   epsilon: 0.48693448001077183    steps: 218    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 1980   score: 1.0   memory length: 359292   epsilon: 0.4865998600107697    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 1981   score: 3.0   memory length: 359538   epsilon: 0.48611278001076663    steps: 246    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1982   score: 0.0   memory length: 359660   epsilon: 0.4858712200107651    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1983   score: 3.0   memory length: 359908   epsilon: 0.485380180010762    steps: 248    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 1984   score: 0.0   memory length: 360031   epsilon: 0.48513664001076046    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1985   score: 2.0   memory length: 360229   epsilon: 0.484744600010758    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1986   score: 0.0   memory length: 360352   epsilon: 0.48450106001075643    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 1987   score: 3.0   memory length: 360598   epsilon: 0.48401398001075335    steps: 246    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1988   score: 3.0   memory length: 360826   epsilon: 0.4835625400107505    steps: 228    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1989   score: 1.0   memory length: 360995   epsilon: 0.4832279200107484    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 1990   score: 0.0   memory length: 361117   epsilon: 0.48298636001074685    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1991   score: 2.0   memory length: 361337   epsilon: 0.4825507600107441    steps: 220    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 1992   score: 3.0   memory length: 361564   epsilon: 0.48210130001074125    steps: 227    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1993   score: 1.0   memory length: 361715   epsilon: 0.48180232001073936    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 1994   score: 1.0   memory length: 361887   epsilon: 0.4814617600107372    steps: 172    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1995   score: 0.0   memory length: 362010   epsilon: 0.48121822001073566    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1996   score: 0.0   memory length: 362132   epsilon: 0.48097666001073414    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 1997   score: 4.0   memory length: 362448   epsilon: 0.4803509800107302    steps: 316    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 1998   score: 1.0   memory length: 362599   epsilon: 0.4800520000107283    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 1999   score: 1.0   memory length: 362750   epsilon: 0.4797530200107264    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2000   score: 2.0   memory length: 362948   epsilon: 0.4793609800107239    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2001   score: 1.0   memory length: 363099   epsilon: 0.479062000010722    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2002   score: 1.0   memory length: 363250   epsilon: 0.47876302001072013    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2003   score: 0.0   memory length: 363372   epsilon: 0.4785214600107186    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2004   score: 1.0   memory length: 363543   epsilon: 0.47818288001071646    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2005   score: 0.0   memory length: 363666   epsilon: 0.4779393400107149    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2006   score: 2.0   memory length: 363863   epsilon: 0.47754928001071245    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2007   score: 1.0   memory length: 364032   epsilon: 0.47721466001071033    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2008   score: 2.0   memory length: 364214   epsilon: 0.47685430001070805    steps: 182    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2009   score: 1.0   memory length: 364383   epsilon: 0.47651968001070594    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2010   score: 3.0   memory length: 364611   epsilon: 0.4760682400107031    steps: 228    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2011   score: 3.0   memory length: 364880   epsilon: 0.4755356200106997    steps: 269    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2012   score: 1.0   memory length: 365049   epsilon: 0.4752010000106976    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 2013   score: 1.0   memory length: 365200   epsilon: 0.4749020200106957    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2014   score: 3.0   memory length: 365447   epsilon: 0.4744129600106926    steps: 247    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2015   score: 0.0   memory length: 365569   epsilon: 0.4741714000106911    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 2016   score: 1.0   memory length: 365738   epsilon: 0.47383678001068896    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 2017   score: 1.0   memory length: 365910   epsilon: 0.4734962200106868    steps: 172    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2018   score: 2.0   memory length: 366132   epsilon: 0.473056660010684    steps: 222    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2019   score: 0.0   memory length: 366254   epsilon: 0.4728151000106825    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2020   score: 2.0   memory length: 366452   epsilon: 0.47242306001068    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2021   score: 1.0   memory length: 366623   epsilon: 0.4720844800106779    steps: 171    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2022   score: 2.0   memory length: 366820   epsilon: 0.4716944200106754    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2023   score: 2.0   memory length: 367038   epsilon: 0.4712627800106727    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2024   score: 2.0   memory length: 367258   epsilon: 0.4708271800106699    steps: 220    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2025   score: 3.0   memory length: 367504   epsilon: 0.47034010001066684    steps: 246    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2026   score: 1.0   memory length: 367655   epsilon: 0.47004112001066495    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 2027   score: 2.0   memory length: 367835   epsilon: 0.4696847200106627    steps: 180    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 2028   score: 1.0   memory length: 368004   epsilon: 0.4693501000106606    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2029   score: 3.0   memory length: 368267   epsilon: 0.4688293600106573    steps: 263    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2030   score: 3.0   memory length: 368514   epsilon: 0.4683403000106542    steps: 247    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2031   score: 3.0   memory length: 368783   epsilon: 0.4678076800106508    steps: 269    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2032   score: 0.0   memory length: 368906   epsilon: 0.4675641400106493    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2033   score: 1.0   memory length: 369057   epsilon: 0.4672651600106474    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2034   score: 0.0   memory length: 369180   epsilon: 0.46702162001064584    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 2035   score: 1.0   memory length: 369348   epsilon: 0.46668898001064374    steps: 168    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2036   score: 2.0   memory length: 369546   epsilon: 0.46629694001064126    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2037   score: 2.0   memory length: 369744   epsilon: 0.4659049000106388    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2038   score: 0.0   memory length: 369866   epsilon: 0.46566334001063725    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2039   score: 2.0   memory length: 370083   epsilon: 0.46523368001063453    steps: 217    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2040   score: 0.0   memory length: 370206   epsilon: 0.464990140010633    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2041   score: 2.0   memory length: 370422   epsilon: 0.4645624600106303    steps: 216    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2042   score: 0.0   memory length: 370544   epsilon: 0.46432090001062876    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 2043   score: 1.0   memory length: 370713   epsilon: 0.46398628001062664    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2044   score: 0.0   memory length: 370835   epsilon: 0.4637447200106251    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2045   score: 1.0   memory length: 370986   epsilon: 0.4634457400106232    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2046   score: 0.0   memory length: 371108   epsilon: 0.4632041800106217    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2047   score: 2.0   memory length: 371326   epsilon: 0.46277254001061896    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2048   score: 1.0   memory length: 371495   epsilon: 0.46243792001061684    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 2049   score: 0.0   memory length: 371617   epsilon: 0.4621963600106153    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2050   score: 0.0   memory length: 371739   epsilon: 0.4619548000106138    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2051   score: 2.0   memory length: 371956   epsilon: 0.46152514001061107    steps: 217    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2052   score: 3.0   memory length: 372190   epsilon: 0.46106182001060814    steps: 234    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2053   score: 1.0   memory length: 372341   epsilon: 0.46076284001060624    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 2054   score: 1.0   memory length: 372511   epsilon: 0.4604262400106041    steps: 170    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2055   score: 1.0   memory length: 372681   epsilon: 0.460089640010602    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2056   score: 2.0   memory length: 372881   epsilon: 0.4596936400105995    steps: 200    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2057   score: 4.0   memory length: 373156   epsilon: 0.45914914001059604    steps: 275    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2058   score: 2.0   memory length: 373353   epsilon: 0.45875908001059357    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2059   score: 0.0   memory length: 373476   epsilon: 0.458515540010592    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2060   score: 3.0   memory length: 373702   epsilon: 0.4580680600105892    steps: 226    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 2061   score: 0.0   memory length: 373825   epsilon: 0.45782452001058765    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2062   score: 2.0   memory length: 374043   epsilon: 0.4573928800105849    steps: 218    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2063   score: 0.0   memory length: 374166   epsilon: 0.4571493400105834    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2064   score: 1.0   memory length: 374316   epsilon: 0.4568523400105815    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2065   score: 3.0   memory length: 374583   epsilon: 0.45632368001057816    steps: 267    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2066   score: 2.0   memory length: 374780   epsilon: 0.4559336200105757    steps: 197    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2067   score: 1.0   memory length: 374930   epsilon: 0.4556366200105738    steps: 150    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2068   score: 2.0   memory length: 375146   epsilon: 0.4552089400105711    steps: 216    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2069   score: 0.0   memory length: 375268   epsilon: 0.4549673800105696    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2070   score: 1.0   memory length: 375438   epsilon: 0.45463078001056745    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2071   score: 0.0   memory length: 375561   epsilon: 0.4543872400105659    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2072   score: 1.0   memory length: 375712   epsilon: 0.454088260010564    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2073   score: 0.0   memory length: 375835   epsilon: 0.4538447200105625    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2074   score: 1.0   memory length: 376004   epsilon: 0.45351010001056036    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2075   score: 1.0   memory length: 376174   epsilon: 0.45317350001055823    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2076   score: 2.0   memory length: 376372   epsilon: 0.45278146001055575    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 2077   score: 1.0   memory length: 376541   epsilon: 0.45244684001055363    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 2078   score: 0.0   memory length: 376664   epsilon: 0.4522033000105521    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 2079   score: 2.0   memory length: 376845   epsilon: 0.4518449200105498    steps: 181    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 2080   score: 1.0   memory length: 376995   epsilon: 0.45154792001054794    steps: 150    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 2081   score: 5.0   memory length: 377304   epsilon: 0.45093610001054407    steps: 309    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2082   score: 1.0   memory length: 377473   epsilon: 0.45060148001054195    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2083   score: 2.0   memory length: 377655   epsilon: 0.4502411200105397    steps: 182    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2084   score: 1.0   memory length: 377805   epsilon: 0.4499441200105378    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2085   score: 0.0   memory length: 377928   epsilon: 0.44970058001053625    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 2086   score: 2.0   memory length: 378143   epsilon: 0.44927488001053356    steps: 215    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2087   score: 1.0   memory length: 378314   epsilon: 0.4489363000105314    steps: 171    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 2088   score: 0.0   memory length: 378437   epsilon: 0.4486927600105299    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 2089   score: 2.0   memory length: 378634   epsilon: 0.4483027000105274    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 2090   score: 0.0   memory length: 378757   epsilon: 0.44805916001052587    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 2091   score: 5.0   memory length: 379102   epsilon: 0.44737606001052155    steps: 345    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2092   score: 1.0   memory length: 379252   epsilon: 0.44707906001051967    steps: 150    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 2093   score: 0.0   memory length: 379374   epsilon: 0.44683750001051814    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 2094   score: 0.0   memory length: 379496   epsilon: 0.4465959400105166    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 2095   score: 2.0   memory length: 379712   epsilon: 0.4461682600105139    steps: 216    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 2096   score: 3.0   memory length: 379960   epsilon: 0.4456772200105108    steps: 248    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2097   score: 2.0   memory length: 380157   epsilon: 0.44528716001050833    steps: 197    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 2098   score: 2.0   memory length: 380355   epsilon: 0.44489512001050585    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2099   score: 2.0   memory length: 380553   epsilon: 0.44450308001050337    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2100   score: 3.0   memory length: 380820   epsilon: 0.4439744200105    steps: 267    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2101   score: 1.0   memory length: 380971   epsilon: 0.44367544001049813    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2102   score: 2.0   memory length: 381169   epsilon: 0.44328340001049565    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2103   score: 4.0   memory length: 381430   epsilon: 0.4427666200104924    steps: 261    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2104   score: 2.0   memory length: 381646   epsilon: 0.4423389400104897    steps: 216    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 2105   score: 0.0   memory length: 381769   epsilon: 0.44209540001048814    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 2106   score: 4.0   memory length: 382061   epsilon: 0.4415172400104845    steps: 292    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2107   score: 2.0   memory length: 382259   epsilon: 0.441125200010482    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2108   score: 3.0   memory length: 382507   epsilon: 0.4406341600104789    steps: 248    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2109   score: 4.0   memory length: 382800   epsilon: 0.4400540200104752    steps: 293    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2110   score: 5.0   memory length: 383141   epsilon: 0.43937884001047095    steps: 341    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2111   score: 2.0   memory length: 383339   epsilon: 0.43898680001046847    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2112   score: 2.0   memory length: 383536   epsilon: 0.438596740010466    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2113   score: 3.0   memory length: 383799   epsilon: 0.4380760000104627    steps: 263    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2114   score: 4.0   memory length: 384091   epsilon: 0.43749784001045905    steps: 292    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2115   score: 1.0   memory length: 384262   epsilon: 0.4371592600104569    steps: 171    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2116   score: 4.0   memory length: 384576   epsilon: 0.436537540010453    steps: 314    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 2117   score: 3.0   memory length: 384824   epsilon: 0.43604650001044987    steps: 248    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 2118   score: 1.0   memory length: 384992   epsilon: 0.43571386001044776    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 2119   score: 3.0   memory length: 385238   epsilon: 0.4352267800104447    steps: 246    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 2120   score: 2.0   memory length: 385456   epsilon: 0.43479514001044195    steps: 218    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 2121   score: 4.0   memory length: 385758   epsilon: 0.43419718001043817    steps: 302    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 2122   score: 2.0   memory length: 385977   epsilon: 0.4337635600104354    steps: 219    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 2123   score: 0.0   memory length: 386100   epsilon: 0.4335200200104339    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 2124   score: 2.0   memory length: 386318   epsilon: 0.43308838001043115    steps: 218    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 2125   score: 2.0   memory length: 386497   epsilon: 0.4327339600104289    steps: 179    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 2126   score: 1.0   memory length: 386648   epsilon: 0.432434980010427    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 2127   score: 0.0   memory length: 386770   epsilon: 0.4321934200104255    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 2128   score: 0.0   memory length: 386893   epsilon: 0.43194988001042395    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 2129   score: 2.0   memory length: 387109   epsilon: 0.43152220001042124    steps: 216    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 2130   score: 0.0   memory length: 387232   epsilon: 0.4312786600104197    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2131   score: 1.0   memory length: 387382   epsilon: 0.4309816600104178    steps: 150    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2132   score: 0.0   memory length: 387505   epsilon: 0.4307381200104163    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2133   score: 0.0   memory length: 387628   epsilon: 0.43049458001041474    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2134   score: 0.0   memory length: 387750   epsilon: 0.4302530200104132    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2135   score: 0.0   memory length: 387872   epsilon: 0.4300114600104117    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2136   score: 4.0   memory length: 388169   epsilon: 0.42942340001040796    steps: 297    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2137   score: 2.0   memory length: 388388   epsilon: 0.4289897800104052    steps: 219    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2138   score: 1.0   memory length: 388538   epsilon: 0.42869278001040334    steps: 150    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2139   score: 3.0   memory length: 388789   epsilon: 0.4281958000104002    steps: 251    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2140   score: 1.0   memory length: 388940   epsilon: 0.4278968200103983    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2141   score: 0.0   memory length: 389062   epsilon: 0.4276552600103968    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2142   score: 0.0   memory length: 389184   epsilon: 0.42741370001039525    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2143   score: 0.0   memory length: 389307   epsilon: 0.4271701600103937    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2144   score: 0.0   memory length: 389429   epsilon: 0.4269286000103922    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2145   score: 1.0   memory length: 389580   epsilon: 0.4266296200103903    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2146   score: 0.0   memory length: 389703   epsilon: 0.42638608001038875    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2147   score: 4.0   memory length: 389990   epsilon: 0.42581782001038515    steps: 287    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2148   score: 1.0   memory length: 390158   epsilon: 0.42548518001038305    steps: 168    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2149   score: 1.0   memory length: 390329   epsilon: 0.4251466000103809    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2150   score: 2.0   memory length: 390548   epsilon: 0.42471298001037816    steps: 219    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 2151   score: 2.0   memory length: 390745   epsilon: 0.4243229200103757    steps: 197    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 2152   score: 4.0   memory length: 391005   epsilon: 0.42380812001037244    steps: 260    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 2153   score: 1.0   memory length: 391157   epsilon: 0.42350716001037053    steps: 152    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 2154   score: 0.0   memory length: 391280   epsilon: 0.423263620010369    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 2155   score: 1.0   memory length: 391450   epsilon: 0.42292702001036686    steps: 170    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 2156   score: 0.0   memory length: 391573   epsilon: 0.4226834800103653    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2157   score: 4.0   memory length: 391870   epsilon: 0.4220954200103616    steps: 297    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2158   score: 2.0   memory length: 392068   epsilon: 0.4217033800103591    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2159   score: 0.0   memory length: 392191   epsilon: 0.4214598400103576    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2160   score: 1.0   memory length: 392342   epsilon: 0.4211608600103557    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2161   score: 2.0   memory length: 392542   epsilon: 0.4207648600103532    steps: 200    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2162   score: 1.0   memory length: 392695   epsilon: 0.42046192001035126    steps: 153    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2163   score: 2.0   memory length: 392911   epsilon: 0.42003424001034856    steps: 216    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 2164   score: 4.0   memory length: 393187   epsilon: 0.4194877600103451    steps: 276    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 2165   score: 1.0   memory length: 393338   epsilon: 0.4191887800103432    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 2166   score: 1.0   memory length: 393509   epsilon: 0.41885020001034107    steps: 171    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 2167   score: 1.0   memory length: 393660   epsilon: 0.4185512200103392    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 2168   score: 0.0   memory length: 393783   epsilon: 0.41830768001033763    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2169   score: 1.0   memory length: 393934   epsilon: 0.41800870001033574    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2170   score: 2.0   memory length: 394155   epsilon: 0.417571120010333    steps: 221    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 2171   score: 1.0   memory length: 394306   epsilon: 0.4172721400103311    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 2172   score: 0.0   memory length: 394428   epsilon: 0.41703058001032955    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 2173   score: 0.0   memory length: 394551   epsilon: 0.416787040010328    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 2174   score: 2.0   memory length: 394748   epsilon: 0.41639698001032555    steps: 197    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 2175   score: 2.0   memory length: 394947   epsilon: 0.41600296001032305    steps: 199    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 2176   score: 0.0   memory length: 395070   epsilon: 0.4157594200103215    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 2177   score: 2.0   memory length: 395288   epsilon: 0.4153277800103188    steps: 218    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 2178   score: 2.0   memory length: 395486   epsilon: 0.4149357400103163    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 2179   score: 2.0   memory length: 395684   epsilon: 0.4145437000103138    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 2180   score: 4.0   memory length: 395997   epsilon: 0.4139239600103099    steps: 313    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 2181   score: 5.0   memory length: 396343   epsilon: 0.41323888001030556    steps: 346    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 2182   score: 1.0   memory length: 396512   epsilon: 0.41290426001030345    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 2183   score: 3.0   memory length: 396740   epsilon: 0.4124528200103006    steps: 228    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 2184   score: 1.0   memory length: 396909   epsilon: 0.4121182000102985    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 2185   score: 4.0   memory length: 397186   epsilon: 0.411569740010295    steps: 277    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 2186   score: 0.0   memory length: 397308   epsilon: 0.4113281800102935    steps: 122    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 2187   score: 1.0   memory length: 397477   epsilon: 0.41099356001029136    steps: 169    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 2188   score: 0.0   memory length: 397600   epsilon: 0.4107500200102898    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 2189   score: 2.0   memory length: 397780   epsilon: 0.41039362001028756    steps: 180    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 2190   score: 2.0   memory length: 397998   epsilon: 0.40996198001028483    steps: 218    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 2191   score: 3.0   memory length: 398226   epsilon: 0.409510540010282    steps: 228    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 2192   score: 1.0   memory length: 398394   epsilon: 0.40917790001027987    steps: 168    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 2193   score: 4.0   memory length: 398672   epsilon: 0.4086274600102764    steps: 278    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 2194   score: 4.0   memory length: 398947   epsilon: 0.40808296001027294    steps: 275    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 2195   score: 1.0   memory length: 399098   epsilon: 0.40778398001027105    steps: 151    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 2196   score: 0.0   memory length: 399221   epsilon: 0.4075404400102695    steps: 123    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 2197   score: 0.0   memory length: 399344   epsilon: 0.40729690001026797    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 2198   score: 7.0   memory length: 399643   epsilon: 0.4067048800102642    steps: 299    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 2199   score: 3.0   memory length: 399871   epsilon: 0.40625344001026137    steps: 228    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 2200   score: 1.0   memory length: 400022   epsilon: 0.4059544600102595    steps: 151    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 2201   score: 2.0   memory length: 400220   epsilon: 0.405562420010257    steps: 198    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 2202   score: 1.0   memory length: 400391   epsilon: 0.40522384001025485    steps: 171    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 2203   score: 3.0   memory length: 400640   epsilon: 0.40473082001025174    steps: 249    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 2204   score: 3.0   memory length: 400888   epsilon: 0.40423978001024863    steps: 248    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 2205   score: 1.0   memory length: 401038   epsilon: 0.40394278001024675    steps: 150    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 2206   score: 2.0   memory length: 401254   epsilon: 0.40351510001024404    steps: 216    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 2207   score: 3.0   memory length: 401503   epsilon: 0.4030220800102409    steps: 249    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 2208   score: 0.0   memory length: 401625   epsilon: 0.4027805200102394    steps: 122    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 2209   score: 2.0   memory length: 401823   epsilon: 0.4023884800102369    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 2210   score: 1.0   memory length: 401994   epsilon: 0.4020499000102348    steps: 171    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 2211   score: 0.0   memory length: 402116   epsilon: 0.40180834001023324    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 2212   score: 1.0   memory length: 402267   epsilon: 0.40150936001023135    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 2213   score: 0.0   memory length: 402390   epsilon: 0.4012658200102298    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 2214   score: 4.0   memory length: 402701   epsilon: 0.4006500400102259    steps: 311    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 2215   score: 2.0   memory length: 402883   epsilon: 0.40028968001022364    steps: 182    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 2216   score: 0.0   memory length: 403006   epsilon: 0.4000461400102221    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2217   score: 0.0   memory length: 403129   epsilon: 0.39980260001022055    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2218   score: 1.0   memory length: 403280   epsilon: 0.39950362001021866    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2219   score: 1.0   memory length: 403449   epsilon: 0.39916900001021655    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2220   score: 0.0   memory length: 403571   epsilon: 0.398927440010215    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 2221   score: 1.0   memory length: 403740   epsilon: 0.3985928200102129    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2222   score: 3.0   memory length: 403984   epsilon: 0.39810970001020984    steps: 244    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2223   score: 0.0   memory length: 404107   epsilon: 0.3978661600102083    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2224   score: 1.0   memory length: 404275   epsilon: 0.3975335200102062    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2225   score: 2.0   memory length: 404458   epsilon: 0.3971711800102039    steps: 183    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2226   score: 0.0   memory length: 404581   epsilon: 0.39692764001020236    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2227   score: 1.0   memory length: 404750   epsilon: 0.39659302001020025    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2228   score: 2.0   memory length: 404947   epsilon: 0.3962029600101978    steps: 197    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2229   score: 1.0   memory length: 405116   epsilon: 0.39586834001019566    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2230   score: 0.0   memory length: 405239   epsilon: 0.3956248000101941    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2231   score: 0.0   memory length: 405361   epsilon: 0.3953832400101926    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2232   score: 0.0   memory length: 405484   epsilon: 0.39513970001019105    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2233   score: 2.0   memory length: 405681   epsilon: 0.3947496400101886    steps: 197    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2234   score: 2.0   memory length: 405899   epsilon: 0.39431800001018585    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2235   score: 0.0   memory length: 406021   epsilon: 0.3940764400101843    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2236   score: 2.0   memory length: 406239   epsilon: 0.3936448000101816    steps: 218    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2237   score: 1.0   memory length: 406408   epsilon: 0.3933101800101795    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2238   score: 0.0   memory length: 406530   epsilon: 0.39306862001017795    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2239   score: 3.0   memory length: 406760   epsilon: 0.39261322001017507    steps: 230    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2240   score: 2.0   memory length: 406978   epsilon: 0.39218158001017234    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2241   score: 3.0   memory length: 407245   epsilon: 0.391652920010169    steps: 267    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2242   score: 0.0   memory length: 407368   epsilon: 0.39140938001016745    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2243   score: 2.0   memory length: 407585   epsilon: 0.39097972001016473    steps: 217    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2244   score: 2.0   memory length: 407801   epsilon: 0.390552040010162    steps: 216    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2245   score: 2.0   memory length: 407999   epsilon: 0.39016000001015955    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2246   score: 1.0   memory length: 408168   epsilon: 0.38982538001015743    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2247   score: 0.0   memory length: 408291   epsilon: 0.3895818400101559    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2248   score: 0.0   memory length: 408413   epsilon: 0.38934028001015436    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2249   score: 0.0   memory length: 408536   epsilon: 0.3890967400101528    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2250   score: 2.0   memory length: 408734   epsilon: 0.38870470001015034    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2251   score: 2.0   memory length: 408932   epsilon: 0.38831266001014786    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2252   score: 3.0   memory length: 409180   epsilon: 0.38782162001014475    steps: 248    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 2253   score: 0.0   memory length: 409303   epsilon: 0.3875780800101432    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2254   score: 2.0   memory length: 409483   epsilon: 0.38722168001014096    steps: 180    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2255   score: 2.0   memory length: 409681   epsilon: 0.3868296400101385    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2256   score: 0.0   memory length: 409804   epsilon: 0.38658610001013693    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2257   score: 1.0   memory length: 409973   epsilon: 0.3862514800101348    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2258   score: 0.0   memory length: 410096   epsilon: 0.3860079400101333    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2259   score: 1.0   memory length: 410267   epsilon: 0.38566936001013113    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2260   score: 0.0   memory length: 410390   epsilon: 0.3854258200101296    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2261   score: 2.0   memory length: 410608   epsilon: 0.38499418001012686    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2262   score: 3.0   memory length: 410838   epsilon: 0.384538780010124    steps: 230    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2263   score: 3.0   memory length: 411087   epsilon: 0.38404576001012086    steps: 249    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 2264   score: 1.0   memory length: 411256   epsilon: 0.38371114001011875    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2265   score: 0.0   memory length: 411378   epsilon: 0.3834695800101172    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2266   score: 5.0   memory length: 411686   epsilon: 0.38285974001011336    steps: 308    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 2267   score: 3.0   memory length: 411936   epsilon: 0.3823647400101102    steps: 250    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2268   score: 2.0   memory length: 412158   epsilon: 0.38192518001010745    steps: 222    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2269   score: 1.0   memory length: 412327   epsilon: 0.38159056001010533    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2270   score: 2.0   memory length: 412524   epsilon: 0.38120050001010286    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2271   score: 1.0   memory length: 412675   epsilon: 0.38090152001010097    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2272   score: 1.0   memory length: 412846   epsilon: 0.3805629400100988    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2273   score: 2.0   memory length: 413044   epsilon: 0.38017090001009635    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2274   score: 0.0   memory length: 413166   epsilon: 0.3799293400100948    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2275   score: 1.0   memory length: 413335   epsilon: 0.3795947200100927    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2276   score: 3.0   memory length: 413564   epsilon: 0.37914130001008983    steps: 229    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2277   score: 3.0   memory length: 413814   epsilon: 0.3786463000100867    steps: 250    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2278   score: 0.0   memory length: 413936   epsilon: 0.37840474001008517    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2279   score: 1.0   memory length: 414105   epsilon: 0.37807012001008306    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2280   score: 0.0   memory length: 414228   epsilon: 0.3778265800100815    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2281   score: 1.0   memory length: 414379   epsilon: 0.3775276000100796    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2282   score: 3.0   memory length: 414625   epsilon: 0.37704052001007654    steps: 246    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2283   score: 0.0   memory length: 414747   epsilon: 0.376798960010075    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2284   score: 0.0   memory length: 414870   epsilon: 0.37655542001007347    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 2285   score: 0.0   memory length: 414993   epsilon: 0.37631188001007193    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2286   score: 1.0   memory length: 415143   epsilon: 0.37601488001007005    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2287   score: 0.0   memory length: 415266   epsilon: 0.3757713400100685    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2288   score: 3.0   memory length: 415494   epsilon: 0.37531990001006565    steps: 228    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2289   score: 1.0   memory length: 415665   epsilon: 0.3749813200100635    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2290   score: 7.0   memory length: 416087   epsilon: 0.3741457600100582    steps: 422    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2291   score: 1.0   memory length: 416238   epsilon: 0.37384678001005633    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2292   score: 2.0   memory length: 416418   epsilon: 0.3734903800100541    steps: 180    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2293   score: 2.0   memory length: 416615   epsilon: 0.3731003200100516    steps: 197    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 2294   score: 0.0   memory length: 416738   epsilon: 0.37285678001005007    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2295   score: 0.0   memory length: 416860   epsilon: 0.37261522001004854    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2296   score: 0.0   memory length: 416983   epsilon: 0.372371680010047    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2297   score: 4.0   memory length: 417277   epsilon: 0.3717895600100433    steps: 294    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2298   score: 2.0   memory length: 417495   epsilon: 0.3713579200100406    steps: 218    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2299   score: 2.0   memory length: 417713   epsilon: 0.37092628001003786    steps: 218    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2300   score: 1.0   memory length: 417864   epsilon: 0.37062730001003596    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2301   score: 0.0   memory length: 417987   epsilon: 0.3703837600100344    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 2302   score: 0.0   memory length: 418110   epsilon: 0.3701402200100329    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 2303   score: 2.0   memory length: 418308   epsilon: 0.3697481800100304    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 2304   score: 1.0   memory length: 418477   epsilon: 0.3694135600100283    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 2305   score: 0.0   memory length: 418600   epsilon: 0.36917002001002674    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 2306   score: 0.0   memory length: 418723   epsilon: 0.3689264800100252    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 2307   score: 0.0   memory length: 418845   epsilon: 0.3686849200100237    steps: 122    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 2308   score: 0.0   memory length: 418968   epsilon: 0.36844138001002213    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 2309   score: 0.0   memory length: 419091   epsilon: 0.3681978400100206    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 2310   score: 0.0   memory length: 419214   epsilon: 0.36795430001001905    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 2311   score: 3.0   memory length: 419459   epsilon: 0.367469200010016    steps: 245    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 2312   score: 2.0   memory length: 419677   epsilon: 0.36703756001001325    steps: 218    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 2313   score: 2.0   memory length: 419874   epsilon: 0.3666475000100108    steps: 197    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 2314   score: 1.0   memory length: 420025   epsilon: 0.3663485200100089    steps: 151    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 2315   score: 3.0   memory length: 420253   epsilon: 0.36589708001000604    steps: 228    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 2316   score: 1.0   memory length: 420404   epsilon: 0.36559810001000415    steps: 151    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 2317   score: 2.0   memory length: 420602   epsilon: 0.36520606001000167    steps: 198    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 2318   score: 4.0   memory length: 420898   epsilon: 0.36461998000999796    steps: 296    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 2319   score: 0.0   memory length: 421020   epsilon: 0.36437842000999643    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 2320   score: 1.0   memory length: 421189   epsilon: 0.3640438000099943    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 2321   score: 0.0   memory length: 421311   epsilon: 0.3638022400099928    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 2322   score: 1.0   memory length: 421480   epsilon: 0.36346762000999067    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 2323   score: 1.0   memory length: 421649   epsilon: 0.36313300000998855    steps: 169    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 2324   score: 2.0   memory length: 421866   epsilon: 0.36270334000998583    steps: 217    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 2325   score: 1.0   memory length: 422036   epsilon: 0.3623667400099837    steps: 170    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 2326   score: 2.0   memory length: 422235   epsilon: 0.3619727200099812    steps: 199    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 2327   score: 1.0   memory length: 422404   epsilon: 0.3616381000099791    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 2328   score: 1.0   memory length: 422573   epsilon: 0.361303480009977    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 2329   score: 2.0   memory length: 422791   epsilon: 0.36087184000997424    steps: 218    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 2330   score: 4.0   memory length: 423086   epsilon: 0.36028774000997055    steps: 295    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2331   score: 0.0   memory length: 423209   epsilon: 0.360044200009969    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2332   score: 0.0   memory length: 423332   epsilon: 0.35980066000996747    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2333   score: 0.0   memory length: 423455   epsilon: 0.3595571200099659    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 2334   score: 0.0   memory length: 423577   epsilon: 0.3593155600099644    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 2335   score: 2.0   memory length: 423774   epsilon: 0.35892550000996193    steps: 197    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 2336   score: 0.0   memory length: 423896   epsilon: 0.3586839400099604    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 2337   score: 0.0   memory length: 424019   epsilon: 0.35844040000995886    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 2338   score: 0.0   memory length: 424142   epsilon: 0.3581968600099573    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 2339   score: 0.0   memory length: 424265   epsilon: 0.3579533200099558    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 2340   score: 0.0   memory length: 424388   epsilon: 0.35770978000995424    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 2341   score: 1.0   memory length: 424559   epsilon: 0.3573712000099521    steps: 171    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 2342   score: 4.0   memory length: 424835   epsilon: 0.35682472000994864    steps: 276    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 2343   score: 0.0   memory length: 424958   epsilon: 0.3565811800099471    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 2344   score: 1.0   memory length: 425109   epsilon: 0.3562822000099452    steps: 151    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 2345   score: 0.0   memory length: 425231   epsilon: 0.3560406400099437    steps: 122    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 2346   score: 2.0   memory length: 425448   epsilon: 0.35561098000994096    steps: 217    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 2347   score: 3.0   memory length: 425716   epsilon: 0.3550803400099376    steps: 268    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 2348   score: 0.0   memory length: 425839   epsilon: 0.35483680000993606    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 2349   score: 2.0   memory length: 426055   epsilon: 0.35440912000993335    steps: 216    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 2350   score: 0.0   memory length: 426178   epsilon: 0.3541655800099318    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 2351   score: 3.0   memory length: 426445   epsilon: 0.35363692000992847    steps: 267    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 2352   score: 0.0   memory length: 426568   epsilon: 0.3533933800099269    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 2353   score: 0.0   memory length: 426691   epsilon: 0.3531498400099254    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 2354   score: 3.0   memory length: 426917   epsilon: 0.35270236000992256    steps: 226    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 2355   score: 2.0   memory length: 427114   epsilon: 0.3523123000099201    steps: 197    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 2356   score: 3.0   memory length: 427381   epsilon: 0.35178364000991674    steps: 267    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 2357   score: 1.0   memory length: 427551   epsilon: 0.3514470400099146    steps: 170    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 2358   score: 2.0   memory length: 427749   epsilon: 0.35105500000991213    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 2359   score: 0.0   memory length: 427872   epsilon: 0.3508114600099106    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 2360   score: 0.0   memory length: 427995   epsilon: 0.35056792000990905    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 2361   score: 5.0   memory length: 428339   epsilon: 0.34988680000990474    steps: 344    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 2362   score: 2.0   memory length: 428539   epsilon: 0.34949080000990224    steps: 200    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 2363   score: 0.0   memory length: 428661   epsilon: 0.3492492400099007    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 2364   score: 2.0   memory length: 428858   epsilon: 0.34885918000989824    steps: 197    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 2365   score: 1.0   memory length: 429030   epsilon: 0.3485186200098961    steps: 172    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 2366   score: 0.0   memory length: 429152   epsilon: 0.34827706000989456    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 2367   score: 2.0   memory length: 429333   epsilon: 0.3479186800098923    steps: 181    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 2368   score: 0.0   memory length: 429456   epsilon: 0.34767514000989075    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 2369   score: 2.0   memory length: 429654   epsilon: 0.34728310000988827    steps: 198    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 2370   score: 1.0   memory length: 429805   epsilon: 0.3469841200098864    steps: 151    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 2371   score: 1.0   memory length: 429956   epsilon: 0.3466851400098845    steps: 151    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 2372   score: 1.0   memory length: 430107   epsilon: 0.3463861600098826    steps: 151    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 2373   score: 0.0   memory length: 430230   epsilon: 0.34614262000988105    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 2374   score: 2.0   memory length: 430430   epsilon: 0.34574662000987855    steps: 200    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 2375   score: 0.0   memory length: 430553   epsilon: 0.345503080009877    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 2376   score: 2.0   memory length: 430750   epsilon: 0.34511302000987454    steps: 197    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 2377   score: 1.0   memory length: 430901   epsilon: 0.34481404000987265    steps: 151    lr: 0.0001     evaluation reward: 1.19\n",
            "episode: 2378   score: 2.0   memory length: 431118   epsilon: 0.34438438000986993    steps: 217    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 2379   score: 2.0   memory length: 431338   epsilon: 0.34394878000986717    steps: 220    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 2380   score: 1.0   memory length: 431510   epsilon: 0.343608220009865    steps: 172    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 2381   score: 0.0   memory length: 431632   epsilon: 0.3433666600098635    steps: 122    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 2382   score: 1.0   memory length: 431804   epsilon: 0.34302610000986133    steps: 172    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 2383   score: 4.0   memory length: 432091   epsilon: 0.34245784000985774    steps: 287    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 2384   score: 1.0   memory length: 432259   epsilon: 0.34212520000985563    steps: 168    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 2385   score: 0.0   memory length: 432382   epsilon: 0.3418816600098541    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 2386   score: 4.0   memory length: 432655   epsilon: 0.3413411200098507    steps: 273    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 2387   score: 1.0   memory length: 432824   epsilon: 0.34100650000984856    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 2388   score: 3.0   memory length: 433036   epsilon: 0.3405867400098459    steps: 212    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 2389   score: 3.0   memory length: 433301   epsilon: 0.3400620400098426    steps: 265    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 2390   score: 1.0   memory length: 433451   epsilon: 0.3397650400098407    steps: 150    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 2391   score: 0.0   memory length: 433574   epsilon: 0.33952150000983916    steps: 123    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 2392   score: 2.0   memory length: 433789   epsilon: 0.33909580000983647    steps: 215    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 2393   score: 0.0   memory length: 433912   epsilon: 0.3388522600098349    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 2394   score: 2.0   memory length: 434128   epsilon: 0.3384245800098322    steps: 216    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 2395   score: 0.0   memory length: 434251   epsilon: 0.3381810400098307    steps: 123    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 2396   score: 3.0   memory length: 434517   epsilon: 0.33765436000982735    steps: 266    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 2397   score: 2.0   memory length: 434697   epsilon: 0.3372979600098251    steps: 180    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 2398   score: 2.0   memory length: 434894   epsilon: 0.3369079000098226    steps: 197    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 2399   score: 3.0   memory length: 435166   epsilon: 0.3363693400098192    steps: 272    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 2400   score: 3.0   memory length: 435435   epsilon: 0.33583672000981585    steps: 269    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 2401   score: 0.0   memory length: 435558   epsilon: 0.3355931800098143    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 2402   score: 1.0   memory length: 435726   epsilon: 0.3352605400098122    steps: 168    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 2403   score: 0.0   memory length: 435848   epsilon: 0.3350189800098107    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 2404   score: 2.0   memory length: 436065   epsilon: 0.33458932000980796    steps: 217    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 2405   score: 5.0   memory length: 436376   epsilon: 0.33397354000980406    steps: 311    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 2406   score: 1.0   memory length: 436545   epsilon: 0.33363892000980194    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 2407   score: 1.0   memory length: 436696   epsilon: 0.33333994000980005    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 2408   score: 2.0   memory length: 436876   epsilon: 0.3329835400097978    steps: 180    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2409   score: 0.0   memory length: 436999   epsilon: 0.33274000000979626    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2410   score: 2.0   memory length: 437219   epsilon: 0.3323044000097935    steps: 220    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2411   score: 4.0   memory length: 437494   epsilon: 0.33175990000979005    steps: 275    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2412   score: 3.0   memory length: 437741   epsilon: 0.33127084000978696    steps: 247    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2413   score: 2.0   memory length: 437959   epsilon: 0.33083920000978423    steps: 218    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2414   score: 0.0   memory length: 438082   epsilon: 0.3305956600097827    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2415   score: 1.0   memory length: 438253   epsilon: 0.33025708000978055    steps: 171    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2416   score: 0.0   memory length: 438376   epsilon: 0.330013540009779    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2417   score: 3.0   memory length: 438604   epsilon: 0.32956210000977615    steps: 228    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2418   score: 2.0   memory length: 438823   epsilon: 0.3291284800097734    steps: 219    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2419   score: 2.0   memory length: 439021   epsilon: 0.3287364400097709    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2420   score: 0.0   memory length: 439144   epsilon: 0.3284929000097694    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2421   score: 0.0   memory length: 439267   epsilon: 0.32824936000976784    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2422   score: 0.0   memory length: 439390   epsilon: 0.3280058200097663    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2423   score: 0.0   memory length: 439512   epsilon: 0.3277642600097648    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 2424   score: 2.0   memory length: 439709   epsilon: 0.3273742000097623    steps: 197    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 2425   score: 4.0   memory length: 440025   epsilon: 0.32674852000975835    steps: 316    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2426   score: 1.0   memory length: 440195   epsilon: 0.3264119200097562    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2427   score: 0.0   memory length: 440317   epsilon: 0.3261703600097547    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2428   score: 1.0   memory length: 440487   epsilon: 0.32583376000975256    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2429   score: 3.0   memory length: 440731   epsilon: 0.3253506400097495    steps: 244    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2430   score: 0.0   memory length: 440854   epsilon: 0.32510710000974796    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 2431   score: 0.0   memory length: 440977   epsilon: 0.3248635600097464    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 2432   score: 4.0   memory length: 441273   epsilon: 0.3242774800097427    steps: 296    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2433   score: 3.0   memory length: 441540   epsilon: 0.32374882000973937    steps: 267    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2434   score: 2.0   memory length: 441738   epsilon: 0.3233567800097369    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2435   score: 0.0   memory length: 441860   epsilon: 0.32311522000973536    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2436   score: 2.0   memory length: 442057   epsilon: 0.3227251600097329    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2437   score: 2.0   memory length: 442259   epsilon: 0.32232520000973036    steps: 202    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 2438   score: 1.0   memory length: 442431   epsilon: 0.3219846400097282    steps: 172    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2439   score: 1.0   memory length: 442602   epsilon: 0.32164606000972606    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2440   score: 4.0   memory length: 442902   epsilon: 0.3210520600097223    steps: 300    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2441   score: 1.0   memory length: 443053   epsilon: 0.3207530800097204    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2442   score: 4.0   memory length: 443349   epsilon: 0.3201670000097167    steps: 296    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2443   score: 1.0   memory length: 443518   epsilon: 0.3198323800097146    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2444   score: 4.0   memory length: 443834   epsilon: 0.31920670000971063    steps: 316    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2445   score: 0.0   memory length: 443957   epsilon: 0.3189631600097091    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2446   score: 3.0   memory length: 444184   epsilon: 0.31851370000970625    steps: 227    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2447   score: 1.0   memory length: 444354   epsilon: 0.3181771000097041    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2448   score: 1.0   memory length: 444524   epsilon: 0.317840500009702    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2449   score: 3.0   memory length: 444771   epsilon: 0.3173514400096989    steps: 247    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2450   score: 1.0   memory length: 444921   epsilon: 0.317054440009697    steps: 150    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2451   score: 0.0   memory length: 445044   epsilon: 0.3168109000096955    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2452   score: 1.0   memory length: 445213   epsilon: 0.31647628000969336    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2453   score: 1.0   memory length: 445364   epsilon: 0.31617730000969146    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2454   score: 1.0   memory length: 445533   epsilon: 0.31584268000968935    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2455   score: 0.0   memory length: 445655   epsilon: 0.3156011200096878    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2456   score: 5.0   memory length: 445992   epsilon: 0.3149338600096836    steps: 337    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2457   score: 2.0   memory length: 446189   epsilon: 0.31454380000968113    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2458   score: 2.0   memory length: 446406   epsilon: 0.3141141400096784    steps: 217    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2459   score: 1.0   memory length: 446575   epsilon: 0.3137795200096763    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2460   score: 2.0   memory length: 446773   epsilon: 0.3133874800096738    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2461   score: 4.0   memory length: 447071   epsilon: 0.3127974400096701    steps: 298    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2462   score: 0.0   memory length: 447194   epsilon: 0.31255390000966854    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2463   score: 3.0   memory length: 447459   epsilon: 0.3120292000096652    steps: 265    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2464   score: 3.0   memory length: 447724   epsilon: 0.3115045000096619    steps: 265    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2465   score: 6.0   memory length: 448099   epsilon: 0.3107620000096572    steps: 375    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 2466   score: 1.0   memory length: 448268   epsilon: 0.3104273800096551    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 2467   score: 2.0   memory length: 448484   epsilon: 0.3099997000096524    steps: 216    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 2468   score: 2.0   memory length: 448703   epsilon: 0.30956608000964964    steps: 219    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 2469   score: 2.0   memory length: 448901   epsilon: 0.30917404000964716    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 2470   score: 1.0   memory length: 449073   epsilon: 0.308833480009645    steps: 172    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 2471   score: 2.0   memory length: 449271   epsilon: 0.3084414400096425    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 2472   score: 4.0   memory length: 449570   epsilon: 0.3078494200096388    steps: 299    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 2473   score: 2.0   memory length: 449768   epsilon: 0.3074573800096363    steps: 198    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 2474   score: 3.0   memory length: 450011   epsilon: 0.30697624000963325    steps: 243    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 2475   score: 0.0   memory length: 450134   epsilon: 0.3067327000096317    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 2476   score: 2.0   memory length: 450332   epsilon: 0.30634066000962923    steps: 198    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 2477   score: 0.0   memory length: 450454   epsilon: 0.3060991000096277    steps: 122    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 2478   score: 1.0   memory length: 450622   epsilon: 0.3057664600096256    steps: 168    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 2479   score: 1.0   memory length: 450793   epsilon: 0.30542788000962345    steps: 171    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 2480   score: 2.0   memory length: 451013   epsilon: 0.3049922800096207    steps: 220    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 2481   score: 2.0   memory length: 451231   epsilon: 0.30456064000961797    steps: 218    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 2482   score: 1.0   memory length: 451381   epsilon: 0.3042636400096161    steps: 150    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 2483   score: 1.0   memory length: 451551   epsilon: 0.30392704000961396    steps: 170    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 2484   score: 4.0   memory length: 451826   epsilon: 0.3033825400096105    steps: 275    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 2485   score: 0.0   memory length: 451949   epsilon: 0.30313900000960897    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 2486   score: 3.0   memory length: 452175   epsilon: 0.30269152000960614    steps: 226    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 2487   score: 3.0   memory length: 452441   epsilon: 0.3021648400096028    steps: 266    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 2488   score: 7.0   memory length: 452857   epsilon: 0.3013411600095976    steps: 416    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 2489   score: 0.0   memory length: 452980   epsilon: 0.30109762000959606    steps: 123    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 2490   score: 1.0   memory length: 453152   epsilon: 0.3007570600095939    steps: 172    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 2491   score: 3.0   memory length: 453378   epsilon: 0.30030958000959107    steps: 226    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 2492   score: 0.0   memory length: 453500   epsilon: 0.30006802000958954    steps: 122    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 2493   score: 0.0   memory length: 453623   epsilon: 0.299824480009588    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 2494   score: 3.0   memory length: 453849   epsilon: 0.29937700000958517    steps: 226    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 2495   score: 2.0   memory length: 454047   epsilon: 0.2989849600095827    steps: 198    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 2496   score: 0.0   memory length: 454170   epsilon: 0.29874142000958115    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 2497   score: 0.0   memory length: 454292   epsilon: 0.2984998600095796    steps: 122    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 2498   score: 1.0   memory length: 454462   epsilon: 0.2981632600095775    steps: 170    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 2499   score: 1.0   memory length: 454612   epsilon: 0.2978662600095756    steps: 150    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 2500   score: 0.0   memory length: 454734   epsilon: 0.2976247000095741    steps: 122    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 2501   score: 1.0   memory length: 454884   epsilon: 0.2973277000095722    steps: 150    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 2502   score: 2.0   memory length: 455100   epsilon: 0.2969000200095695    steps: 216    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 2503   score: 1.0   memory length: 455272   epsilon: 0.29655946000956734    steps: 172    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 2504   score: 1.0   memory length: 455442   epsilon: 0.2962228600095652    steps: 170    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 2505   score: 4.0   memory length: 455758   epsilon: 0.29559718000956126    steps: 316    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 2506   score: 1.0   memory length: 455928   epsilon: 0.2952605800095591    steps: 170    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 2507   score: 2.0   memory length: 456125   epsilon: 0.29487052000955666    steps: 197    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 2508   score: 1.0   memory length: 456276   epsilon: 0.29457154000955477    steps: 151    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 2509   score: 1.0   memory length: 456445   epsilon: 0.29423692000955265    steps: 169    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 2510   score: 3.0   memory length: 456712   epsilon: 0.2937082600095493    steps: 267    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 2511   score: 4.0   memory length: 456986   epsilon: 0.29316574000954587    steps: 274    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 2512   score: 1.0   memory length: 457156   epsilon: 0.29282914000954374    steps: 170    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 2513   score: 2.0   memory length: 457375   epsilon: 0.292395520009541    steps: 219    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 2514   score: 0.0   memory length: 457498   epsilon: 0.29215198000953946    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 2515   score: 1.0   memory length: 457649   epsilon: 0.29185300000953757    steps: 151    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 2516   score: 2.0   memory length: 457849   epsilon: 0.29145700000953506    steps: 200    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 2517   score: 2.0   memory length: 458047   epsilon: 0.2910649600095326    steps: 198    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 2518   score: 5.0   memory length: 458375   epsilon: 0.29041552000952847    steps: 328    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 2519   score: 5.0   memory length: 458696   epsilon: 0.28977994000952445    steps: 321    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 2520   score: 0.0   memory length: 458819   epsilon: 0.2895364000095229    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 2521   score: 2.0   memory length: 459018   epsilon: 0.2891423800095204    steps: 199    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 2522   score: 2.0   memory length: 459221   epsilon: 0.2887404400095179    steps: 203    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 2523   score: 0.0   memory length: 459343   epsilon: 0.28849888000951635    steps: 122    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 2524   score: 1.0   memory length: 459512   epsilon: 0.28816426000951423    steps: 169    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 2525   score: 4.0   memory length: 459787   epsilon: 0.2876197600095108    steps: 275    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 2526   score: 1.0   memory length: 459959   epsilon: 0.28727920000950863    steps: 172    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 2527   score: 0.0   memory length: 460082   epsilon: 0.2870356600095071    steps: 123    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 2528   score: 2.0   memory length: 460280   epsilon: 0.2866436200095046    steps: 198    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 2529   score: 0.0   memory length: 460403   epsilon: 0.28640008000950307    steps: 123    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 2530   score: 6.0   memory length: 460796   epsilon: 0.28562194000949814    steps: 393    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 2531   score: 0.0   memory length: 460919   epsilon: 0.2853784000094966    steps: 123    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 2532   score: 1.0   memory length: 461070   epsilon: 0.2850794200094947    steps: 151    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 2533   score: 2.0   memory length: 461269   epsilon: 0.2846854000094922    steps: 199    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 2534   score: 0.0   memory length: 461391   epsilon: 0.2844438400094907    steps: 122    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 2535   score: 2.0   memory length: 461589   epsilon: 0.2840518000094882    steps: 198    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 2536   score: 0.0   memory length: 461712   epsilon: 0.28380826000948667    steps: 123    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 2537   score: 3.0   memory length: 461922   epsilon: 0.28339246000948404    steps: 210    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 2538   score: 0.0   memory length: 462044   epsilon: 0.2831509000094825    steps: 122    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 2539   score: 1.0   memory length: 462214   epsilon: 0.2828143000094804    steps: 170    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 2540   score: 0.0   memory length: 462336   epsilon: 0.28257274000947885    steps: 122    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 2541   score: 2.0   memory length: 462515   epsilon: 0.2822183200094766    steps: 179    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 2542   score: 3.0   memory length: 462763   epsilon: 0.2817272800094735    steps: 248    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 2543   score: 0.0   memory length: 462886   epsilon: 0.28148374000947196    steps: 123    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 2544   score: 0.0   memory length: 463008   epsilon: 0.28124218000947043    steps: 122    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 2545   score: 1.0   memory length: 463159   epsilon: 0.28094320000946854    steps: 151    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 2546   score: 1.0   memory length: 463329   epsilon: 0.2806066000094664    steps: 170    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 2547   score: 1.0   memory length: 463480   epsilon: 0.2803076200094645    steps: 151    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 2548   score: 2.0   memory length: 463678   epsilon: 0.27991558000946204    steps: 198    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 2549   score: 0.0   memory length: 463801   epsilon: 0.2796720400094605    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 2550   score: 4.0   memory length: 464081   epsilon: 0.279117640009457    steps: 280    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 2551   score: 2.0   memory length: 464278   epsilon: 0.2787275800094545    steps: 197    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 2552   score: 3.0   memory length: 464504   epsilon: 0.2782801000094517    steps: 226    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 2553   score: 0.0   memory length: 464627   epsilon: 0.27803656000945015    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 2554   score: 1.0   memory length: 464797   epsilon: 0.277699960009448    steps: 170    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 2555   score: 1.0   memory length: 464967   epsilon: 0.2773633600094459    steps: 170    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 2556   score: 1.0   memory length: 465117   epsilon: 0.277066360009444    steps: 150    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 2557   score: 1.0   memory length: 465267   epsilon: 0.27676936000944213    steps: 150    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 2558   score: 2.0   memory length: 465484   epsilon: 0.2763397000094394    steps: 217    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 2559   score: 6.0   memory length: 465736   epsilon: 0.27584074000943626    steps: 252    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 2560   score: 0.0   memory length: 465859   epsilon: 0.2755972000094347    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 2561   score: 0.0   memory length: 465981   epsilon: 0.2753556400094332    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 2562   score: 1.0   memory length: 466150   epsilon: 0.27502102000943107    steps: 169    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 2563   score: 1.0   memory length: 466320   epsilon: 0.27468442000942894    steps: 170    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 2564   score: 0.0   memory length: 466442   epsilon: 0.2744428600094274    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 2565   score: 0.0   memory length: 466565   epsilon: 0.2741993200094259    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2566   score: 2.0   memory length: 466782   epsilon: 0.27376966000942315    steps: 217    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2567   score: 2.0   memory length: 466962   epsilon: 0.2734132600094209    steps: 180    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2568   score: 0.0   memory length: 467084   epsilon: 0.27317170000941937    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2569   score: 0.0   memory length: 467206   epsilon: 0.27293014000941784    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2570   score: 3.0   memory length: 467451   epsilon: 0.2724450400094148    steps: 245    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2571   score: 0.0   memory length: 467574   epsilon: 0.27220150000941323    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2572   score: 1.0   memory length: 467746   epsilon: 0.2718609400094111    steps: 172    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2573   score: 0.0   memory length: 467868   epsilon: 0.27161938000940955    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2574   score: 1.0   memory length: 468036   epsilon: 0.27128674000940745    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2575   score: 3.0   memory length: 468248   epsilon: 0.2708669800094048    steps: 212    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 2576   score: 1.0   memory length: 468398   epsilon: 0.2705699800094029    steps: 150    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2577   score: 3.0   memory length: 468623   epsilon: 0.2701244800094001    steps: 225    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2578   score: 0.0   memory length: 468745   epsilon: 0.26988292000939856    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2579   score: 3.0   memory length: 468970   epsilon: 0.26943742000939574    steps: 225    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2580   score: 3.0   memory length: 469195   epsilon: 0.2689919200093929    steps: 225    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2581   score: 0.0   memory length: 469318   epsilon: 0.2687483800093914    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2582   score: 0.0   memory length: 469440   epsilon: 0.26850682000938986    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2583   score: 1.0   memory length: 469609   epsilon: 0.26817220000938774    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2584   score: 2.0   memory length: 469827   epsilon: 0.267740560009385    steps: 218    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2585   score: 1.0   memory length: 469998   epsilon: 0.26740198000938287    steps: 171    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 2586   score: 0.0   memory length: 470121   epsilon: 0.2671584400093813    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2587   score: 2.0   memory length: 470319   epsilon: 0.26676640000937885    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2588   score: 2.0   memory length: 470517   epsilon: 0.26637436000937637    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2589   score: 0.0   memory length: 470640   epsilon: 0.2661308200093748    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2590   score: 1.0   memory length: 470808   epsilon: 0.2657981800093727    steps: 168    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2591   score: 0.0   memory length: 470931   epsilon: 0.2655546400093712    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2592   score: 1.0   memory length: 471100   epsilon: 0.26522002000936906    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2593   score: 0.0   memory length: 471223   epsilon: 0.2649764800093675    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2594   score: 3.0   memory length: 471448   epsilon: 0.2645309800093647    steps: 225    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2595   score: 2.0   memory length: 471664   epsilon: 0.264103300009362    steps: 216    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2596   score: 0.0   memory length: 471787   epsilon: 0.26385976000936046    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2597   score: 1.0   memory length: 471957   epsilon: 0.2635231600093583    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2598   score: 2.0   memory length: 472173   epsilon: 0.2630954800093556    steps: 216    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2599   score: 4.0   memory length: 472423   epsilon: 0.2626004800093525    steps: 250    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2600   score: 2.0   memory length: 472643   epsilon: 0.26216488000934973    steps: 220    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2601   score: 1.0   memory length: 472812   epsilon: 0.2618302600093476    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2602   score: 0.0   memory length: 472935   epsilon: 0.2615867200093461    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2603   score: 4.0   memory length: 473232   epsilon: 0.26099866000934235    steps: 297    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2604   score: 0.0   memory length: 473355   epsilon: 0.2607551200093408    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2605   score: 2.0   memory length: 473552   epsilon: 0.26036506000933834    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2606   score: 2.0   memory length: 473767   epsilon: 0.25993936000933565    steps: 215    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 2607   score: 0.0   memory length: 473889   epsilon: 0.2596978000093341    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2608   score: 3.0   memory length: 474136   epsilon: 0.25920874000933103    steps: 247    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 2609   score: 2.0   memory length: 474338   epsilon: 0.2588087800093285    steps: 202    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2610   score: 2.0   memory length: 474535   epsilon: 0.25841872000932603    steps: 197    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 2611   score: 1.0   memory length: 474686   epsilon: 0.25811974000932414    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2612   score: 2.0   memory length: 474883   epsilon: 0.25772968000932167    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2613   score: 5.0   memory length: 475212   epsilon: 0.25707826000931755    steps: 329    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2614   score: 0.0   memory length: 475335   epsilon: 0.256834720009316    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2615   score: 3.0   memory length: 475604   epsilon: 0.25630210000931264    steps: 269    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2616   score: 1.0   memory length: 475774   epsilon: 0.2559655000093105    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2617   score: 1.0   memory length: 475943   epsilon: 0.2556308800093084    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2618   score: 0.0   memory length: 476066   epsilon: 0.25538734000930685    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2619   score: 0.0   memory length: 476189   epsilon: 0.2551438000093053    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 2620   score: 2.0   memory length: 476387   epsilon: 0.25475176000930283    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2621   score: 1.0   memory length: 476559   epsilon: 0.2544112000093007    steps: 172    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2622   score: 0.0   memory length: 476682   epsilon: 0.25416766000929913    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 2623   score: 2.0   memory length: 476900   epsilon: 0.2537360200092964    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2624   score: 0.0   memory length: 477023   epsilon: 0.25349248000929486    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 2625   score: 2.0   memory length: 477221   epsilon: 0.2531004400092924    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 2626   score: 0.0   memory length: 477344   epsilon: 0.25285690000929084    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 2627   score: 0.0   memory length: 477467   epsilon: 0.2526133600092893    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 2628   score: 3.0   memory length: 477711   epsilon: 0.25213024000928624    steps: 244    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 2629   score: 2.0   memory length: 477909   epsilon: 0.25173820000928376    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 2630   score: 3.0   memory length: 478158   epsilon: 0.25124518000928064    steps: 249    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 2631   score: 1.0   memory length: 478326   epsilon: 0.25091254000927854    steps: 168    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 2632   score: 2.0   memory length: 478523   epsilon: 0.25052248000927607    steps: 197    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 2633   score: 5.0   memory length: 478845   epsilon: 0.24988492000927204    steps: 322    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2634   score: 0.0   memory length: 478968   epsilon: 0.2496413800092705    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2635   score: 1.0   memory length: 479118   epsilon: 0.24934438000926862    steps: 150    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2636   score: 0.0   memory length: 479241   epsilon: 0.24910084000926708    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2637   score: 3.0   memory length: 479487   epsilon: 0.248613760009264    steps: 246    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2638   score: 1.0   memory length: 479656   epsilon: 0.24827914000926188    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2639   score: 1.0   memory length: 479825   epsilon: 0.24794452000925976    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2640   score: 3.0   memory length: 480050   epsilon: 0.24749902000925694    steps: 225    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2641   score: 2.0   memory length: 480248   epsilon: 0.24710698000925446    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2642   score: 3.0   memory length: 480515   epsilon: 0.24657832000925112    steps: 267    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2643   score: 3.0   memory length: 480765   epsilon: 0.24608332000924799    steps: 250    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2644   score: 0.0   memory length: 480888   epsilon: 0.24583978000924644    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2645   score: 4.0   memory length: 481188   epsilon: 0.2452457800092427    steps: 300    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2646   score: 0.0   memory length: 481310   epsilon: 0.24500422000924116    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2647   score: 1.0   memory length: 481480   epsilon: 0.24466762000923903    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2648   score: 3.0   memory length: 481748   epsilon: 0.24413698000923567    steps: 268    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2649   score: 0.0   memory length: 481871   epsilon: 0.24389344000923413    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2650   score: 3.0   memory length: 482119   epsilon: 0.24340240000923102    steps: 248    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2651   score: 2.0   memory length: 482317   epsilon: 0.24301036000922854    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2652   score: 0.0   memory length: 482439   epsilon: 0.24276880000922701    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2653   score: 1.0   memory length: 482589   epsilon: 0.24247180000922514    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2654   score: 2.0   memory length: 482786   epsilon: 0.24208174000922267    steps: 197    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 2655   score: 2.0   memory length: 482984   epsilon: 0.2416897000092202    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2656   score: 2.0   memory length: 483181   epsilon: 0.24129964000921772    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2657   score: 0.0   memory length: 483304   epsilon: 0.24105610000921618    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2658   score: 3.0   memory length: 483569   epsilon: 0.24053140000921286    steps: 265    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2659   score: 0.0   memory length: 483692   epsilon: 0.24028786000921132    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2660   score: 0.0   memory length: 483815   epsilon: 0.24004432000920978    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2661   score: 1.0   memory length: 483986   epsilon: 0.23970574000920764    steps: 171    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2662   score: 2.0   memory length: 484202   epsilon: 0.23927806000920493    steps: 216    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2663   score: 0.0   memory length: 484325   epsilon: 0.2390345200092034    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2664   score: 3.0   memory length: 484571   epsilon: 0.2385474400092003    steps: 246    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 2665   score: 0.0   memory length: 484694   epsilon: 0.23830390000919877    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 2666   score: 4.0   memory length: 485011   epsilon: 0.2376762400091948    steps: 317    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2667   score: 3.0   memory length: 485238   epsilon: 0.23722678000919195    steps: 227    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2668   score: 0.0   memory length: 485360   epsilon: 0.23698522000919042    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2669   score: 1.0   memory length: 485529   epsilon: 0.2366506000091883    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2670   score: 2.0   memory length: 485726   epsilon: 0.23626054000918584    steps: 197    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2671   score: 2.0   memory length: 485945   epsilon: 0.2358269200091831    steps: 219    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 2672   score: 3.0   memory length: 486192   epsilon: 0.23533786000918    steps: 247    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2673   score: 4.0   memory length: 486454   epsilon: 0.23481910000917672    steps: 262    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2674   score: 1.0   memory length: 486623   epsilon: 0.2344844800091746    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2675   score: 2.0   memory length: 486841   epsilon: 0.23405284000917187    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2676   score: 1.0   memory length: 487010   epsilon: 0.23371822000916975    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2677   score: 1.0   memory length: 487181   epsilon: 0.2333796400091676    steps: 171    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2678   score: 1.0   memory length: 487352   epsilon: 0.23304106000916547    steps: 171    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2679   score: 0.0   memory length: 487474   epsilon: 0.23279950000916394    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2680   score: 1.0   memory length: 487625   epsilon: 0.23250052000916205    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2681   score: 0.0   memory length: 487748   epsilon: 0.2322569800091605    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2682   score: 2.0   memory length: 487927   epsilon: 0.23190256000915827    steps: 179    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2683   score: 2.0   memory length: 488124   epsilon: 0.2315125000091558    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2684   score: 1.0   memory length: 488294   epsilon: 0.23117590000915367    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2685   score: 1.0   memory length: 488445   epsilon: 0.23087692000915178    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2686   score: 2.0   memory length: 488643   epsilon: 0.2304848800091493    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2687   score: 2.0   memory length: 488860   epsilon: 0.23005522000914658    steps: 217    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2688   score: 5.0   memory length: 489182   epsilon: 0.22941766000914254    steps: 322    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2689   score: 3.0   memory length: 489428   epsilon: 0.22893058000913946    steps: 246    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2690   score: 1.0   memory length: 489597   epsilon: 0.22859596000913734    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2691   score: 0.0   memory length: 489719   epsilon: 0.22835440000913582    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2692   score: 1.0   memory length: 489869   epsilon: 0.22805740000913394    steps: 150    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2693   score: 0.0   memory length: 489992   epsilon: 0.2278138600091324    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2694   score: 1.0   memory length: 490143   epsilon: 0.2275148800091305    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2695   score: 2.0   memory length: 490340   epsilon: 0.22712482000912804    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2696   score: 1.0   memory length: 490508   epsilon: 0.22679218000912593    steps: 168    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2697   score: 3.0   memory length: 490752   epsilon: 0.22630906000912288    steps: 244    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 2698   score: 2.0   memory length: 490969   epsilon: 0.22587940000912016    steps: 217    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 2699   score: 0.0   memory length: 491092   epsilon: 0.22563586000911862    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2700   score: 1.0   memory length: 491264   epsilon: 0.22529530000911646    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2701   score: 2.0   memory length: 491462   epsilon: 0.22490326000911398    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2702   score: 1.0   memory length: 491630   epsilon: 0.22457062000911188    steps: 168    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2703   score: 2.0   memory length: 491827   epsilon: 0.2241805600091094    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2704   score: 2.0   memory length: 492024   epsilon: 0.22379050000910694    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2705   score: 4.0   memory length: 492318   epsilon: 0.22320838000910326    steps: 294    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2706   score: 3.0   memory length: 492530   epsilon: 0.2227886200091006    steps: 212    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 2707   score: 1.0   memory length: 492699   epsilon: 0.22245400000909848    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 2708   score: 2.0   memory length: 492897   epsilon: 0.222061960009096    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 2709   score: 0.0   memory length: 493019   epsilon: 0.22182040000909448    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2710   score: 2.0   memory length: 493216   epsilon: 0.221430340009092    steps: 197    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2711   score: 2.0   memory length: 493414   epsilon: 0.22103830000908953    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2712   score: 2.0   memory length: 493632   epsilon: 0.2206066600090868    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2713   score: 1.0   memory length: 493783   epsilon: 0.2203076800090849    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2714   score: 1.0   memory length: 493934   epsilon: 0.220008700009083    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2715   score: 0.0   memory length: 494056   epsilon: 0.21976714000908149    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2716   score: 1.0   memory length: 494226   epsilon: 0.21943054000907936    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2717   score: 1.0   memory length: 494377   epsilon: 0.21913156000907746    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2718   score: 2.0   memory length: 494596   epsilon: 0.21869794000907472    steps: 219    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2719   score: 1.0   memory length: 494767   epsilon: 0.21835936000907258    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2720   score: 2.0   memory length: 494965   epsilon: 0.2179673200090701    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2721   score: 0.0   memory length: 495088   epsilon: 0.21772378000906856    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2722   score: 3.0   memory length: 495357   epsilon: 0.2171911600090652    steps: 269    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2723   score: 1.0   memory length: 495526   epsilon: 0.21685654000906307    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2724   score: 0.0   memory length: 495648   epsilon: 0.21661498000906154    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2725   score: 0.0   memory length: 495771   epsilon: 0.21637144000906    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2726   score: 2.0   memory length: 495969   epsilon: 0.21597940000905752    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2727   score: 2.0   memory length: 496190   epsilon: 0.21554182000905475    steps: 221    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 2728   score: 2.0   memory length: 496387   epsilon: 0.21515176000905228    steps: 197    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2729   score: 0.0   memory length: 496510   epsilon: 0.21490822000905074    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2730   score: 3.0   memory length: 496736   epsilon: 0.2144607400090479    steps: 226    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2731   score: 2.0   memory length: 496934   epsilon: 0.21406870000904543    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 2732   score: 3.0   memory length: 497160   epsilon: 0.2136212200090426    steps: 226    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 2733   score: 2.0   memory length: 497379   epsilon: 0.21318760000903986    steps: 219    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2734   score: 1.0   memory length: 497530   epsilon: 0.21288862000903797    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2735   score: 0.0   memory length: 497652   epsilon: 0.21264706000903644    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 2736   score: 1.0   memory length: 497822   epsilon: 0.2123104600090343    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 2737   score: 0.0   memory length: 497945   epsilon: 0.21206692000903277    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2738   score: 0.0   memory length: 498068   epsilon: 0.21182338000903123    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 2739   score: 0.0   memory length: 498191   epsilon: 0.21157984000902968    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2740   score: 2.0   memory length: 498409   epsilon: 0.21114820000902695    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 2741   score: 3.0   memory length: 498675   epsilon: 0.21062152000902362    steps: 266    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2742   score: 3.0   memory length: 498922   epsilon: 0.21013246000902053    steps: 247    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2743   score: 0.0   memory length: 499044   epsilon: 0.209890900009019    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2744   score: 1.0   memory length: 499216   epsilon: 0.20955034000901684    steps: 172    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 2745   score: 2.0   memory length: 499414   epsilon: 0.20915830000901436    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2746   score: 1.0   memory length: 499582   epsilon: 0.20882566000901226    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2747   score: 0.0   memory length: 499704   epsilon: 0.20858410000901073    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2748   score: 0.0   memory length: 499827   epsilon: 0.2083405600090092    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2749   score: 2.0   memory length: 500024   epsilon: 0.20795050000900672    steps: 197    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 2750   score: 0.0   memory length: 500146   epsilon: 0.2077089400090052    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2751   score: 2.0   memory length: 500363   epsilon: 0.20727928000900248    steps: 217    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2752   score: 1.0   memory length: 500534   epsilon: 0.20694070000900033    steps: 171    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2753   score: 0.0   memory length: 500657   epsilon: 0.2066971600089988    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2754   score: 0.0   memory length: 500780   epsilon: 0.20645362000899725    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2755   score: 2.0   memory length: 500977   epsilon: 0.20606356000899478    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2756   score: 1.0   memory length: 501149   epsilon: 0.20572300000899263    steps: 172    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2757   score: 2.0   memory length: 501347   epsilon: 0.20533096000899015    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2758   score: 2.0   memory length: 501545   epsilon: 0.20493892000898767    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2759   score: 0.0   memory length: 501667   epsilon: 0.20469736000898614    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2760   score: 3.0   memory length: 501892   epsilon: 0.20425186000898332    steps: 225    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2761   score: 0.0   memory length: 502015   epsilon: 0.20400832000898178    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2762   score: 1.0   memory length: 502183   epsilon: 0.20367568000897968    steps: 168    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2763   score: 1.0   memory length: 502354   epsilon: 0.20333710000897753    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2764   score: 3.0   memory length: 502621   epsilon: 0.2028084400089742    steps: 267    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2765   score: 0.0   memory length: 502743   epsilon: 0.20256688000897266    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2766   score: 2.0   memory length: 502962   epsilon: 0.20213326000896992    steps: 219    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2767   score: 0.0   memory length: 503085   epsilon: 0.20188972000896838    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2768   score: 0.0   memory length: 503207   epsilon: 0.20164816000896685    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2769   score: 4.0   memory length: 503482   epsilon: 0.2011036600089634    steps: 275    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2770   score: 5.0   memory length: 503776   epsilon: 0.20052154000895972    steps: 294    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2771   score: 2.0   memory length: 503961   epsilon: 0.2001552400089574    steps: 185    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2772   score: 2.0   memory length: 504177   epsilon: 0.1997275600089547    steps: 216    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2773   score: 1.0   memory length: 504346   epsilon: 0.19939294000895258    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2774   score: 2.0   memory length: 504566   epsilon: 0.19895734000894982    steps: 220    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2775   score: 3.0   memory length: 504797   epsilon: 0.19849996000894693    steps: 231    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2776   score: 0.0   memory length: 504919   epsilon: 0.1982584000089454    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2777   score: 0.0   memory length: 505042   epsilon: 0.19801486000894386    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2778   score: 1.0   memory length: 505211   epsilon: 0.19768024000894174    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2779   score: 1.0   memory length: 505380   epsilon: 0.19734562000893963    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2780   score: 0.0   memory length: 505503   epsilon: 0.19710208000893809    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2781   score: 3.0   memory length: 505728   epsilon: 0.19665658000893527    steps: 225    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2782   score: 2.0   memory length: 505926   epsilon: 0.1962645400089328    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2783   score: 3.0   memory length: 506177   epsilon: 0.19576756000892964    steps: 251    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2784   score: 0.0   memory length: 506300   epsilon: 0.1955240200089281    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2785   score: 0.0   memory length: 506422   epsilon: 0.19528246000892657    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2786   score: 0.0   memory length: 506544   epsilon: 0.19504090000892504    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2787   score: 3.0   memory length: 506808   epsilon: 0.19451818000892174    steps: 264    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2788   score: 0.0   memory length: 506930   epsilon: 0.1942766200089202    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 2789   score: 1.0   memory length: 507098   epsilon: 0.1939439800089181    steps: 168    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 2790   score: 2.0   memory length: 507296   epsilon: 0.19355194000891562    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 2791   score: 2.0   memory length: 507494   epsilon: 0.19315990000891314    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2792   score: 2.0   memory length: 507714   epsilon: 0.1927243000089104    steps: 220    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2793   score: 0.0   memory length: 507837   epsilon: 0.19248076000890885    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2794   score: 2.0   memory length: 508035   epsilon: 0.19208872000890637    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2795   score: 1.0   memory length: 508204   epsilon: 0.19175410000890425    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2796   score: 1.0   memory length: 508376   epsilon: 0.1914135400089021    steps: 172    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2797   score: 1.0   memory length: 508546   epsilon: 0.19107694000889996    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 2798   score: 2.0   memory length: 508728   epsilon: 0.19071658000889768    steps: 182    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 2799   score: 1.0   memory length: 508879   epsilon: 0.1904176000088958    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2800   score: 3.0   memory length: 509104   epsilon: 0.18997210000889297    steps: 225    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2801   score: 2.0   memory length: 509302   epsilon: 0.1895800600088905    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2802   score: 1.0   memory length: 509471   epsilon: 0.18924544000888838    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2803   score: 1.0   memory length: 509622   epsilon: 0.18894646000888649    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2804   score: 1.0   memory length: 509773   epsilon: 0.1886474800088846    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2805   score: 1.0   memory length: 509942   epsilon: 0.18831286000888248    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 2806   score: 1.0   memory length: 510111   epsilon: 0.18797824000888036    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 2807   score: 6.0   memory length: 510484   epsilon: 0.1872397000088757    steps: 373    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2808   score: 1.0   memory length: 510655   epsilon: 0.18690112000887354    steps: 171    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 2809   score: 1.0   memory length: 510806   epsilon: 0.18660214000887165    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2810   score: 0.0   memory length: 510928   epsilon: 0.18636058000887012    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 2811   score: 4.0   memory length: 511222   epsilon: 0.18577846000886644    steps: 294    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2812   score: 5.0   memory length: 511566   epsilon: 0.18509734000886213    steps: 344    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2813   score: 1.0   memory length: 511734   epsilon: 0.18476470000886003    steps: 168    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2814   score: 2.0   memory length: 511952   epsilon: 0.1843330600088573    steps: 218    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2815   score: 1.0   memory length: 512102   epsilon: 0.18403606000885542    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2816   score: 2.0   memory length: 512299   epsilon: 0.18364600000885295    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2817   score: 0.0   memory length: 512422   epsilon: 0.1834024600088514    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2818   score: 1.0   memory length: 512573   epsilon: 0.18310348000884952    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2819   score: 0.0   memory length: 512696   epsilon: 0.18285994000884798    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2820   score: 3.0   memory length: 512941   epsilon: 0.1823748400088449    steps: 245    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2821   score: 2.0   memory length: 513139   epsilon: 0.18198280000884243    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2822   score: 1.0   memory length: 513310   epsilon: 0.18164422000884028    steps: 171    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2823   score: 0.0   memory length: 513432   epsilon: 0.18140266000883876    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2824   score: 0.0   memory length: 513555   epsilon: 0.18115912000883722    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2825   score: 1.0   memory length: 513706   epsilon: 0.18086014000883532    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2826   score: 4.0   memory length: 513994   epsilon: 0.18028990000883172    steps: 288    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2827   score: 0.0   memory length: 514116   epsilon: 0.1800483400088302    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2828   score: 0.0   memory length: 514239   epsilon: 0.17980480000882865    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2829   score: 2.0   memory length: 514437   epsilon: 0.17941276000882617    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2830   score: 1.0   memory length: 514606   epsilon: 0.17907814000882405    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2831   score: 0.0   memory length: 514729   epsilon: 0.1788346000088225    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2832   score: 3.0   memory length: 514954   epsilon: 0.1783891000088197    steps: 225    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 2833   score: 3.0   memory length: 515180   epsilon: 0.17794162000881686    steps: 226    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 2834   score: 2.0   memory length: 515377   epsilon: 0.1775515600088144    steps: 197    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2835   score: 1.0   memory length: 515528   epsilon: 0.1772525800088125    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2836   score: 0.0   memory length: 515651   epsilon: 0.17700904000881096    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 2837   score: 2.0   memory length: 515849   epsilon: 0.17661700000880848    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2838   score: 1.0   memory length: 515999   epsilon: 0.1763200000088066    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 2839   score: 3.0   memory length: 516247   epsilon: 0.1758289600088035    steps: 248    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 2840   score: 1.0   memory length: 516416   epsilon: 0.17549434000880137    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 2841   score: 2.0   memory length: 516614   epsilon: 0.1751023000087989    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2842   score: 1.0   memory length: 516765   epsilon: 0.174803320008797    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2843   score: 0.0   memory length: 516888   epsilon: 0.17455978000879546    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2844   score: 1.0   memory length: 517039   epsilon: 0.17426080000879357    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 2845   score: 1.0   memory length: 517191   epsilon: 0.17395984000879167    steps: 152    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2846   score: 1.0   memory length: 517360   epsilon: 0.17362522000878955    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 2847   score: 3.0   memory length: 517588   epsilon: 0.1731737800087867    steps: 228    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 2848   score: 5.0   memory length: 517913   epsilon: 0.17253028000878262    steps: 325    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2849   score: 1.0   memory length: 518084   epsilon: 0.17219170000878048    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 2850   score: 1.0   memory length: 518235   epsilon: 0.1718927200087786    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2851   score: 2.0   memory length: 518433   epsilon: 0.1715006800087761    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2852   score: 1.0   memory length: 518601   epsilon: 0.171168040008774    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 2853   score: 5.0   memory length: 518948   epsilon: 0.17048098000876966    steps: 347    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2854   score: 1.0   memory length: 519099   epsilon: 0.17018200000876776    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2855   score: 2.0   memory length: 519297   epsilon: 0.16978996000876528    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 2856   score: 0.0   memory length: 519419   epsilon: 0.16954840000876376    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 2857   score: 0.0   memory length: 519542   epsilon: 0.16930486000876221    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 2858   score: 0.0   memory length: 519664   epsilon: 0.1690633000087607    steps: 122    lr: 0.0001     evaluation reward: 1.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e8WsQh42OsT"
      },
      "source": [
        "# Visualize Agent Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20vLFGcp2OsT"
      },
      "source": [
        "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
        "\n",
        "Please save your model before running this portion of the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXQttKal2OsT"
      },
      "source": [
        "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ibe_QZ8c3J_c",
        "outputId": "993a0c93-5c78-4498-b173-f0cc91932e73"
      },
      "source": [
        "pip install pyvirtualdisplay"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/8a/643043cc70791367bee2d19eb20e00ed1a246ac48e5dbe57bbbcc8be40a9/PyVirtualDisplay-1.3.2-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-1.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZqvZeoJ2OsU"
      },
      "source": [
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Displaying the game live\n",
        "def show_state(env, step=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
        "    plt.axis('off')\n",
        "\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "    \n",
        "# Recording the game and replaying the game afterwards\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoMFyPg82OsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef599d99-9952-4838-eb29-80ba797087b7"
      },
      "source": [
        "!sudo apt-get install -y xvfb python-opengl ffmpeg"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl xvfb\n",
            "0 upgraded, 2 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 1,280 kB of archives.\n",
            "After this operation, 7,686 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 1,280 kB in 1s (940 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "id": "uThODipO2OsU",
        "outputId": "db271949-d8d9-48f1-d0af-3e440b30e55c"
      },
      "source": [
        "display = Display(visible=0, size=(300, 200))\n",
        "display.start()\n",
        "\n",
        "# Load agent\n",
        "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
        "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
        "\n",
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "env = wrap_env(env)\n",
        "\n",
        "done = False\n",
        "score = 0\n",
        "step = 0\n",
        "state = env.reset()\n",
        "next_state = state\n",
        "life = number_lives\n",
        "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "get_init_state(history, state)\n",
        "\n",
        "while not done:\n",
        "    \n",
        "    # Render breakout\n",
        "    env.render()\n",
        "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
        "\n",
        "    step += 1\n",
        "    frame += 1\n",
        "\n",
        "    # Perform a fire action if ball is no longer on screen\n",
        "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "        action = 0\n",
        "    else:\n",
        "        action = torch.from_numpy(agent.get_action(torch.from_numpy(np.float32(history[:4, :, :]) / 255.)))\n",
        "    state = next_state\n",
        "    \n",
        "    next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "    frame_next_state = get_frame(next_state)\n",
        "    history[4, :, :] = frame_next_state\n",
        "    terminal_state = check_live(life, info['ale.lives'])\n",
        "        \n",
        "    life = info['ale.lives']\n",
        "    r = np.clip(reward, -1, 1) \n",
        "    r = reward\n",
        "\n",
        "    # Store the transition in memory \n",
        "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "    # Start training after random sample generation\n",
        "    score += reward\n",
        "    \n",
        "    history[:4, :, :] = history[1:, :, :]\n",
        "env.close()\n",
        "show_video()\n",
        "display.stop()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e027c8ab025b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/agent.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Choose the best action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR1t810C60Ye"
      },
      "source": [
        "double_dqn = True # set to True if using double DQN agent\n",
        "\n",
        "if double_dqn:\n",
        "    from agent_double import Agent\n",
        "else:\n",
        "    from agent import Agent\n",
        "\n",
        "agent = Agent(action_size)\n",
        "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
        "frame = 0\n",
        "memory_size = 0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7415Fkuj62je",
        "outputId": "48595952-c820-4fca-c9ee-686c490485f2"
      },
      "source": [
        "rewards, episodes = [], []\n",
        "best_eval_reward = 0\n",
        "for e in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "    step = 0\n",
        "    d = False\n",
        "    state = env.reset()\n",
        "    next_state = state\n",
        "    life = number_lives\n",
        "\n",
        "    get_init_state(history, state)\n",
        "\n",
        "    while not done:\n",
        "        step += 1\n",
        "        frame += 1\n",
        "\n",
        "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
        "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "            action = 0\n",
        "        else:\n",
        "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "        state = next_state\n",
        "        next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "        frame_next_state = get_frame(next_state)\n",
        "        history[4, :, :] = frame_next_state\n",
        "        terminal_state = check_live(life, info['ale.lives'])\n",
        "\n",
        "        life = info['ale.lives']\n",
        "        r = np.clip(reward, -1, 1) \n",
        "        r = reward\n",
        "\n",
        "        # Store the transition in memory \n",
        "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "        # Start training after random sample generation\n",
        "        if(frame >= train_frame):\n",
        "            agent.train_policy_net(frame)\n",
        "            # Update the target network only for Double DQN only\n",
        "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
        "                agent.update_target_net()\n",
        "        score += reward\n",
        "        history[:4, :, :] = history[1:, :, :]\n",
        "            \n",
        "        if done:\n",
        "            evaluation_reward.append(score)\n",
        "            rewards.append(np.mean(evaluation_reward))\n",
        "            episodes.append(e)\n",
        "            pylab.plot(episodes, rewards, 'b')\n",
        "            pylab.xlabel('Episodes')\n",
        "            pylab.ylabel('Rewards') \n",
        "            pylab.title('Episodes vs Reward')\n",
        "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
        "            \n",
        "            # every episode, plot the play time\n",
        "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
        "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
        "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
        "\n",
        "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
        "            ### Change this save condition to whatever you prefer ###\n",
        "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
        "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
        "                best_eval_reward = np.mean(evaluation_reward)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0   score: 4.0   memory length: 296   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 4.0\n",
            "episode: 1   score: 2.0   memory length: 494   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 3.0\n",
            "episode: 2   score: 2.0   memory length: 692   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 2.6666666666666665\n",
            "episode: 3   score: 0.0   memory length: 815   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 4   score: 3.0   memory length: 1084   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 2.2\n",
            "episode: 5   score: 2.0   memory length: 1300   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 2.1666666666666665\n",
            "episode: 6   score: 2.0   memory length: 1518   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 2.142857142857143\n",
            "episode: 7   score: 1.0   memory length: 1689   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 8   score: 3.0   memory length: 1915   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 2.111111111111111\n",
            "episode: 9   score: 0.0   memory length: 2038   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 10   score: 1.0   memory length: 2189   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.8181818181818181\n",
            "episode: 11   score: 2.0   memory length: 2368   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.8333333333333333\n",
            "episode: 12   score: 2.0   memory length: 2586   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.8461538461538463\n",
            "episode: 13   score: 2.0   memory length: 2784   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.8571428571428572\n",
            "episode: 14   score: 4.0   memory length: 3062   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 15   score: 0.0   memory length: 3185   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.875\n",
            "episode: 16   score: 3.0   memory length: 3416   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.9411764705882353\n",
            "episode: 17   score: 4.0   memory length: 3712   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 2.0555555555555554\n",
            "episode: 18   score: 2.0   memory length: 3910   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 2.0526315789473686\n",
            "episode: 19   score: 1.0   memory length: 4078   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 20   score: 0.0   memory length: 4200   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.9047619047619047\n",
            "episode: 21   score: 1.0   memory length: 4369   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.8636363636363635\n",
            "episode: 22   score: 1.0   memory length: 4538   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.826086956521739\n",
            "episode: 23   score: 1.0   memory length: 4709   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.7916666666666667\n",
            "episode: 24   score: 3.0   memory length: 4919   epsilon: 1.0    steps: 210    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 25   score: 1.0   memory length: 5087   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.8076923076923077\n",
            "episode: 26   score: 4.0   memory length: 5350   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.8888888888888888\n",
            "episode: 27   score: 2.0   memory length: 5568   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.8928571428571428\n",
            "episode: 28   score: 0.0   memory length: 5690   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.8275862068965518\n",
            "episode: 29   score: 2.0   memory length: 5887   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.8333333333333333\n",
            "episode: 30   score: 0.0   memory length: 6010   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7741935483870968\n",
            "episode: 31   score: 1.0   memory length: 6161   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 32   score: 2.0   memory length: 6340   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.7575757575757576\n",
            "episode: 33   score: 1.0   memory length: 6509   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.7352941176470589\n",
            "episode: 34   score: 1.0   memory length: 6680   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.7142857142857142\n",
            "episode: 35   score: 1.0   memory length: 6831   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6944444444444444\n",
            "episode: 36   score: 0.0   memory length: 6953   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6486486486486487\n",
            "episode: 37   score: 0.0   memory length: 7076   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.605263157894737\n",
            "episode: 38   score: 1.0   memory length: 7245   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5897435897435896\n",
            "episode: 39   score: 0.0   memory length: 7368   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 40   score: 0.0   memory length: 7490   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5121951219512195\n",
            "episode: 41   score: 1.0   memory length: 7658   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 42   score: 5.0   memory length: 7983   epsilon: 1.0    steps: 325    lr: 0.0001     evaluation reward: 1.5813953488372092\n",
            "episode: 43   score: 1.0   memory length: 8152   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5681818181818181\n",
            "episode: 44   score: 2.0   memory length: 8350   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5777777777777777\n",
            "episode: 45   score: 2.0   memory length: 8529   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.5869565217391304\n",
            "episode: 46   score: 1.0   memory length: 8679   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.574468085106383\n",
            "episode: 47   score: 3.0   memory length: 8929   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.6041666666666667\n",
            "episode: 48   score: 3.0   memory length: 9176   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.6326530612244898\n",
            "episode: 49   score: 2.0   memory length: 9374   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 50   score: 4.0   memory length: 9649   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.6862745098039216\n",
            "episode: 51   score: 2.0   memory length: 9867   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6923076923076923\n",
            "episode: 52   score: 2.0   memory length: 10065   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6981132075471699\n",
            "episode: 53   score: 3.0   memory length: 10296   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.7222222222222223\n",
            "episode: 54   score: 0.0   memory length: 10419   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.690909090909091\n",
            "episode: 55   score: 2.0   memory length: 10619   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.6964285714285714\n",
            "episode: 56   score: 0.0   memory length: 10742   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
            "episode: 57   score: 2.0   memory length: 10960   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6724137931034482\n",
            "episode: 58   score: 1.0   memory length: 11129   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6610169491525424\n",
            "episode: 59   score: 0.0   memory length: 11251   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6333333333333333\n",
            "episode: 60   score: 1.0   memory length: 11422   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6229508196721312\n",
            "episode: 61   score: 4.0   memory length: 11718   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.6612903225806452\n",
            "episode: 62   score: 0.0   memory length: 11840   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6349206349206349\n",
            "episode: 63   score: 3.0   memory length: 12086   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.65625\n",
            "episode: 64   score: 1.0   memory length: 12255   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.646153846153846\n",
            "episode: 65   score: 1.0   memory length: 12423   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
            "episode: 66   score: 1.0   memory length: 12573   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.626865671641791\n",
            "episode: 67   score: 1.0   memory length: 12743   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6176470588235294\n",
            "episode: 68   score: 3.0   memory length: 12986   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.6376811594202898\n",
            "episode: 69   score: 2.0   memory length: 13207   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.6428571428571428\n",
            "episode: 70   score: 1.0   memory length: 13376   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6338028169014085\n",
            "episode: 71   score: 0.0   memory length: 13499   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6111111111111112\n",
            "episode: 72   score: 3.0   memory length: 13744   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.63013698630137\n",
            "episode: 73   score: 0.0   memory length: 13867   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6081081081081081\n",
            "episode: 74   score: 3.0   memory length: 14133   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.6266666666666667\n",
            "episode: 75   score: 0.0   memory length: 14255   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.605263157894737\n",
            "episode: 76   score: 2.0   memory length: 14453   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6103896103896105\n",
            "episode: 77   score: 2.0   memory length: 14671   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6153846153846154\n",
            "episode: 78   score: 2.0   memory length: 14868   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.620253164556962\n",
            "episode: 79   score: 1.0   memory length: 15039   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6125\n",
            "episode: 80   score: 1.0   memory length: 15210   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6049382716049383\n",
            "episode: 81   score: 0.0   memory length: 15333   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5853658536585367\n",
            "episode: 82   score: 5.0   memory length: 15610   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.6265060240963856\n",
            "episode: 83   score: 0.0   memory length: 15732   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6071428571428572\n",
            "episode: 84   score: 0.0   memory length: 15855   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.588235294117647\n",
            "episode: 85   score: 2.0   memory length: 16078   epsilon: 1.0    steps: 223    lr: 0.0001     evaluation reward: 1.5930232558139534\n",
            "episode: 86   score: 2.0   memory length: 16258   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.5977011494252873\n",
            "episode: 87   score: 1.0   memory length: 16409   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5909090909090908\n",
            "episode: 88   score: 1.0   memory length: 16578   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5842696629213484\n",
            "episode: 89   score: 2.0   memory length: 16775   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5888888888888888\n",
            "episode: 90   score: 0.0   memory length: 16898   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5714285714285714\n",
            "episode: 91   score: 2.0   memory length: 17114   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.576086956521739\n",
            "episode: 92   score: 2.0   memory length: 17331   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.5806451612903225\n",
            "episode: 93   score: 0.0   memory length: 17454   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5638297872340425\n",
            "episode: 94   score: 1.0   memory length: 17623   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5578947368421052\n",
            "episode: 95   score: 1.0   memory length: 17792   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5520833333333333\n",
            "episode: 96   score: 3.0   memory length: 18018   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.5670103092783505\n",
            "episode: 97   score: 1.0   memory length: 18186   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.5612244897959184\n",
            "episode: 98   score: 0.0   memory length: 18309   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5454545454545454\n",
            "episode: 99   score: 0.0   memory length: 18432   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 100   score: 0.0   memory length: 18555   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 101   score: 1.0   memory length: 18726   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 102   score: 2.0   memory length: 18945   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 103   score: 2.0   memory length: 19143   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 104   score: 2.0   memory length: 19343   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 105   score: 1.0   memory length: 19512   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 106   score: 1.0   memory length: 19682   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 107   score: 2.0   memory length: 19880   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 108   score: 3.0   memory length: 20126   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 109   score: 1.0   memory length: 20277   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 110   score: 3.0   memory length: 20522   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 111   score: 2.0   memory length: 20740   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 112   score: 0.0   memory length: 20863   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 113   score: 2.0   memory length: 21079   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 114   score: 1.0   memory length: 21248   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 115   score: 0.0   memory length: 21371   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 116   score: 0.0   memory length: 21494   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 117   score: 1.0   memory length: 21645   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 118   score: 3.0   memory length: 21891   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 119   score: 2.0   memory length: 22089   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 120   score: 4.0   memory length: 22390   epsilon: 1.0    steps: 301    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 121   score: 2.0   memory length: 22588   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 122   score: 2.0   memory length: 22786   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 123   score: 2.0   memory length: 22984   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 124   score: 1.0   memory length: 23135   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 125   score: 2.0   memory length: 23332   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 126   score: 0.0   memory length: 23455   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 127   score: 3.0   memory length: 23687   epsilon: 1.0    steps: 232    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 128   score: 1.0   memory length: 23856   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 129   score: 2.0   memory length: 24074   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 130   score: 2.0   memory length: 24272   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 131   score: 3.0   memory length: 24540   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 132   score: 5.0   memory length: 24848   epsilon: 1.0    steps: 308    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 133   score: 0.0   memory length: 24971   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 134   score: 2.0   memory length: 25169   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 135   score: 0.0   memory length: 25292   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 136   score: 1.0   memory length: 25443   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 137   score: 1.0   memory length: 25614   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 138   score: 1.0   memory length: 25785   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 139   score: 3.0   memory length: 26037   epsilon: 1.0    steps: 252    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 140   score: 1.0   memory length: 26187   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 141   score: 2.0   memory length: 26409   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 142   score: 1.0   memory length: 26580   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 143   score: 2.0   memory length: 26797   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 144   score: 0.0   memory length: 26919   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 145   score: 1.0   memory length: 27089   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 146   score: 1.0   memory length: 27258   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 147   score: 0.0   memory length: 27381   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 148   score: 0.0   memory length: 27504   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 149   score: 0.0   memory length: 27627   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 150   score: 0.0   memory length: 27750   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 151   score: 1.0   memory length: 27919   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 152   score: 2.0   memory length: 28117   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 153   score: 0.0   memory length: 28239   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 154   score: 1.0   memory length: 28390   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 155   score: 0.0   memory length: 28513   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 156   score: 1.0   memory length: 28665   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 157   score: 2.0   memory length: 28866   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 158   score: 0.0   memory length: 28988   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 159   score: 0.0   memory length: 29110   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 160   score: 1.0   memory length: 29280   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 161   score: 1.0   memory length: 29430   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 162   score: 1.0   memory length: 29581   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 163   score: 2.0   memory length: 29799   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 164   score: 2.0   memory length: 30017   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 165   score: 2.0   memory length: 30235   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 166   score: 2.0   memory length: 30455   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 167   score: 0.0   memory length: 30577   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 168   score: 2.0   memory length: 30774   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 169   score: 2.0   memory length: 30972   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 170   score: 0.0   memory length: 31095   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 171   score: 0.0   memory length: 31217   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 172   score: 2.0   memory length: 31415   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 173   score: 1.0   memory length: 31587   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 174   score: 1.0   memory length: 31757   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 175   score: 2.0   memory length: 31955   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 176   score: 2.0   memory length: 32152   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 177   score: 3.0   memory length: 32395   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 178   score: 4.0   memory length: 32669   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 179   score: 3.0   memory length: 32914   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 180   score: 2.0   memory length: 33130   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 181   score: 2.0   memory length: 33328   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 182   score: 2.0   memory length: 33526   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 183   score: 3.0   memory length: 33773   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 184   score: 1.0   memory length: 33924   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 185   score: 1.0   memory length: 34095   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 186   score: 0.0   memory length: 34217   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 187   score: 2.0   memory length: 34435   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 188   score: 2.0   memory length: 34637   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 189   score: 2.0   memory length: 34855   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 190   score: 0.0   memory length: 34978   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 191   score: 4.0   memory length: 35255   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 192   score: 1.0   memory length: 35406   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 193   score: 0.0   memory length: 35529   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 194   score: 0.0   memory length: 35652   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 195   score: 1.0   memory length: 35821   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 196   score: 3.0   memory length: 36048   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 197   score: 2.0   memory length: 36264   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 198   score: 1.0   memory length: 36433   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 199   score: 0.0   memory length: 36556   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 200   score: 2.0   memory length: 36756   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 201   score: 2.0   memory length: 36957   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 202   score: 2.0   memory length: 37139   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 203   score: 2.0   memory length: 37357   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 204   score: 1.0   memory length: 37525   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 205   score: 1.0   memory length: 37693   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 206   score: 1.0   memory length: 37862   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 207   score: 0.0   memory length: 37985   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 208   score: 3.0   memory length: 38228   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 209   score: 2.0   memory length: 38448   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 210   score: 0.0   memory length: 38571   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 211   score: 3.0   memory length: 38799   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 212   score: 3.0   memory length: 39046   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 213   score: 2.0   memory length: 39268   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 214   score: 0.0   memory length: 39390   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 215   score: 2.0   memory length: 39610   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 216   score: 2.0   memory length: 39829   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 217   score: 2.0   memory length: 40030   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 218   score: 1.0   memory length: 40200   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 219   score: 2.0   memory length: 40398   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 220   score: 3.0   memory length: 40644   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 221   score: 2.0   memory length: 40862   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 222   score: 5.0   memory length: 41170   epsilon: 1.0    steps: 308    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 223   score: 1.0   memory length: 41339   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 224   score: 0.0   memory length: 41461   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 225   score: 4.0   memory length: 41735   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 226   score: 3.0   memory length: 41980   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 227   score: 7.0   memory length: 42373   epsilon: 1.0    steps: 393    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 228   score: 0.0   memory length: 42495   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 229   score: 0.0   memory length: 42618   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 230   score: 1.0   memory length: 42771   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 231   score: 3.0   memory length: 43023   epsilon: 1.0    steps: 252    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 232   score: 1.0   memory length: 43192   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 233   score: 1.0   memory length: 43361   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 234   score: 1.0   memory length: 43512   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 235   score: 4.0   memory length: 43788   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 236   score: 1.0   memory length: 43938   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 237   score: 1.0   memory length: 44089   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 238   score: 1.0   memory length: 44261   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 239   score: 0.0   memory length: 44383   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 240   score: 2.0   memory length: 44566   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 241   score: 0.0   memory length: 44689   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 242   score: 0.0   memory length: 44812   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 243   score: 0.0   memory length: 44935   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 244   score: 3.0   memory length: 45183   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 245   score: 0.0   memory length: 45305   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 246   score: 2.0   memory length: 45503   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 247   score: 2.0   memory length: 45719   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 248   score: 3.0   memory length: 45983   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 249   score: 1.0   memory length: 46155   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 250   score: 2.0   memory length: 46353   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 251   score: 2.0   memory length: 46551   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 252   score: 0.0   memory length: 46674   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 253   score: 3.0   memory length: 46941   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 254   score: 0.0   memory length: 47064   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 255   score: 0.0   memory length: 47186   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 256   score: 0.0   memory length: 47308   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 257   score: 1.0   memory length: 47459   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 258   score: 0.0   memory length: 47582   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 259   score: 0.0   memory length: 47704   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 260   score: 1.0   memory length: 47854   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 261   score: 2.0   memory length: 48071   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 262   score: 3.0   memory length: 48337   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 263   score: 3.0   memory length: 48602   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 264   score: 1.0   memory length: 48753   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 265   score: 2.0   memory length: 48951   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 266   score: 4.0   memory length: 49209   epsilon: 1.0    steps: 258    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 267   score: 0.0   memory length: 49331   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 268   score: 0.0   memory length: 49454   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 269   score: 0.0   memory length: 49576   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 270   score: 6.0   memory length: 49969   epsilon: 1.0    steps: 393    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 271   score: 0.0   memory length: 50092   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 272   score: 4.0   memory length: 50370   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 273   score: 1.0   memory length: 50520   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 274   score: 2.0   memory length: 50738   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 275   score: 0.0   memory length: 50861   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 276   score: 5.0   memory length: 51148   epsilon: 1.0    steps: 287    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 277   score: 2.0   memory length: 51368   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 278   score: 3.0   memory length: 51614   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 279   score: 4.0   memory length: 51911   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 280   score: 3.0   memory length: 52155   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 281   score: 2.0   memory length: 52352   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 282   score: 1.0   memory length: 52523   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 283   score: 1.0   memory length: 52674   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 284   score: 0.0   memory length: 52797   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 285   score: 0.0   memory length: 52919   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 286   score: 3.0   memory length: 53183   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 287   score: 2.0   memory length: 53401   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 288   score: 2.0   memory length: 53616   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 289   score: 2.0   memory length: 53834   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 290   score: 0.0   memory length: 53956   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 291   score: 3.0   memory length: 54182   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 292   score: 2.0   memory length: 54380   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 293   score: 1.0   memory length: 54548   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 294   score: 0.0   memory length: 54670   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 295   score: 0.0   memory length: 54793   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 296   score: 2.0   memory length: 54990   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 297   score: 2.0   memory length: 55208   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 298   score: 1.0   memory length: 55358   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 299   score: 0.0   memory length: 55480   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 300   score: 0.0   memory length: 55603   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 301   score: 2.0   memory length: 55801   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 302   score: 4.0   memory length: 56097   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 303   score: 0.0   memory length: 56219   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 304   score: 0.0   memory length: 56342   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 305   score: 4.0   memory length: 56633   epsilon: 1.0    steps: 291    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 306   score: 2.0   memory length: 56851   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 307   score: 3.0   memory length: 57103   epsilon: 1.0    steps: 252    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 308   score: 1.0   memory length: 57254   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 309   score: 0.0   memory length: 57377   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 310   score: 1.0   memory length: 57546   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 311   score: 3.0   memory length: 57790   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 312   score: 0.0   memory length: 57912   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 313   score: 0.0   memory length: 58034   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 314   score: 3.0   memory length: 58268   epsilon: 1.0    steps: 234    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 315   score: 2.0   memory length: 58486   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 316   score: 0.0   memory length: 58609   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 317   score: 1.0   memory length: 58781   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 318   score: 0.0   memory length: 58904   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 319   score: 4.0   memory length: 59219   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 320   score: 0.0   memory length: 59341   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 321   score: 2.0   memory length: 59559   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 322   score: 2.0   memory length: 59777   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 323   score: 0.0   memory length: 59899   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 324   score: 3.0   memory length: 60145   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 325   score: 1.0   memory length: 60296   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 326   score: 0.0   memory length: 60419   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 327   score: 1.0   memory length: 60588   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 328   score: 1.0   memory length: 60757   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 329   score: 1.0   memory length: 60908   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 330   score: 1.0   memory length: 61078   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 331   score: 2.0   memory length: 61260   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 332   score: 2.0   memory length: 61475   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 333   score: 1.0   memory length: 61625   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 334   score: 3.0   memory length: 61874   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 335   score: 3.0   memory length: 62123   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 336   score: 5.0   memory length: 62464   epsilon: 1.0    steps: 341    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 337   score: 2.0   memory length: 62679   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 338   score: 0.0   memory length: 62802   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 339   score: 1.0   memory length: 62972   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 340   score: 1.0   memory length: 63123   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 341   score: 1.0   memory length: 63274   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 342   score: 0.0   memory length: 63397   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 343   score: 0.0   memory length: 63520   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 344   score: 1.0   memory length: 63690   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 345   score: 2.0   memory length: 63888   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 346   score: 1.0   memory length: 64057   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 347   score: 0.0   memory length: 64180   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 348   score: 3.0   memory length: 64407   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 349   score: 0.0   memory length: 64529   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 350   score: 1.0   memory length: 64699   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 351   score: 2.0   memory length: 64897   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 352   score: 3.0   memory length: 65143   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 353   score: 2.0   memory length: 65360   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 354   score: 3.0   memory length: 65607   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 355   score: 1.0   memory length: 65777   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 356   score: 0.0   memory length: 65900   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 357   score: 2.0   memory length: 66097   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 358   score: 5.0   memory length: 66404   epsilon: 1.0    steps: 307    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 359   score: 4.0   memory length: 66676   epsilon: 1.0    steps: 272    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 360   score: 2.0   memory length: 66891   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 361   score: 0.0   memory length: 67013   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 362   score: 2.0   memory length: 67230   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 363   score: 2.0   memory length: 67447   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 364   score: 1.0   memory length: 67616   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 365   score: 2.0   memory length: 67814   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 366   score: 1.0   memory length: 67966   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 367   score: 0.0   memory length: 68088   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 368   score: 1.0   memory length: 68259   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 369   score: 0.0   memory length: 68382   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 370   score: 1.0   memory length: 68552   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 371   score: 2.0   memory length: 68750   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 372   score: 1.0   memory length: 68901   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 373   score: 0.0   memory length: 69024   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 374   score: 0.0   memory length: 69146   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 375   score: 6.0   memory length: 69522   epsilon: 1.0    steps: 376    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 376   score: 1.0   memory length: 69691   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 377   score: 3.0   memory length: 69940   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 378   score: 0.0   memory length: 70062   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 379   score: 0.0   memory length: 70185   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 380   score: 0.0   memory length: 70307   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 381   score: 4.0   memory length: 70583   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 382   score: 0.0   memory length: 70706   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 383   score: 0.0   memory length: 70829   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 384   score: 1.0   memory length: 70998   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 385   score: 1.0   memory length: 71166   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 386   score: 1.0   memory length: 71336   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 387   score: 1.0   memory length: 71507   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 388   score: 0.0   memory length: 71629   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 389   score: 0.0   memory length: 71751   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 390   score: 0.0   memory length: 71873   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 391   score: 2.0   memory length: 72089   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 392   score: 2.0   memory length: 72307   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 393   score: 2.0   memory length: 72525   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 394   score: 2.0   memory length: 72747   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 395   score: 1.0   memory length: 72917   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 396   score: 0.0   memory length: 73040   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 397   score: 1.0   memory length: 73209   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 398   score: 6.0   memory length: 73588   epsilon: 1.0    steps: 379    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 399   score: 2.0   memory length: 73807   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 400   score: 0.0   memory length: 73930   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 401   score: 4.0   memory length: 74227   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 402   score: 0.0   memory length: 74349   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 403   score: 3.0   memory length: 74596   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 404   score: 1.0   memory length: 74746   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 405   score: 1.0   memory length: 74915   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 406   score: 0.0   memory length: 75038   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 407   score: 4.0   memory length: 75334   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 408   score: 1.0   memory length: 75503   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 409   score: 2.0   memory length: 75722   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 410   score: 3.0   memory length: 75950   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 411   score: 1.0   memory length: 76101   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 412   score: 3.0   memory length: 76328   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 413   score: 2.0   memory length: 76526   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 414   score: 1.0   memory length: 76695   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 415   score: 4.0   memory length: 76955   epsilon: 1.0    steps: 260    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 416   score: 2.0   memory length: 77153   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 417   score: 2.0   memory length: 77369   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 418   score: 1.0   memory length: 77539   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 419   score: 2.0   memory length: 77737   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 420   score: 2.0   memory length: 77919   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 421   score: 2.0   memory length: 78116   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 422   score: 0.0   memory length: 78238   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 423   score: 0.0   memory length: 78361   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 424   score: 1.0   memory length: 78512   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 425   score: 0.0   memory length: 78635   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 426   score: 0.0   memory length: 78758   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 427   score: 0.0   memory length: 78881   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 428   score: 1.0   memory length: 79051   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 429   score: 1.0   memory length: 79203   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 430   score: 2.0   memory length: 79401   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 431   score: 3.0   memory length: 79626   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 432   score: 0.0   memory length: 79749   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 433   score: 3.0   memory length: 79977   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 434   score: 3.0   memory length: 80221   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 435   score: 0.0   memory length: 80344   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 436   score: 2.0   memory length: 80542   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 437   score: 1.0   memory length: 80713   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 438   score: 2.0   memory length: 80929   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 439   score: 1.0   memory length: 81100   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 440   score: 1.0   memory length: 81268   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 441   score: 0.0   memory length: 81391   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 442   score: 1.0   memory length: 81560   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 443   score: 2.0   memory length: 81758   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 444   score: 1.0   memory length: 81927   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 445   score: 0.0   memory length: 82049   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 446   score: 1.0   memory length: 82219   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 447   score: 2.0   memory length: 82420   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 448   score: 2.0   memory length: 82620   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 449   score: 0.0   memory length: 82742   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 450   score: 0.0   memory length: 82865   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 451   score: 2.0   memory length: 83084   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 452   score: 2.0   memory length: 83284   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 453   score: 0.0   memory length: 83407   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 454   score: 3.0   memory length: 83655   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 455   score: 0.0   memory length: 83778   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 456   score: 1.0   memory length: 83929   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 457   score: 2.0   memory length: 84126   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 458   score: 0.0   memory length: 84249   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 459   score: 0.0   memory length: 84371   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 460   score: 0.0   memory length: 84494   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 461   score: 1.0   memory length: 84662   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 462   score: 4.0   memory length: 84955   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 463   score: 1.0   memory length: 85123   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 464   score: 1.0   memory length: 85291   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 465   score: 0.0   memory length: 85414   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 466   score: 0.0   memory length: 85536   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 467   score: 3.0   memory length: 85765   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 468   score: 1.0   memory length: 85933   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 469   score: 1.0   memory length: 86083   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 470   score: 1.0   memory length: 86234   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 471   score: 0.0   memory length: 86357   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 472   score: 4.0   memory length: 86653   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 473   score: 1.0   memory length: 86824   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 474   score: 2.0   memory length: 87023   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 475   score: 0.0   memory length: 87146   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 476   score: 3.0   memory length: 87393   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 477   score: 2.0   memory length: 87577   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 478   score: 3.0   memory length: 87822   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 479   score: 0.0   memory length: 87945   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 480   score: 2.0   memory length: 88143   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 481   score: 2.0   memory length: 88342   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 482   score: 2.0   memory length: 88540   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 483   score: 4.0   memory length: 88832   epsilon: 1.0    steps: 292    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 484   score: 1.0   memory length: 88982   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 485   score: 4.0   memory length: 89238   epsilon: 1.0    steps: 256    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 486   score: 3.0   memory length: 89484   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 487   score: 4.0   memory length: 89800   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 488   score: 0.0   memory length: 89923   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 489   score: 1.0   memory length: 90092   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 490   score: 2.0   memory length: 90290   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 491   score: 0.0   memory length: 90412   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 492   score: 1.0   memory length: 90581   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 493   score: 0.0   memory length: 90704   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 494   score: 3.0   memory length: 90947   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 495   score: 0.0   memory length: 91070   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 496   score: 4.0   memory length: 91332   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 497   score: 0.0   memory length: 91455   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 498   score: 0.0   memory length: 91577   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 499   score: 2.0   memory length: 91760   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 500   score: 0.0   memory length: 91883   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 501   score: 2.0   memory length: 92081   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 502   score: 2.0   memory length: 92300   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 503   score: 3.0   memory length: 92526   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 504   score: 0.0   memory length: 92649   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 505   score: 2.0   memory length: 92846   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 506   score: 2.0   memory length: 93043   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 507   score: 2.0   memory length: 93241   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 508   score: 1.0   memory length: 93391   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 509   score: 1.0   memory length: 93542   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 510   score: 2.0   memory length: 93759   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 511   score: 6.0   memory length: 94108   epsilon: 1.0    steps: 349    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 512   score: 1.0   memory length: 94276   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 513   score: 2.0   memory length: 94474   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 514   score: 1.0   memory length: 94644   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 515   score: 3.0   memory length: 94891   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 516   score: 1.0   memory length: 95042   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 517   score: 0.0   memory length: 95165   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 518   score: 3.0   memory length: 95412   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 519   score: 3.0   memory length: 95677   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 520   score: 2.0   memory length: 95876   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 521   score: 0.0   memory length: 95999   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 522   score: 2.0   memory length: 96217   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 523   score: 1.0   memory length: 96389   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 524   score: 5.0   memory length: 96698   epsilon: 1.0    steps: 309    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 525   score: 2.0   memory length: 96896   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 526   score: 3.0   memory length: 97160   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 527   score: 0.0   memory length: 97283   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 528   score: 0.0   memory length: 97406   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 529   score: 8.0   memory length: 97720   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 530   score: 1.0   memory length: 97871   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 531   score: 1.0   memory length: 98039   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 532   score: 3.0   memory length: 98286   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 533   score: 1.0   memory length: 98436   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 534   score: 0.0   memory length: 98559   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 535   score: 2.0   memory length: 98774   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 536   score: 0.0   memory length: 98897   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 537   score: 1.0   memory length: 99048   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 538   score: 1.0   memory length: 99217   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 539   score: 2.0   memory length: 99416   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 540   score: 0.0   memory length: 99539   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 541   score: 0.0   memory length: 99662   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-291524992219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Start training after random sample generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_policy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;31m# Update the target network only for Double DQN only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdouble_dqn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mupdate_target_network_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/agent_double.py\u001b[0m in \u001b[0;36mtrain_policy_net\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Compute Q(s_t, a), the Q-value of the current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m### CODE ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mstate_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnon_terminal_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Compute Q function of next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: gather_out_cuda(): Expected dtype int64 for index"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q26qCm666wV"
      },
      "source": [
        "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04TJiIjZ6-4F"
      },
      "source": [
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Displaying the game live\n",
        "def show_state(env, step=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
        "    plt.axis('off')\n",
        "\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "    \n",
        "# Recording the game and replaying the game afterwards\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m9lADDH7H_V"
      },
      "source": [
        "display = Display(visible=0, size=(300, 200))\n",
        "display.start()\n",
        "\n",
        "# Load agent\n",
        "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
        "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
        "\n",
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "env = wrap_env(env)\n",
        "\n",
        "done = False\n",
        "score = 0\n",
        "step = 0\n",
        "state = env.reset()\n",
        "next_state = state\n",
        "life = number_lives\n",
        "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "get_init_state(history, state)\n",
        "\n",
        "while not done:\n",
        "    \n",
        "    # Render breakout\n",
        "    env.render()\n",
        "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
        "\n",
        "    step += 1\n",
        "    frame += 1\n",
        "\n",
        "    # Perform a fire action if ball is no longer on screen\n",
        "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "        action = 0\n",
        "    else:\n",
        "        action = torch.from_numpy(agent.get_action(torch.from_numpy(np.float32(history[:4, :, :]) / 255.)))\n",
        "    state = next_state\n",
        "    \n",
        "    next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "    frame_next_state = get_frame(next_state)\n",
        "    history[4, :, :] = frame_next_state\n",
        "    terminal_state = check_live(life, info['ale.lives'])\n",
        "        \n",
        "    life = info['ale.lives']\n",
        "    r = np.clip(reward, -1, 1) \n",
        "    r = reward\n",
        "\n",
        "    # Store the transition in memory \n",
        "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "    # Start training after random sample generation\n",
        "    score += reward\n",
        "    \n",
        "    history[:4, :, :] = history[1:, :, :]\n",
        "env.close()\n",
        "show_video()\n",
        "display.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}