{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "MP5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zxAxDhW2OsR"
      },
      "source": [
        "# Deep Q-Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRToCjBW2OsS"
      },
      "source": [
        "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViIj8Nlo2OsS",
        "outputId": "82c0cb52-7c50-44f6-db0e-af8a9f4c9169"
      },
      "source": [
        "!pip3 install gym pyvirtualdisplay\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/8a/643043cc70791367bee2d19eb20e00ed1a246ac48e5dbe57bbbcc8be40a9/PyVirtualDisplay-1.3.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-1.3.2\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,370 kB]\n",
            "Get:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,692 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,134 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,243 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [866 kB]\n",
            "Get:19 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [46.5 kB]\n",
            "Fetched 8,646 kB in 4s (2,278 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl xvfb\n",
            "0 upgraded, 2 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 1,280 kB of archives.\n",
            "After this operation, 7,686 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 1,280 kB in 1s (956 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-5RO7UD2OsS",
        "outputId": "05d83c64-dd9e-42d7-e933-58a7fef74ec1"
      },
      "source": [
        "!pip3 install --upgrade setuptools\n",
        "!pip3 install ez_setup \n",
        "!pip3 install gym[atari] "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (50.3.2)\n",
            "Collecting ez_setup\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/2c/743df41bd6b3298706dfe91b0c7ecdc47f2dc1a3104abeb6e9aa4a45fa5d/ez_setup-0.9.tar.gz\n",
            "Building wheels for collected packages: ez-setup\n",
            "  Building wheel for ez-setup (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ez-setup: filename=ez_setup-0.9-cp36-none-any.whl size=11016 sha256=87f9f0039201c751449409f9a52db1a90d54aadc9d33623aaa3a1cd2af198b93\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/e8/6b/3d5ff5a3efd7b5338d1e173ac981771e2628ceb2f7866d49ad\n",
            "Successfully built ez-setup\n",
            "Installing collected packages: ez-setup\n",
            "Successfully installed ez-setup-0.9\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmk1bjPS2OsS"
      },
      "source": [
        "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DK6GLvh2OsS"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "import gym\n",
        "import torch\n",
        "import pylab\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from copy import deepcopy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
        "from model import DQN\n",
        "from config import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q39zvwKh2OsS"
      },
      "source": [
        "## Understanding the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1NL9Ykx2OsS"
      },
      "source": [
        "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
        "\n",
        "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwck-dp82OsS"
      },
      "source": [
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "state = env.reset()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wiHY5ZF2OsS"
      },
      "source": [
        "number_lives = find_max_lives(env)\n",
        "state_size = env.observation_space.shape\n",
        "action_size = 3 #fire, left, and right"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX9dHkWk2OsS"
      },
      "source": [
        "## Creating a DQN Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRX7F26O2OsS"
      },
      "source": [
        "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
        "\n",
        "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
        "\n",
        "__Frame__ : Number of frames processed in total.\n",
        "\n",
        "__Memory Size__ : The current size of the replay memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHR49W102OsS"
      },
      "source": [
        "double_dqn = False # set to True if using double DQN agent\n",
        "\n",
        "if double_dqn:\n",
        "    from agent_double import Agent\n",
        "else:\n",
        "    from agent import Agent\n",
        "\n",
        "agent = Agent(action_size)\n",
        "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
        "frame = 0\n",
        "memory_size = 0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DeEFu7Z2OsT"
      },
      "source": [
        "### Main Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOthVzF92OsT"
      },
      "source": [
        "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7_5zXMZ2OsT",
        "outputId": "de146bdf-308f-4f4f-9525-6b2e0e22a3f9"
      },
      "source": [
        "rewards, episodes = [], []\n",
        "best_eval_reward = 0\n",
        "for e in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "    step = 0\n",
        "    d = False\n",
        "    state = env.reset()\n",
        "    next_state = state\n",
        "    life = number_lives\n",
        "\n",
        "    get_init_state(history, state)\n",
        "\n",
        "    while not done:\n",
        "        step += 1\n",
        "        frame += 1\n",
        "\n",
        "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
        "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "            action = 0\n",
        "        else:\n",
        "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "        state = next_state\n",
        "        next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "        frame_next_state = get_frame(next_state)\n",
        "        history[4, :, :] = frame_next_state\n",
        "        terminal_state = check_live(life, info['ale.lives'])\n",
        "\n",
        "        life = info['ale.lives']\n",
        "        r = np.clip(reward, -1, 1) \n",
        "        r = reward\n",
        "\n",
        "        # Store the transition in memory \n",
        "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "        # Start training after random sample generation\n",
        "        if(frame >= train_frame):\n",
        "            agent.train_policy_net(frame)\n",
        "            # Update the target network only for Double DQN only\n",
        "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
        "                agent.update_target_net()\n",
        "        score += reward\n",
        "        history[:4, :, :] = history[1:, :, :]\n",
        "            \n",
        "        if done:\n",
        "            evaluation_reward.append(score)\n",
        "            rewards.append(np.mean(evaluation_reward))\n",
        "            episodes.append(e)\n",
        "            pylab.plot(episodes, rewards, 'b')\n",
        "            pylab.xlabel('Episodes')\n",
        "            pylab.ylabel('Rewards') \n",
        "            pylab.title('Episodes vs Reward')\n",
        "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
        "            \n",
        "            # every episode, plot the play time\n",
        "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
        "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
        "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
        "\n",
        "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
        "            ### Change this save condition to whatever you prefer ###\n",
        "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
        "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
        "                best_eval_reward = np.mean(evaluation_reward)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0   score: 1.0   memory length: 170   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 1   score: 1.0   memory length: 322   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 2   score: 0.0   memory length: 445   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.6666666666666666\n",
            "episode: 3   score: 0.0   memory length: 568   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.5\n",
            "episode: 4   score: 1.0   memory length: 738   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 0.6\n",
            "episode: 5   score: 2.0   memory length: 937   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 0.8333333333333334\n",
            "episode: 6   score: 3.0   memory length: 1186   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.1428571428571428\n",
            "episode: 7   score: 0.0   memory length: 1308   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 8   score: 3.0   memory length: 1553   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.2222222222222223\n",
            "episode: 9   score: 2.0   memory length: 1771   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 10   score: 3.0   memory length: 1996   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.4545454545454546\n",
            "episode: 11   score: 1.0   memory length: 2164   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.4166666666666667\n",
            "episode: 12   score: 1.0   memory length: 2333   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3846153846153846\n",
            "episode: 13   score: 0.0   memory length: 2456   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2857142857142858\n",
            "episode: 14   score: 2.0   memory length: 2654   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
            "episode: 15   score: 3.0   memory length: 2901   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.4375\n",
            "episode: 16   score: 3.0   memory length: 3126   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.5294117647058822\n",
            "episode: 17   score: 0.0   memory length: 3248   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4444444444444444\n",
            "episode: 18   score: 1.0   memory length: 3399   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4210526315789473\n",
            "episode: 19   score: 0.0   memory length: 3522   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 20   score: 0.0   memory length: 3645   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2857142857142858\n",
            "episode: 21   score: 0.0   memory length: 3768   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2272727272727273\n",
            "episode: 22   score: 2.0   memory length: 3987   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.2608695652173914\n",
            "episode: 23   score: 2.0   memory length: 4185   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.2916666666666667\n",
            "episode: 24   score: 1.0   memory length: 4335   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 25   score: 0.0   memory length: 4458   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2307692307692308\n",
            "episode: 26   score: 2.0   memory length: 4657   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.2592592592592593\n",
            "episode: 27   score: 0.0   memory length: 4779   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2142857142857142\n",
            "episode: 28   score: 0.0   memory length: 4901   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.1724137931034482\n",
            "episode: 29   score: 1.0   memory length: 5051   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.1666666666666667\n",
            "episode: 30   score: 4.0   memory length: 5347   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.2580645161290323\n",
            "episode: 31   score: 2.0   memory length: 5545   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.28125\n",
            "episode: 32   score: 4.0   memory length: 5804   epsilon: 1.0    steps: 259    lr: 0.0001     evaluation reward: 1.3636363636363635\n",
            "episode: 33   score: 0.0   memory length: 5926   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3235294117647058\n",
            "episode: 34   score: 1.0   memory length: 6095   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3142857142857143\n",
            "episode: 35   score: 2.0   memory length: 6313   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
            "episode: 36   score: 4.0   memory length: 6610   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.4054054054054055\n",
            "episode: 37   score: 1.0   memory length: 6781   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.394736842105263\n",
            "episode: 38   score: 0.0   memory length: 6904   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.358974358974359\n",
            "episode: 39   score: 1.0   memory length: 7055   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 40   score: 2.0   memory length: 7253   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3658536585365855\n",
            "episode: 41   score: 1.0   memory length: 7422   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3571428571428572\n",
            "episode: 42   score: 4.0   memory length: 7681   epsilon: 1.0    steps: 259    lr: 0.0001     evaluation reward: 1.4186046511627908\n",
            "episode: 43   score: 4.0   memory length: 7978   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.4772727272727273\n",
            "episode: 44   score: 1.0   memory length: 8147   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4666666666666666\n",
            "episode: 45   score: 2.0   memory length: 8365   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.4782608695652173\n",
            "episode: 46   score: 1.0   memory length: 8516   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4680851063829787\n",
            "episode: 47   score: 1.0   memory length: 8688   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.4583333333333333\n",
            "episode: 48   score: 0.0   memory length: 8811   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
            "episode: 49   score: 0.0   memory length: 8934   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 50   score: 3.0   memory length: 9180   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.4313725490196079\n",
            "episode: 51   score: 0.0   memory length: 9302   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4038461538461537\n",
            "episode: 52   score: 3.0   memory length: 9549   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.4339622641509433\n",
            "episode: 53   score: 2.0   memory length: 9769   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.4444444444444444\n",
            "episode: 54   score: 1.0   memory length: 9938   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4363636363636363\n",
            "episode: 55   score: 0.0   memory length: 10061   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4107142857142858\n",
            "episode: 56   score: 1.0   memory length: 10230   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4035087719298245\n",
            "episode: 57   score: 1.0   memory length: 10399   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.396551724137931\n",
            "episode: 58   score: 3.0   memory length: 10643   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.423728813559322\n",
            "episode: 59   score: 0.0   memory length: 10765   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 60   score: 3.0   memory length: 11035   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.4262295081967213\n",
            "episode: 61   score: 3.0   memory length: 11280   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.4516129032258065\n",
            "episode: 62   score: 2.0   memory length: 11495   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.4603174603174602\n",
            "episode: 63   score: 0.0   memory length: 11618   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4375\n",
            "episode: 64   score: 0.0   memory length: 11741   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4153846153846155\n",
            "episode: 65   score: 0.0   memory length: 11863   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.393939393939394\n",
            "episode: 66   score: 0.0   memory length: 11986   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.373134328358209\n",
            "episode: 67   score: 1.0   memory length: 12155   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3676470588235294\n",
            "episode: 68   score: 1.0   memory length: 12306   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3623188405797102\n",
            "episode: 69   score: 2.0   memory length: 12503   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.3714285714285714\n",
            "episode: 70   score: 1.0   memory length: 12672   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3661971830985915\n",
            "episode: 71   score: 0.0   memory length: 12795   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3472222222222223\n",
            "episode: 72   score: 3.0   memory length: 13040   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.36986301369863\n",
            "episode: 73   score: 3.0   memory length: 13267   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.3918918918918919\n",
            "episode: 74   score: 2.0   memory length: 13465   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 75   score: 1.0   memory length: 13635   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.394736842105263\n",
            "episode: 76   score: 0.0   memory length: 13758   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3766233766233766\n",
            "episode: 77   score: 0.0   memory length: 13880   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.358974358974359\n",
            "episode: 78   score: 0.0   memory length: 14003   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3417721518987342\n",
            "episode: 79   score: 2.0   memory length: 14204   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 80   score: 3.0   memory length: 14453   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.3703703703703705\n",
            "episode: 81   score: 1.0   memory length: 14604   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3658536585365855\n",
            "episode: 82   score: 1.0   memory length: 14776   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.3614457831325302\n",
            "episode: 83   score: 1.0   memory length: 14948   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.3571428571428572\n",
            "episode: 84   score: 0.0   memory length: 15071   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3411764705882352\n",
            "episode: 85   score: 0.0   memory length: 15193   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3255813953488371\n",
            "episode: 86   score: 0.0   memory length: 15316   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3103448275862069\n",
            "episode: 87   score: 1.0   memory length: 15486   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.3068181818181819\n",
            "episode: 88   score: 4.0   memory length: 15760   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.3370786516853932\n",
            "episode: 89   score: 4.0   memory length: 16055   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.3666666666666667\n",
            "episode: 90   score: 2.0   memory length: 16253   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3736263736263736\n",
            "episode: 91   score: 0.0   memory length: 16375   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.358695652173913\n",
            "episode: 92   score: 1.0   memory length: 16526   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3548387096774193\n",
            "episode: 93   score: 1.0   memory length: 16695   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.351063829787234\n",
            "episode: 94   score: 0.0   memory length: 16818   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3368421052631578\n",
            "episode: 95   score: 3.0   memory length: 17048   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.3541666666666667\n",
            "episode: 96   score: 2.0   memory length: 17250   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.3608247422680413\n",
            "episode: 97   score: 2.0   memory length: 17448   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3673469387755102\n",
            "episode: 98   score: 1.0   memory length: 17617   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3636363636363635\n",
            "episode: 99   score: 0.0   memory length: 17740   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 100   score: 0.0   memory length: 17863   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 101   score: 0.0   memory length: 17986   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 102   score: 3.0   memory length: 18234   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 103   score: 2.0   memory length: 18432   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 104   score: 1.0   memory length: 18602   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 105   score: 0.0   memory length: 18724   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 106   score: 1.0   memory length: 18874   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 107   score: 1.0   memory length: 19026   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 108   score: 1.0   memory length: 19195   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 109   score: 0.0   memory length: 19318   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 110   score: 0.0   memory length: 19441   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 111   score: 2.0   memory length: 19660   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 112   score: 2.0   memory length: 19857   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 113   score: 2.0   memory length: 20073   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 114   score: 1.0   memory length: 20244   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 115   score: 2.0   memory length: 20442   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 116   score: 0.0   memory length: 20565   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 117   score: 4.0   memory length: 20881   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 118   score: 0.0   memory length: 21004   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 119   score: 1.0   memory length: 21173   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 120   score: 1.0   memory length: 21342   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 121   score: 2.0   memory length: 21539   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 122   score: 2.0   memory length: 21756   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 123   score: 0.0   memory length: 21879   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 124   score: 2.0   memory length: 22078   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 125   score: 0.0   memory length: 22200   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 126   score: 0.0   memory length: 22323   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 127   score: 2.0   memory length: 22524   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 128   score: 0.0   memory length: 22647   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 129   score: 0.0   memory length: 22769   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 130   score: 2.0   memory length: 22967   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 131   score: 3.0   memory length: 23213   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 132   score: 1.0   memory length: 23383   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 133   score: 1.0   memory length: 23555   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 134   score: 2.0   memory length: 23753   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 135   score: 2.0   memory length: 23951   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 136   score: 1.0   memory length: 24101   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 137   score: 0.0   memory length: 24223   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 138   score: 1.0   memory length: 24394   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 139   score: 0.0   memory length: 24516   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 140   score: 0.0   memory length: 24638   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 141   score: 5.0   memory length: 24962   epsilon: 1.0    steps: 324    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 142   score: 4.0   memory length: 25220   epsilon: 1.0    steps: 258    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 143   score: 3.0   memory length: 25466   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 144   score: 0.0   memory length: 25589   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 145   score: 4.0   memory length: 25887   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 146   score: 1.0   memory length: 26038   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 147   score: 6.0   memory length: 26423   epsilon: 1.0    steps: 385    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 148   score: 3.0   memory length: 26649   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 149   score: 0.0   memory length: 26772   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 150   score: 0.0   memory length: 26895   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 151   score: 0.0   memory length: 27018   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 152   score: 0.0   memory length: 27141   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 153   score: 0.0   memory length: 27264   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 154   score: 4.0   memory length: 27560   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 155   score: 0.0   memory length: 27682   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 156   score: 4.0   memory length: 27979   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 157   score: 3.0   memory length: 28208   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 158   score: 2.0   memory length: 28425   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 159   score: 1.0   memory length: 28597   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 160   score: 3.0   memory length: 28808   epsilon: 1.0    steps: 211    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 161   score: 4.0   memory length: 29070   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 162   score: 0.0   memory length: 29192   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 163   score: 0.0   memory length: 29314   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 164   score: 1.0   memory length: 29482   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 165   score: 3.0   memory length: 29730   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 166   score: 3.0   memory length: 29976   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 167   score: 4.0   memory length: 30271   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 168   score: 3.0   memory length: 30516   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 169   score: 3.0   memory length: 30783   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 170   score: 1.0   memory length: 30951   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 171   score: 0.0   memory length: 31074   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 172   score: 2.0   memory length: 31291   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 173   score: 1.0   memory length: 31460   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 174   score: 1.0   memory length: 31629   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 175   score: 3.0   memory length: 31855   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 176   score: 1.0   memory length: 32023   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 177   score: 1.0   memory length: 32193   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 178   score: 1.0   memory length: 32344   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 179   score: 2.0   memory length: 32543   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 180   score: 1.0   memory length: 32712   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 181   score: 2.0   memory length: 32910   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 182   score: 7.0   memory length: 33229   epsilon: 1.0    steps: 319    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 183   score: 6.0   memory length: 33569   epsilon: 1.0    steps: 340    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 184   score: 0.0   memory length: 33692   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 185   score: 0.0   memory length: 33815   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 186   score: 2.0   memory length: 34033   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 187   score: 2.0   memory length: 34230   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 188   score: 2.0   memory length: 34428   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 189   score: 2.0   memory length: 34644   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 190   score: 2.0   memory length: 34842   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 191   score: 0.0   memory length: 34964   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 192   score: 2.0   memory length: 35161   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 193   score: 3.0   memory length: 35427   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 194   score: 1.0   memory length: 35599   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 195   score: 1.0   memory length: 35767   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 196   score: 3.0   memory length: 36013   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 197   score: 0.0   memory length: 36136   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 198   score: 2.0   memory length: 36336   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 199   score: 2.0   memory length: 36534   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 200   score: 3.0   memory length: 36781   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 201   score: 0.0   memory length: 36904   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 202   score: 1.0   memory length: 37074   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 203   score: 1.0   memory length: 37243   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 204   score: 3.0   memory length: 37513   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 205   score: 2.0   memory length: 37711   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 206   score: 0.0   memory length: 37834   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 207   score: 0.0   memory length: 37956   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 208   score: 3.0   memory length: 38219   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 209   score: 2.0   memory length: 38439   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 210   score: 2.0   memory length: 38637   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 211   score: 0.0   memory length: 38759   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 212   score: 2.0   memory length: 38977   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 213   score: 2.0   memory length: 39161   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 214   score: 3.0   memory length: 39388   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 215   score: 1.0   memory length: 39557   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 216   score: 0.0   memory length: 39680   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 217   score: 4.0   memory length: 39957   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 218   score: 1.0   memory length: 40129   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 219   score: 0.0   memory length: 40252   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 220   score: 1.0   memory length: 40402   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 221   score: 0.0   memory length: 40525   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 222   score: 0.0   memory length: 40648   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 223   score: 0.0   memory length: 40770   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 224   score: 4.0   memory length: 41047   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 225   score: 0.0   memory length: 41170   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 226   score: 3.0   memory length: 41437   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 227   score: 2.0   memory length: 41636   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 228   score: 3.0   memory length: 41861   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 229   score: 1.0   memory length: 42012   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 230   score: 2.0   memory length: 42227   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 231   score: 2.0   memory length: 42425   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 232   score: 1.0   memory length: 42593   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 233   score: 4.0   memory length: 42887   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 234   score: 1.0   memory length: 43038   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 235   score: 1.0   memory length: 43207   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 236   score: 2.0   memory length: 43425   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 237   score: 2.0   memory length: 43645   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 238   score: 0.0   memory length: 43767   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 239   score: 3.0   memory length: 44013   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 240   score: 1.0   memory length: 44164   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 241   score: 2.0   memory length: 44379   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 242   score: 1.0   memory length: 44548   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 243   score: 2.0   memory length: 44729   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 244   score: 1.0   memory length: 44880   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 245   score: 4.0   memory length: 45157   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 246   score: 0.0   memory length: 45280   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 247   score: 0.0   memory length: 45403   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 248   score: 1.0   memory length: 45553   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 249   score: 0.0   memory length: 45675   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 250   score: 4.0   memory length: 45951   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 251   score: 2.0   memory length: 46149   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 252   score: 1.0   memory length: 46319   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 253   score: 1.0   memory length: 46488   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 254   score: 3.0   memory length: 46753   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 255   score: 1.0   memory length: 46925   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 256   score: 4.0   memory length: 47168   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 257   score: 0.0   memory length: 47291   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 258   score: 5.0   memory length: 47599   epsilon: 1.0    steps: 308    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 259   score: 1.0   memory length: 47771   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 260   score: 1.0   memory length: 47942   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 261   score: 1.0   memory length: 48092   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 262   score: 1.0   memory length: 48264   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 263   score: 1.0   memory length: 48415   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 264   score: 2.0   memory length: 48615   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 265   score: 0.0   memory length: 48738   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 266   score: 2.0   memory length: 48920   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 267   score: 4.0   memory length: 49217   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 268   score: 2.0   memory length: 49415   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 269   score: 1.0   memory length: 49565   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 270   score: 2.0   memory length: 49766   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 271   score: 1.0   memory length: 49935   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 272   score: 4.0   memory length: 50226   epsilon: 1.0    steps: 291    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 273   score: 2.0   memory length: 50442   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 274   score: 3.0   memory length: 50687   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 275   score: 3.0   memory length: 50915   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 276   score: 3.0   memory length: 51124   epsilon: 1.0    steps: 209    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 277   score: 3.0   memory length: 51371   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 278   score: 2.0   memory length: 51568   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 279   score: 1.0   memory length: 51737   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 280   score: 3.0   memory length: 52003   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 281   score: 0.0   memory length: 52126   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 282   score: 2.0   memory length: 52345   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 283   score: 4.0   memory length: 52620   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 284   score: 1.0   memory length: 52789   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 285   score: 3.0   memory length: 53018   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 286   score: 0.0   memory length: 53141   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 287   score: 3.0   memory length: 53370   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 288   score: 2.0   memory length: 53567   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 289   score: 3.0   memory length: 53832   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 290   score: 1.0   memory length: 54000   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 291   score: 0.0   memory length: 54122   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 292   score: 6.0   memory length: 54462   epsilon: 1.0    steps: 340    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 293   score: 4.0   memory length: 54705   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 294   score: 2.0   memory length: 54902   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 295   score: 1.0   memory length: 55072   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 296   score: 2.0   memory length: 55291   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 297   score: 2.0   memory length: 55489   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 298   score: 0.0   memory length: 55612   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 299   score: 0.0   memory length: 55735   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 300   score: 2.0   memory length: 55933   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 301   score: 1.0   memory length: 56084   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 302   score: 2.0   memory length: 56282   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 303   score: 3.0   memory length: 56528   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 304   score: 2.0   memory length: 56744   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 305   score: 0.0   memory length: 56866   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 306   score: 3.0   memory length: 57130   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 307   score: 0.0   memory length: 57253   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 308   score: 1.0   memory length: 57404   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 309   score: 1.0   memory length: 57573   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 310   score: 3.0   memory length: 57839   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 311   score: 3.0   memory length: 58067   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 312   score: 0.0   memory length: 58189   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 313   score: 3.0   memory length: 58438   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 314   score: 4.0   memory length: 58734   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 315   score: 2.0   memory length: 58933   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 316   score: 0.0   memory length: 59056   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 317   score: 5.0   memory length: 59380   epsilon: 1.0    steps: 324    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 318   score: 1.0   memory length: 59550   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 319   score: 2.0   memory length: 59750   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 320   score: 1.0   memory length: 59918   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 321   score: 1.0   memory length: 60090   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 322   score: 0.0   memory length: 60212   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 323   score: 0.0   memory length: 60335   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 324   score: 3.0   memory length: 60604   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 325   score: 1.0   memory length: 60775   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 326   score: 5.0   memory length: 61121   epsilon: 1.0    steps: 346    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 327   score: 1.0   memory length: 61271   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 328   score: 1.0   memory length: 61439   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 329   score: 4.0   memory length: 61733   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 330   score: 2.0   memory length: 61930   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 331   score: 1.0   memory length: 62098   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 332   score: 0.0   memory length: 62221   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 333   score: 3.0   memory length: 62450   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 334   score: 1.0   memory length: 62600   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 335   score: 3.0   memory length: 62847   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 336   score: 0.0   memory length: 62970   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e8WsQh42OsT"
      },
      "source": [
        "# Visualize Agent Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20vLFGcp2OsT"
      },
      "source": [
        "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
        "\n",
        "Please save your model before running this portion of the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXQttKal2OsT"
      },
      "source": [
        "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZqvZeoJ2OsU"
      },
      "source": [
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Displaying the game live\n",
        "def show_state(env, step=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
        "    plt.axis('off')\n",
        "\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "    \n",
        "# Recording the game and replaying the game afterwards\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "uThODipO2OsU",
        "outputId": "cd095564-9c94-4afd-d226-cdb0c5c47b41"
      },
      "source": [
        "display = Display(visible=0, size=(300, 200))\n",
        "display.start()\n",
        "\n",
        "# Load agent\n",
        "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
        "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
        "\n",
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "env = wrap_env(env)\n",
        "\n",
        "done = False\n",
        "score = 0\n",
        "step = 0\n",
        "state = env.reset()\n",
        "next_state = state\n",
        "life = number_lives\n",
        "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "get_init_state(history, state)\n",
        "\n",
        "while not done:\n",
        "    \n",
        "    # Render breakout\n",
        "    env.render()\n",
        "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
        "\n",
        "    step += 1\n",
        "    frame += 1\n",
        "\n",
        "    # Perform a fire action if ball is no longer on screen\n",
        "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "        action = 0\n",
        "    else:\n",
        "        action = torch.from_numpy(agent.get_action(np.float32(history[:4, :, :]) / 255.))\n",
        "    state = next_state\n",
        "    \n",
        "    next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "    frame_next_state = get_frame(next_state)\n",
        "    history[4, :, :] = frame_next_state\n",
        "    terminal_state = check_live(life, info['ale.lives'])\n",
        "        \n",
        "    life = info['ale.lives']\n",
        "    r = np.clip(reward, -1, 1) \n",
        "    r = reward\n",
        "\n",
        "    # Store the transition in memory \n",
        "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "    # Start training after random sample generation\n",
        "    score += reward\n",
        "    \n",
        "    history[:4, :, :] = history[1:, :, :]\n",
        "env.close()\n",
        "show_video()\n",
        "display.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-20a869d7b7a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/agent.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m               \u001b[0;31m#print('best action ', self.policy_net(state).max(1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m               \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# pick samples randomly from replay memory (with batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: conv2d(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoMFyPg82OsU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}