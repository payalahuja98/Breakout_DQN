{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    },
    "colab": {
      "name": "MP5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zxAxDhW2OsR"
      },
      "source": [
        "# Deep Q-Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRToCjBW2OsS"
      },
      "source": [
        "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViIj8Nlo2OsS",
        "outputId": "b71d98d4-9445-4818-9773-e963c182d1d4"
      },
      "source": [
        "!pip3 install gym pyvirtualdisplay\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Fetched 252 kB in 1s (324 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-5RO7UD2OsS",
        "outputId": "fa533ab0-4b7b-4ef1-dc16-d55e3de01219"
      },
      "source": [
        "!pip3 install --upgrade setuptools\n",
        "!pip3 install ez_setup \n",
        "!pip3 install gym[atari] "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (51.0.0)\n",
            "Requirement already satisfied: ez_setup in /usr/local/lib/python3.6/dist-packages (0.9)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.18.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmk1bjPS2OsS"
      },
      "source": [
        "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVutbLfuidTL",
        "outputId": "14ba5a2d-6a48-4da7-fbe1-db5950643668"
      },
      "source": [
        "pip install torch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DK6GLvh2OsS"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "import gym\n",
        "import torch\n",
        "import pylab\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from copy import deepcopy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
        "from model import DQN\n",
        "from config import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q39zvwKh2OsS"
      },
      "source": [
        "## Understanding the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1NL9Ykx2OsS"
      },
      "source": [
        "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
        "\n",
        "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwck-dp82OsS"
      },
      "source": [
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "state = env.reset()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wiHY5ZF2OsS"
      },
      "source": [
        "number_lives = find_max_lives(env)\n",
        "state_size = env.observation_space.shape\n",
        "action_size = 3 #fire, left, and right"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX9dHkWk2OsS"
      },
      "source": [
        "## Creating a DQN Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRX7F26O2OsS"
      },
      "source": [
        "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
        "\n",
        "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
        "\n",
        "__Frame__ : Number of frames processed in total.\n",
        "\n",
        "__Memory Size__ : The current size of the replay memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHR49W102OsS"
      },
      "source": [
        "double_dqn = False # set to True if using double DQN agent\n",
        "\n",
        "if double_dqn:\n",
        "    from agent_double import Agent\n",
        "else:\n",
        "    from agent import Agent\n",
        "\n",
        "agent = Agent(action_size)\n",
        "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
        "frame = 0\n",
        "memory_size = 0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DeEFu7Z2OsT"
      },
      "source": [
        "### Main Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOthVzF92OsT"
      },
      "source": [
        "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W7_5zXMZ2OsT",
        "outputId": "0b986ae5-816b-40e9-c3d3-d154c771fa5a"
      },
      "source": [
        "rewards, episodes = [], []\n",
        "best_eval_reward = 0\n",
        "for e in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "    step = 0\n",
        "    d = False\n",
        "    state = env.reset()\n",
        "    next_state = state\n",
        "    life = number_lives\n",
        "\n",
        "    get_init_state(history, state)\n",
        "\n",
        "    while not done:\n",
        "        step += 1\n",
        "        frame += 1\n",
        "\n",
        "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
        "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "            action = 0\n",
        "        else:\n",
        "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "        state = next_state\n",
        "        next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "        frame_next_state = get_frame(next_state)\n",
        "        history[4, :, :] = frame_next_state\n",
        "        terminal_state = check_live(life, info['ale.lives'])\n",
        "\n",
        "        life = info['ale.lives']\n",
        "        r = np.clip(reward, -1, 1) \n",
        "        r = reward\n",
        "\n",
        "        # Store the transition in memory \n",
        "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "        # Start training after random sample generation\n",
        "        if(frame >= train_frame):\n",
        "            agent.train_policy_net(frame)\n",
        "            # Update the target network only for Double DQN only\n",
        "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
        "                agent.update_target_net()\n",
        "        score += reward\n",
        "        history[:4, :, :] = history[1:, :, :]\n",
        "            \n",
        "        if done:\n",
        "            evaluation_reward.append(score)\n",
        "            rewards.append(np.mean(evaluation_reward))\n",
        "            episodes.append(e)\n",
        "            pylab.plot(episodes, rewards, 'b')\n",
        "            pylab.xlabel('Episodes')\n",
        "            pylab.ylabel('Rewards') \n",
        "            pylab.title('Episodes vs Reward')\n",
        "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
        "            \n",
        "            # every episode, plot the play time\n",
        "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
        "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
        "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
        "\n",
        "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
        "            ### Change this save condition to whatever you prefer ###\n",
        "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
        "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
        "                best_eval_reward = np.mean(evaluation_reward)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0   score: 4.0   memory length: 296   epsilon: 1.0    steps: 296    lr: 1e-05     evaluation reward: 4.0\n",
            "episode: 1   score: 4.0   memory length: 592   epsilon: 1.0    steps: 296    lr: 1e-05     evaluation reward: 4.0\n",
            "episode: 2   score: 0.0   memory length: 715   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 2.6666666666666665\n",
            "episode: 3   score: 2.0   memory length: 913   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 2.5\n",
            "episode: 4   score: 0.0   memory length: 1035   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 2.0\n",
            "episode: 5   score: 2.0   memory length: 1233   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 2.0\n",
            "episode: 6   score: 2.0   memory length: 1430   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 2.0\n",
            "episode: 7   score: 3.0   memory length: 1676   epsilon: 1.0    steps: 246    lr: 1e-05     evaluation reward: 2.125\n",
            "episode: 8   score: 1.0   memory length: 1845   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 2.0\n",
            "episode: 9   score: 1.0   memory length: 2014   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.9\n",
            "episode: 10   score: 0.0   memory length: 2137   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.7272727272727273\n",
            "episode: 11   score: 2.0   memory length: 2335   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.75\n",
            "episode: 12   score: 0.0   memory length: 2458   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.6153846153846154\n",
            "episode: 13   score: 1.0   memory length: 2609   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.5714285714285714\n",
            "episode: 14   score: 1.0   memory length: 2778   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.5333333333333334\n",
            "episode: 15   score: 3.0   memory length: 3022   epsilon: 1.0    steps: 244    lr: 1e-05     evaluation reward: 1.625\n",
            "episode: 16   score: 1.0   memory length: 3191   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.588235294117647\n",
            "episode: 17   score: 0.0   memory length: 3313   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 18   score: 3.0   memory length: 3560   epsilon: 1.0    steps: 247    lr: 1e-05     evaluation reward: 1.5789473684210527\n",
            "episode: 19   score: 2.0   memory length: 3775   epsilon: 1.0    steps: 215    lr: 1e-05     evaluation reward: 1.6\n",
            "episode: 20   score: 0.0   memory length: 3898   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.5238095238095237\n",
            "episode: 21   score: 0.0   memory length: 4021   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.4545454545454546\n",
            "episode: 22   score: 2.0   memory length: 4200   epsilon: 1.0    steps: 179    lr: 1e-05     evaluation reward: 1.4782608695652173\n",
            "episode: 23   score: 2.0   memory length: 4381   epsilon: 1.0    steps: 181    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 24   score: 1.0   memory length: 4531   epsilon: 1.0    steps: 150    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 25   score: 3.0   memory length: 4776   epsilon: 1.0    steps: 245    lr: 1e-05     evaluation reward: 1.5384615384615385\n",
            "episode: 26   score: 0.0   memory length: 4899   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.4814814814814814\n",
            "episode: 27   score: 3.0   memory length: 5144   epsilon: 1.0    steps: 245    lr: 1e-05     evaluation reward: 1.5357142857142858\n",
            "episode: 28   score: 1.0   memory length: 5294   epsilon: 1.0    steps: 150    lr: 1e-05     evaluation reward: 1.5172413793103448\n",
            "episode: 29   score: 0.0   memory length: 5417   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.4666666666666666\n",
            "episode: 30   score: 0.0   memory length: 5540   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.4193548387096775\n",
            "episode: 31   score: 1.0   memory length: 5691   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.40625\n",
            "episode: 32   score: 0.0   memory length: 5814   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.3636363636363635\n",
            "episode: 33   score: 0.0   memory length: 5937   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.3235294117647058\n",
            "episode: 34   score: 0.0   memory length: 6060   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.2857142857142858\n",
            "episode: 35   score: 3.0   memory length: 6286   epsilon: 1.0    steps: 226    lr: 1e-05     evaluation reward: 1.3333333333333333\n",
            "episode: 36   score: 2.0   memory length: 6483   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.3513513513513513\n",
            "episode: 37   score: 2.0   memory length: 6703   epsilon: 1.0    steps: 220    lr: 1e-05     evaluation reward: 1.368421052631579\n",
            "episode: 38   score: 5.0   memory length: 7048   epsilon: 1.0    steps: 345    lr: 1e-05     evaluation reward: 1.4615384615384615\n",
            "episode: 39   score: 1.0   memory length: 7199   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 40   score: 4.0   memory length: 7444   epsilon: 1.0    steps: 245    lr: 1e-05     evaluation reward: 1.5121951219512195\n",
            "episode: 41   score: 0.0   memory length: 7566   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.4761904761904763\n",
            "episode: 42   score: 0.0   memory length: 7688   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.441860465116279\n",
            "episode: 43   score: 2.0   memory length: 7886   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.4545454545454546\n",
            "episode: 44   score: 1.0   memory length: 8057   epsilon: 1.0    steps: 171    lr: 1e-05     evaluation reward: 1.4444444444444444\n",
            "episode: 45   score: 0.0   memory length: 8180   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.4130434782608696\n",
            "episode: 46   score: 4.0   memory length: 8477   epsilon: 1.0    steps: 297    lr: 1e-05     evaluation reward: 1.4680851063829787\n",
            "episode: 47   score: 0.0   memory length: 8600   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.4375\n",
            "episode: 48   score: 0.0   memory length: 8722   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.4081632653061225\n",
            "episode: 49   score: 1.0   memory length: 8873   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 50   score: 1.0   memory length: 9042   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.392156862745098\n",
            "episode: 51   score: 2.0   memory length: 9240   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.4038461538461537\n",
            "episode: 52   score: 2.0   memory length: 9456   epsilon: 1.0    steps: 216    lr: 1e-05     evaluation reward: 1.4150943396226414\n",
            "episode: 53   score: 3.0   memory length: 9682   epsilon: 1.0    steps: 226    lr: 1e-05     evaluation reward: 1.4444444444444444\n",
            "episode: 54   score: 1.0   memory length: 9851   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.4363636363636363\n",
            "episode: 55   score: 3.0   memory length: 10115   epsilon: 1.0    steps: 264    lr: 1e-05     evaluation reward: 1.4642857142857142\n",
            "episode: 56   score: 2.0   memory length: 10316   epsilon: 1.0    steps: 201    lr: 1e-05     evaluation reward: 1.4736842105263157\n",
            "episode: 57   score: 0.0   memory length: 10438   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.4482758620689655\n",
            "episode: 58   score: 0.0   memory length: 10561   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.423728813559322\n",
            "episode: 59   score: 1.0   memory length: 10729   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.4166666666666667\n",
            "episode: 60   score: 1.0   memory length: 10898   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.4098360655737705\n",
            "episode: 61   score: 4.0   memory length: 11185   epsilon: 1.0    steps: 287    lr: 1e-05     evaluation reward: 1.4516129032258065\n",
            "episode: 62   score: 1.0   memory length: 11354   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.4444444444444444\n",
            "episode: 63   score: 2.0   memory length: 11552   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.453125\n",
            "episode: 64   score: 3.0   memory length: 11821   epsilon: 1.0    steps: 269    lr: 1e-05     evaluation reward: 1.476923076923077\n",
            "episode: 65   score: 5.0   memory length: 12166   epsilon: 1.0    steps: 345    lr: 1e-05     evaluation reward: 1.5303030303030303\n",
            "episode: 66   score: 1.0   memory length: 12335   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.5223880597014925\n",
            "episode: 67   score: 0.0   memory length: 12458   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 68   score: 6.0   memory length: 12828   epsilon: 1.0    steps: 370    lr: 1e-05     evaluation reward: 1.565217391304348\n",
            "episode: 69   score: 1.0   memory length: 12979   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.5571428571428572\n",
            "episode: 70   score: 1.0   memory length: 13147   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.5492957746478873\n",
            "episode: 71   score: 0.0   memory length: 13270   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.5277777777777777\n",
            "episode: 72   score: 1.0   memory length: 13421   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.5205479452054795\n",
            "episode: 73   score: 1.0   memory length: 13590   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.5135135135135136\n",
            "episode: 74   score: 2.0   memory length: 13788   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 75   score: 1.0   memory length: 13956   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.513157894736842\n",
            "episode: 76   score: 1.0   memory length: 14107   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.5064935064935066\n",
            "episode: 77   score: 3.0   memory length: 14358   epsilon: 1.0    steps: 251    lr: 1e-05     evaluation reward: 1.5256410256410255\n",
            "episode: 78   score: 2.0   memory length: 14574   epsilon: 1.0    steps: 216    lr: 1e-05     evaluation reward: 1.5316455696202531\n",
            "episode: 79   score: 0.0   memory length: 14696   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.5125\n",
            "episode: 80   score: 4.0   memory length: 14990   epsilon: 1.0    steps: 294    lr: 1e-05     evaluation reward: 1.5432098765432098\n",
            "episode: 81   score: 1.0   memory length: 15159   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.5365853658536586\n",
            "episode: 82   score: 3.0   memory length: 15406   epsilon: 1.0    steps: 247    lr: 1e-05     evaluation reward: 1.5542168674698795\n",
            "episode: 83   score: 0.0   memory length: 15529   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.5357142857142858\n",
            "episode: 84   score: 1.0   memory length: 15680   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.5294117647058822\n",
            "episode: 85   score: 1.0   memory length: 15849   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.5232558139534884\n",
            "episode: 86   score: 2.0   memory length: 16047   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.528735632183908\n",
            "episode: 87   score: 2.0   memory length: 16266   epsilon: 1.0    steps: 219    lr: 1e-05     evaluation reward: 1.5340909090909092\n",
            "episode: 88   score: 1.0   memory length: 16416   epsilon: 1.0    steps: 150    lr: 1e-05     evaluation reward: 1.5280898876404494\n",
            "episode: 89   score: 2.0   memory length: 16614   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.5333333333333334\n",
            "episode: 90   score: 2.0   memory length: 16812   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.5384615384615385\n",
            "episode: 91   score: 2.0   memory length: 17028   epsilon: 1.0    steps: 216    lr: 1e-05     evaluation reward: 1.5434782608695652\n",
            "episode: 92   score: 5.0   memory length: 17334   epsilon: 1.0    steps: 306    lr: 1e-05     evaluation reward: 1.5806451612903225\n",
            "episode: 93   score: 2.0   memory length: 17533   epsilon: 1.0    steps: 199    lr: 1e-05     evaluation reward: 1.5851063829787233\n",
            "episode: 94   score: 2.0   memory length: 17754   epsilon: 1.0    steps: 221    lr: 1e-05     evaluation reward: 1.5894736842105264\n",
            "episode: 95   score: 3.0   memory length: 18002   epsilon: 1.0    steps: 248    lr: 1e-05     evaluation reward: 1.6041666666666667\n",
            "episode: 96   score: 4.0   memory length: 18319   epsilon: 1.0    steps: 317    lr: 1e-05     evaluation reward: 1.6288659793814433\n",
            "episode: 97   score: 0.0   memory length: 18441   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.6122448979591837\n",
            "episode: 98   score: 0.0   memory length: 18563   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.595959595959596\n",
            "episode: 99   score: 3.0   memory length: 18790   epsilon: 1.0    steps: 227    lr: 1e-05     evaluation reward: 1.61\n",
            "episode: 100   score: 3.0   memory length: 19040   epsilon: 1.0    steps: 250    lr: 1e-05     evaluation reward: 1.6\n",
            "episode: 101   score: 0.0   memory length: 19163   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 102   score: 0.0   memory length: 19286   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 103   score: 3.0   memory length: 19553   epsilon: 1.0    steps: 267    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 104   score: 0.0   memory length: 19676   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 105   score: 0.0   memory length: 19798   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 106   score: 1.0   memory length: 19968   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.54\n",
            "episode: 107   score: 1.0   memory length: 20138   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 108   score: 1.0   memory length: 20308   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 109   score: 0.0   memory length: 20431   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 110   score: 2.0   memory length: 20649   epsilon: 1.0    steps: 218    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 111   score: 0.0   memory length: 20772   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 112   score: 3.0   memory length: 21038   epsilon: 1.0    steps: 266    lr: 1e-05     evaluation reward: 1.54\n",
            "episode: 113   score: 0.0   memory length: 21161   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 114   score: 1.0   memory length: 21311   epsilon: 1.0    steps: 150    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 115   score: 0.0   memory length: 21433   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 116   score: 3.0   memory length: 21658   epsilon: 1.0    steps: 225    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 117   score: 1.0   memory length: 21830   epsilon: 1.0    steps: 172    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 118   score: 6.0   memory length: 22174   epsilon: 1.0    steps: 344    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 119   score: 2.0   memory length: 22374   epsilon: 1.0    steps: 200    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 120   score: 2.0   memory length: 22571   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 121   score: 1.0   memory length: 22741   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.59\n",
            "episode: 122   score: 3.0   memory length: 23010   epsilon: 1.0    steps: 269    lr: 1e-05     evaluation reward: 1.6\n",
            "episode: 123   score: 3.0   memory length: 23237   epsilon: 1.0    steps: 227    lr: 1e-05     evaluation reward: 1.61\n",
            "episode: 124   score: 2.0   memory length: 23419   epsilon: 1.0    steps: 182    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 125   score: 3.0   memory length: 23688   epsilon: 1.0    steps: 269    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 126   score: 1.0   memory length: 23857   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 127   score: 4.0   memory length: 24135   epsilon: 1.0    steps: 278    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 128   score: 3.0   memory length: 24405   epsilon: 1.0    steps: 270    lr: 1e-05     evaluation reward: 1.66\n",
            "episode: 129   score: 1.0   memory length: 24555   epsilon: 1.0    steps: 150    lr: 1e-05     evaluation reward: 1.67\n",
            "episode: 130   score: 1.0   memory length: 24725   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.68\n",
            "episode: 131   score: 3.0   memory length: 24995   epsilon: 1.0    steps: 270    lr: 1e-05     evaluation reward: 1.7\n",
            "episode: 132   score: 0.0   memory length: 25117   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.7\n",
            "episode: 133   score: 1.0   memory length: 25285   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.71\n",
            "episode: 134   score: 3.0   memory length: 25552   epsilon: 1.0    steps: 267    lr: 1e-05     evaluation reward: 1.74\n",
            "episode: 135   score: 0.0   memory length: 25675   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.71\n",
            "episode: 136   score: 0.0   memory length: 25798   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.69\n",
            "episode: 137   score: 0.0   memory length: 25921   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.67\n",
            "episode: 138   score: 2.0   memory length: 26119   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 139   score: 3.0   memory length: 26386   epsilon: 1.0    steps: 267    lr: 1e-05     evaluation reward: 1.66\n",
            "episode: 140   score: 2.0   memory length: 26584   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 141   score: 3.0   memory length: 26809   epsilon: 1.0    steps: 225    lr: 1e-05     evaluation reward: 1.67\n",
            "episode: 142   score: 0.0   memory length: 26931   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.67\n",
            "episode: 143   score: 3.0   memory length: 27158   epsilon: 1.0    steps: 227    lr: 1e-05     evaluation reward: 1.68\n",
            "episode: 144   score: 2.0   memory length: 27378   epsilon: 1.0    steps: 220    lr: 1e-05     evaluation reward: 1.69\n",
            "episode: 145   score: 2.0   memory length: 27594   epsilon: 1.0    steps: 216    lr: 1e-05     evaluation reward: 1.71\n",
            "episode: 146   score: 2.0   memory length: 27794   epsilon: 1.0    steps: 200    lr: 1e-05     evaluation reward: 1.69\n",
            "episode: 147   score: 3.0   memory length: 28058   epsilon: 1.0    steps: 264    lr: 1e-05     evaluation reward: 1.72\n",
            "episode: 148   score: 1.0   memory length: 28226   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.73\n",
            "episode: 149   score: 1.0   memory length: 28395   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.73\n",
            "episode: 150   score: 0.0   memory length: 28517   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.72\n",
            "episode: 151   score: 1.0   memory length: 28668   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.71\n",
            "episode: 152   score: 1.0   memory length: 28837   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.7\n",
            "episode: 153   score: 1.0   memory length: 29006   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.68\n",
            "episode: 154   score: 4.0   memory length: 29265   epsilon: 1.0    steps: 259    lr: 1e-05     evaluation reward: 1.71\n",
            "episode: 155   score: 1.0   memory length: 29434   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.69\n",
            "episode: 156   score: 0.0   memory length: 29557   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.67\n",
            "episode: 157   score: 0.0   memory length: 29680   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.67\n",
            "episode: 158   score: 2.0   memory length: 29863   epsilon: 1.0    steps: 183    lr: 1e-05     evaluation reward: 1.69\n",
            "episode: 159   score: 2.0   memory length: 30081   epsilon: 1.0    steps: 218    lr: 1e-05     evaluation reward: 1.7\n",
            "episode: 160   score: 0.0   memory length: 30203   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.69\n",
            "episode: 161   score: 0.0   memory length: 30326   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.65\n",
            "episode: 162   score: 3.0   memory length: 30555   epsilon: 1.0    steps: 229    lr: 1e-05     evaluation reward: 1.67\n",
            "episode: 163   score: 1.0   memory length: 30725   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.66\n",
            "episode: 164   score: 0.0   memory length: 30848   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 165   score: 2.0   memory length: 31066   epsilon: 1.0    steps: 218    lr: 1e-05     evaluation reward: 1.6\n",
            "episode: 166   score: 2.0   memory length: 31285   epsilon: 1.0    steps: 219    lr: 1e-05     evaluation reward: 1.61\n",
            "episode: 167   score: 4.0   memory length: 31598   epsilon: 1.0    steps: 313    lr: 1e-05     evaluation reward: 1.65\n",
            "episode: 168   score: 3.0   memory length: 31827   epsilon: 1.0    steps: 229    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 169   score: 1.0   memory length: 31998   epsilon: 1.0    steps: 171    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 170   score: 2.0   memory length: 32216   epsilon: 1.0    steps: 218    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 171   score: 0.0   memory length: 32339   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 172   score: 3.0   memory length: 32588   epsilon: 1.0    steps: 249    lr: 1e-05     evaluation reward: 1.65\n",
            "episode: 173   score: 1.0   memory length: 32739   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.65\n",
            "episode: 174   score: 0.0   memory length: 32862   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 175   score: 3.0   memory length: 33088   epsilon: 1.0    steps: 226    lr: 1e-05     evaluation reward: 1.65\n",
            "episode: 176   score: 0.0   memory length: 33210   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 177   score: 1.0   memory length: 33361   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 178   score: 1.0   memory length: 33529   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.61\n",
            "episode: 179   score: 1.0   memory length: 33698   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 180   score: 0.0   memory length: 33821   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 181   score: 1.0   memory length: 33990   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 182   score: 1.0   memory length: 34161   epsilon: 1.0    steps: 171    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 183   score: 2.0   memory length: 34359   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 184   score: 1.0   memory length: 34510   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 185   score: 1.0   memory length: 34678   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 186   score: 2.0   memory length: 34876   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 187   score: 0.0   memory length: 34999   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 188   score: 4.0   memory length: 35274   epsilon: 1.0    steps: 275    lr: 1e-05     evaluation reward: 1.59\n",
            "episode: 189   score: 3.0   memory length: 35500   epsilon: 1.0    steps: 226    lr: 1e-05     evaluation reward: 1.6\n",
            "episode: 190   score: 0.0   memory length: 35623   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 191   score: 0.0   memory length: 35745   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 192   score: 2.0   memory length: 35943   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 193   score: 1.0   memory length: 36094   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 194   score: 2.0   memory length: 36292   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 195   score: 1.0   memory length: 36443   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 196   score: 3.0   memory length: 36689   epsilon: 1.0    steps: 246    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 197   score: 0.0   memory length: 36812   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 198   score: 0.0   memory length: 36934   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 199   score: 0.0   memory length: 37056   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 200   score: 3.0   memory length: 37303   epsilon: 1.0    steps: 247    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 201   score: 2.0   memory length: 37521   epsilon: 1.0    steps: 218    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 202   score: 1.0   memory length: 37690   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 203   score: 1.0   memory length: 37859   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 204   score: 3.0   memory length: 38123   epsilon: 1.0    steps: 264    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 205   score: 0.0   memory length: 38246   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 206   score: 4.0   memory length: 38546   epsilon: 1.0    steps: 300    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 207   score: 0.0   memory length: 38669   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 208   score: 3.0   memory length: 38915   epsilon: 1.0    steps: 246    lr: 1e-05     evaluation reward: 1.54\n",
            "episode: 209   score: 1.0   memory length: 39083   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 210   score: 2.0   memory length: 39281   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 211   score: 1.0   memory length: 39432   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 212   score: 3.0   memory length: 39667   epsilon: 1.0    steps: 235    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 213   score: 2.0   memory length: 39865   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 214   score: 3.0   memory length: 40132   epsilon: 1.0    steps: 267    lr: 1e-05     evaluation reward: 1.6\n",
            "episode: 215   score: 2.0   memory length: 40349   epsilon: 1.0    steps: 217    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 216   score: 2.0   memory length: 40547   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.61\n",
            "episode: 217   score: 1.0   memory length: 40697   epsilon: 1.0    steps: 150    lr: 1e-05     evaluation reward: 1.61\n",
            "episode: 218   score: 2.0   memory length: 40918   epsilon: 1.0    steps: 221    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 219   score: 2.0   memory length: 41115   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 220   score: 1.0   memory length: 41284   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 221   score: 0.0   memory length: 41407   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 222   score: 2.0   memory length: 41606   epsilon: 1.0    steps: 199    lr: 1e-05     evaluation reward: 1.54\n",
            "episode: 223   score: 3.0   memory length: 41834   epsilon: 1.0    steps: 228    lr: 1e-05     evaluation reward: 1.54\n",
            "episode: 224   score: 1.0   memory length: 42002   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 225   score: 0.0   memory length: 42124   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 226   score: 3.0   memory length: 42373   epsilon: 1.0    steps: 249    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 227   score: 1.0   memory length: 42524   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 228   score: 2.0   memory length: 42722   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 229   score: 0.0   memory length: 42844   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 230   score: 0.0   memory length: 42966   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 231   score: 3.0   memory length: 43192   epsilon: 1.0    steps: 226    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 232   score: 2.0   memory length: 43390   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 233   score: 0.0   memory length: 43512   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 234   score: 0.0   memory length: 43635   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 235   score: 1.0   memory length: 43804   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 236   score: 0.0   memory length: 43927   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 237   score: 0.0   memory length: 44050   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 238   score: 1.0   memory length: 44219   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 239   score: 2.0   memory length: 44416   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 240   score: 1.0   memory length: 44567   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 241   score: 1.0   memory length: 44738   epsilon: 1.0    steps: 171    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 242   score: 1.0   memory length: 44907   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 243   score: 0.0   memory length: 45029   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.38\n",
            "episode: 244   score: 1.0   memory length: 45179   epsilon: 1.0    steps: 150    lr: 1e-05     evaluation reward: 1.37\n",
            "episode: 245   score: 0.0   memory length: 45301   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 246   score: 0.0   memory length: 45423   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.33\n",
            "episode: 247   score: 2.0   memory length: 45640   epsilon: 1.0    steps: 217    lr: 1e-05     evaluation reward: 1.32\n",
            "episode: 248   score: 1.0   memory length: 45791   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.32\n",
            "episode: 249   score: 2.0   memory length: 45989   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.33\n",
            "episode: 250   score: 0.0   memory length: 46112   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.33\n",
            "episode: 251   score: 1.0   memory length: 46281   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.33\n",
            "episode: 252   score: 1.0   memory length: 46452   epsilon: 1.0    steps: 171    lr: 1e-05     evaluation reward: 1.33\n",
            "episode: 253   score: 3.0   memory length: 46700   epsilon: 1.0    steps: 248    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 254   score: 2.0   memory length: 46898   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.33\n",
            "episode: 255   score: 2.0   memory length: 47096   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.34\n",
            "episode: 256   score: 2.0   memory length: 47293   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 257   score: 0.0   memory length: 47415   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 258   score: 0.0   memory length: 47538   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.34\n",
            "episode: 259   score: 2.0   memory length: 47756   epsilon: 1.0    steps: 218    lr: 1e-05     evaluation reward: 1.34\n",
            "episode: 260   score: 2.0   memory length: 47954   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 261   score: 1.0   memory length: 48123   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.37\n",
            "episode: 262   score: 1.0   memory length: 48291   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 263   score: 12.0   memory length: 48779   epsilon: 1.0    steps: 488    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 264   score: 0.0   memory length: 48902   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 265   score: 0.0   memory length: 49025   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 266   score: 2.0   memory length: 49223   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 267   score: 0.0   memory length: 49346   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 268   score: 0.0   memory length: 49469   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.37\n",
            "episode: 269   score: 2.0   memory length: 49666   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.38\n",
            "episode: 270   score: 4.0   memory length: 49943   epsilon: 1.0    steps: 277    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 271   score: 3.0   memory length: 50188   epsilon: 1.0    steps: 245    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 272   score: 0.0   memory length: 50310   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 273   score: 1.0   memory length: 50461   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 274   score: 0.0   memory length: 50583   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 275   score: 1.0   memory length: 50754   epsilon: 1.0    steps: 171    lr: 1e-05     evaluation reward: 1.38\n",
            "episode: 276   score: 1.0   memory length: 50923   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 277   score: 5.0   memory length: 51259   epsilon: 1.0    steps: 336    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 278   score: 1.0   memory length: 51430   epsilon: 1.0    steps: 171    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 279   score: 0.0   memory length: 51553   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 280   score: 1.0   memory length: 51722   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 281   score: 0.0   memory length: 51844   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 282   score: 2.0   memory length: 52042   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 283   score: 3.0   memory length: 52285   epsilon: 1.0    steps: 243    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 284   score: 2.0   memory length: 52482   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 285   score: 0.0   memory length: 52605   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 286   score: 2.0   memory length: 52803   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 287   score: 1.0   memory length: 52972   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 288   score: 0.0   memory length: 53095   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 289   score: 0.0   memory length: 53218   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.38\n",
            "episode: 290   score: 2.0   memory length: 53435   epsilon: 1.0    steps: 217    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 291   score: 3.0   memory length: 53679   epsilon: 1.0    steps: 244    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 292   score: 0.0   memory length: 53802   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 293   score: 1.0   memory length: 53953   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 294   score: 1.0   memory length: 54125   epsilon: 1.0    steps: 172    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 295   score: 1.0   memory length: 54276   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 296   score: 1.0   memory length: 54447   epsilon: 1.0    steps: 171    lr: 1e-05     evaluation reward: 1.38\n",
            "episode: 297   score: 4.0   memory length: 54763   epsilon: 1.0    steps: 316    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 298   score: 4.0   memory length: 55060   epsilon: 1.0    steps: 297    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 299   score: 0.0   memory length: 55182   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 300   score: 2.0   memory length: 55401   epsilon: 1.0    steps: 219    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 301   score: 0.0   memory length: 55523   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 302   score: 1.0   memory length: 55674   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 303   score: 2.0   memory length: 55872   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 304   score: 0.0   memory length: 55994   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 305   score: 1.0   memory length: 56164   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 306   score: 2.0   memory length: 56361   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 307   score: 2.0   memory length: 56580   epsilon: 1.0    steps: 219    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 308   score: 0.0   memory length: 56703   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 309   score: 2.0   memory length: 56901   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 310   score: 3.0   memory length: 57145   epsilon: 1.0    steps: 244    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 311   score: 0.0   memory length: 57268   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 312   score: 1.0   memory length: 57440   epsilon: 1.0    steps: 172    lr: 1e-05     evaluation reward: 1.38\n",
            "episode: 313   score: 1.0   memory length: 57610   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.37\n",
            "episode: 314   score: 2.0   memory length: 57808   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 315   score: 3.0   memory length: 58054   epsilon: 1.0    steps: 246    lr: 1e-05     evaluation reward: 1.37\n",
            "episode: 316   score: 2.0   memory length: 58252   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.37\n",
            "episode: 317   score: 0.0   memory length: 58375   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 318   score: 0.0   memory length: 58497   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.34\n",
            "episode: 319   score: 2.0   memory length: 58694   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.34\n",
            "episode: 320   score: 2.0   memory length: 58892   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 321   score: 1.0   memory length: 59061   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 322   score: 1.0   memory length: 59230   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 323   score: 2.0   memory length: 59428   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.34\n",
            "episode: 324   score: 2.0   memory length: 59626   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 325   score: 4.0   memory length: 59921   epsilon: 1.0    steps: 295    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 326   score: 0.0   memory length: 60043   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 327   score: 0.0   memory length: 60165   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 328   score: 0.0   memory length: 60288   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.33\n",
            "episode: 329   score: 0.0   memory length: 60411   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.33\n",
            "episode: 330   score: 3.0   memory length: 60657   epsilon: 1.0    steps: 246    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 331   score: 0.0   memory length: 60780   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.33\n",
            "episode: 332   score: 0.0   memory length: 60902   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.31\n",
            "episode: 333   score: 1.0   memory length: 61072   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.32\n",
            "episode: 334   score: 4.0   memory length: 61367   epsilon: 1.0    steps: 295    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 335   score: 1.0   memory length: 61518   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 336   score: 0.0   memory length: 61640   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 337   score: 4.0   memory length: 61916   epsilon: 1.0    steps: 276    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 338   score: 1.0   memory length: 62086   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 339   score: 3.0   memory length: 62351   epsilon: 1.0    steps: 265    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 340   score: 0.0   memory length: 62474   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 341   score: 0.0   memory length: 62596   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 342   score: 1.0   memory length: 62765   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 343   score: 2.0   memory length: 62963   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 344   score: 0.0   memory length: 63085   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 345   score: 3.0   memory length: 63349   epsilon: 1.0    steps: 264    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 346   score: 4.0   memory length: 63645   epsilon: 1.0    steps: 296    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 347   score: 1.0   memory length: 63817   epsilon: 1.0    steps: 172    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 348   score: 2.0   memory length: 64014   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 349   score: 0.0   memory length: 64137   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 350   score: 2.0   memory length: 64354   epsilon: 1.0    steps: 217    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 351   score: 0.0   memory length: 64477   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 352   score: 1.0   memory length: 64647   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 353   score: 1.0   memory length: 64797   epsilon: 1.0    steps: 150    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 354   score: 4.0   memory length: 65087   epsilon: 1.0    steps: 290    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 355   score: 3.0   memory length: 65314   epsilon: 1.0    steps: 227    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 356   score: 1.0   memory length: 65483   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 357   score: 2.0   memory length: 65701   epsilon: 1.0    steps: 218    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 358   score: 1.0   memory length: 65870   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 359   score: 1.0   memory length: 66038   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 360   score: 0.0   memory length: 66160   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 361   score: 2.0   memory length: 66357   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 362   score: 1.0   memory length: 66508   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 363   score: 2.0   memory length: 66708   epsilon: 1.0    steps: 200    lr: 1e-05     evaluation reward: 1.37\n",
            "episode: 364   score: 2.0   memory length: 66926   epsilon: 1.0    steps: 218    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 365   score: 0.0   memory length: 67049   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 366   score: 1.0   memory length: 67220   epsilon: 1.0    steps: 171    lr: 1e-05     evaluation reward: 1.38\n",
            "episode: 367   score: 8.0   memory length: 67700   epsilon: 1.0    steps: 480    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 368   score: 1.0   memory length: 67869   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 369   score: 1.0   memory length: 68040   epsilon: 1.0    steps: 171    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 370   score: 0.0   memory length: 68163   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 371   score: 2.0   memory length: 68360   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 372   score: 2.0   memory length: 68558   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 373   score: 2.0   memory length: 68755   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 374   score: 3.0   memory length: 69022   epsilon: 1.0    steps: 267    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 375   score: 0.0   memory length: 69145   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 376   score: 0.0   memory length: 69267   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 377   score: 2.0   memory length: 69485   epsilon: 1.0    steps: 218    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 378   score: 1.0   memory length: 69635   epsilon: 1.0    steps: 150    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 379   score: 0.0   memory length: 69757   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 380   score: 3.0   memory length: 70004   epsilon: 1.0    steps: 247    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 381   score: 0.0   memory length: 70126   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 382   score: 0.0   memory length: 70249   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 383   score: 1.0   memory length: 70417   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 384   score: 3.0   memory length: 70664   epsilon: 1.0    steps: 247    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 385   score: 5.0   memory length: 71002   epsilon: 1.0    steps: 338    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 386   score: 1.0   memory length: 71153   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 387   score: 1.0   memory length: 71322   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 388   score: 1.0   memory length: 71473   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 389   score: 2.0   memory length: 71672   epsilon: 1.0    steps: 199    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 390   score: 1.0   memory length: 71823   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 391   score: 3.0   memory length: 72070   epsilon: 1.0    steps: 247    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 392   score: 1.0   memory length: 72240   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 393   score: 1.0   memory length: 72391   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 394   score: 1.0   memory length: 72561   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 395   score: 0.0   memory length: 72683   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 396   score: 3.0   memory length: 72927   epsilon: 1.0    steps: 244    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 397   score: 3.0   memory length: 73194   epsilon: 1.0    steps: 267    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 398   score: 0.0   memory length: 73316   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 399   score: 0.0   memory length: 73439   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 400   score: 3.0   memory length: 73706   epsilon: 1.0    steps: 267    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 401   score: 4.0   memory length: 73984   epsilon: 1.0    steps: 278    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 402   score: 0.0   memory length: 74107   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 403   score: 4.0   memory length: 74425   epsilon: 1.0    steps: 318    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 404   score: 2.0   memory length: 74623   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 405   score: 0.0   memory length: 74745   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 406   score: 1.0   memory length: 74915   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 407   score: 0.0   memory length: 75038   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 408   score: 1.0   memory length: 75188   epsilon: 1.0    steps: 150    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 409   score: 0.0   memory length: 75311   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 410   score: 3.0   memory length: 75576   epsilon: 1.0    steps: 265    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 411   score: 4.0   memory length: 75856   epsilon: 1.0    steps: 280    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 412   score: 4.0   memory length: 76152   epsilon: 1.0    steps: 296    lr: 1e-05     evaluation reward: 1.54\n",
            "episode: 413   score: 0.0   memory length: 76275   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 414   score: 1.0   memory length: 76427   epsilon: 1.0    steps: 152    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 415   score: 0.0   memory length: 76550   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 416   score: 0.0   memory length: 76673   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 417   score: 0.0   memory length: 76795   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 418   score: 1.0   memory length: 76946   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 419   score: 3.0   memory length: 77212   epsilon: 1.0    steps: 266    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 420   score: 2.0   memory length: 77412   epsilon: 1.0    steps: 200    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 421   score: 2.0   memory length: 77610   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 422   score: 0.0   memory length: 77732   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 423   score: 2.0   memory length: 77929   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 424   score: 1.0   memory length: 78081   epsilon: 1.0    steps: 152    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 425   score: 0.0   memory length: 78204   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 426   score: 0.0   memory length: 78327   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 427   score: 0.0   memory length: 78450   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 428   score: 2.0   memory length: 78666   epsilon: 1.0    steps: 216    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 429   score: 1.0   memory length: 78836   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 430   score: 2.0   memory length: 79022   epsilon: 1.0    steps: 186    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 431   score: 4.0   memory length: 79288   epsilon: 1.0    steps: 266    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 432   score: 1.0   memory length: 79459   epsilon: 1.0    steps: 171    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 433   score: 1.0   memory length: 79610   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 434   score: 3.0   memory length: 79879   epsilon: 1.0    steps: 269    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 435   score: 2.0   memory length: 80077   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 436   score: 4.0   memory length: 80353   epsilon: 1.0    steps: 276    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 437   score: 5.0   memory length: 80669   epsilon: 1.0    steps: 316    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 438   score: 2.0   memory length: 80867   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 439   score: 1.0   memory length: 81036   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 440   score: 1.0   memory length: 81186   epsilon: 1.0    steps: 150    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 441   score: 0.0   memory length: 81308   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 442   score: 0.0   memory length: 81431   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 443   score: 0.0   memory length: 81553   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 444   score: 0.0   memory length: 81675   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 445   score: 1.0   memory length: 81843   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 446   score: 0.0   memory length: 81966   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 447   score: 1.0   memory length: 82118   epsilon: 1.0    steps: 152    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 448   score: 2.0   memory length: 82318   epsilon: 1.0    steps: 200    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 449   score: 2.0   memory length: 82502   epsilon: 1.0    steps: 184    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 450   score: 3.0   memory length: 82728   epsilon: 1.0    steps: 226    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 451   score: 0.0   memory length: 82850   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 452   score: 1.0   memory length: 83018   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 453   score: 1.0   memory length: 83169   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 454   score: 0.0   memory length: 83291   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 455   score: 1.0   memory length: 83460   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 456   score: 0.0   memory length: 83582   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 457   score: 1.0   memory length: 83753   epsilon: 1.0    steps: 171    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 458   score: 2.0   memory length: 83970   epsilon: 1.0    steps: 217    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 459   score: 3.0   memory length: 84217   epsilon: 1.0    steps: 247    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 460   score: 3.0   memory length: 84463   epsilon: 1.0    steps: 246    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 461   score: 0.0   memory length: 84586   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 462   score: 2.0   memory length: 84806   epsilon: 1.0    steps: 220    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 463   score: 3.0   memory length: 85053   epsilon: 1.0    steps: 247    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 464   score: 1.0   memory length: 85222   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 465   score: 3.0   memory length: 85468   epsilon: 1.0    steps: 246    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 466   score: 2.0   memory length: 85649   epsilon: 1.0    steps: 181    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 467   score: 1.0   memory length: 85800   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 468   score: 0.0   memory length: 85923   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 469   score: 2.0   memory length: 86140   epsilon: 1.0    steps: 217    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 470   score: 5.0   memory length: 86430   epsilon: 1.0    steps: 290    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 471   score: 1.0   memory length: 86598   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 472   score: 1.0   memory length: 86766   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 473   score: 1.0   memory length: 86917   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 474   score: 1.0   memory length: 87068   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 475   score: 1.0   memory length: 87218   epsilon: 1.0    steps: 150    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 476   score: 2.0   memory length: 87416   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 477   score: 0.0   memory length: 87538   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 478   score: 0.0   memory length: 87661   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 479   score: 0.0   memory length: 87784   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 480   score: 0.0   memory length: 87906   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 481   score: 2.0   memory length: 88106   epsilon: 1.0    steps: 200    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 482   score: 3.0   memory length: 88353   epsilon: 1.0    steps: 247    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 483   score: 2.0   memory length: 88551   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 484   score: 3.0   memory length: 88797   epsilon: 1.0    steps: 246    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 485   score: 0.0   memory length: 88920   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 486   score: 4.0   memory length: 89215   epsilon: 1.0    steps: 295    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 487   score: 0.0   memory length: 89337   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 488   score: 2.0   memory length: 89517   epsilon: 1.0    steps: 180    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 489   score: 2.0   memory length: 89735   epsilon: 1.0    steps: 218    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 490   score: 2.0   memory length: 89933   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 491   score: 1.0   memory length: 90102   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 492   score: 2.0   memory length: 90299   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 493   score: 0.0   memory length: 90422   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 494   score: 1.0   memory length: 90591   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 495   score: 0.0   memory length: 90713   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 496   score: 1.0   memory length: 90863   epsilon: 1.0    steps: 150    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 497   score: 3.0   memory length: 91112   epsilon: 1.0    steps: 249    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 498   score: 1.0   memory length: 91282   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 499   score: 1.0   memory length: 91451   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 500   score: 2.0   memory length: 91668   epsilon: 1.0    steps: 217    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 501   score: 1.0   memory length: 91819   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 502   score: 0.0   memory length: 91942   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 503   score: 2.0   memory length: 92158   epsilon: 1.0    steps: 216    lr: 1e-05     evaluation reward: 1.38\n",
            "episode: 504   score: 1.0   memory length: 92328   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.37\n",
            "episode: 505   score: 2.0   memory length: 92525   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 506   score: 2.0   memory length: 92723   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 507   score: 1.0   memory length: 92892   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 508   score: 3.0   memory length: 93140   epsilon: 1.0    steps: 248    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 509   score: 1.0   memory length: 93308   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 510   score: 0.0   memory length: 93431   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 511   score: 2.0   memory length: 93631   epsilon: 1.0    steps: 200    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 512   score: 0.0   memory length: 93753   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 513   score: 1.0   memory length: 93923   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 514   score: 1.0   memory length: 94092   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 515   score: 0.0   memory length: 94215   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 516   score: 3.0   memory length: 94442   epsilon: 1.0    steps: 227    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 517   score: 1.0   memory length: 94593   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 518   score: 0.0   memory length: 94716   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 519   score: 2.0   memory length: 94897   epsilon: 1.0    steps: 181    lr: 1e-05     evaluation reward: 1.38\n",
            "episode: 520   score: 0.0   memory length: 95019   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 521   score: 0.0   memory length: 95142   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.34\n",
            "episode: 522   score: 1.0   memory length: 95312   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 523   score: 2.0   memory length: 95509   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 524   score: 1.0   memory length: 95679   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 525   score: 1.0   memory length: 95830   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 526   score: 1.0   memory length: 95981   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.37\n",
            "episode: 527   score: 4.0   memory length: 96277   epsilon: 1.0    steps: 296    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 528   score: 0.0   memory length: 96399   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 529   score: 2.0   memory length: 96598   epsilon: 1.0    steps: 199    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 530   score: 2.0   memory length: 96795   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 531   score: 2.0   memory length: 97015   epsilon: 1.0    steps: 220    lr: 1e-05     evaluation reward: 1.38\n",
            "episode: 532   score: 2.0   memory length: 97235   epsilon: 1.0    steps: 220    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 533   score: 4.0   memory length: 97510   epsilon: 1.0    steps: 275    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 534   score: 4.0   memory length: 97800   epsilon: 1.0    steps: 290    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 535   score: 2.0   memory length: 98000   epsilon: 1.0    steps: 200    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 536   score: 1.0   memory length: 98151   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 537   score: 3.0   memory length: 98397   epsilon: 1.0    steps: 246    lr: 1e-05     evaluation reward: 1.38\n",
            "episode: 538   score: 0.0   memory length: 98520   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 539   score: 0.0   memory length: 98643   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 540   score: 0.0   memory length: 98766   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.34\n",
            "episode: 541   score: 3.0   memory length: 99012   epsilon: 1.0    steps: 246    lr: 1e-05     evaluation reward: 1.37\n",
            "episode: 542   score: 3.0   memory length: 99242   epsilon: 1.0    steps: 230    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 543   score: 0.0   memory length: 99364   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 544   score: 4.0   memory length: 99624   epsilon: 1.0    steps: 260    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 545   score: 2.0   memory length: 99821   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 546   score: 1.0   memory length: 99972   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 547   score: 2.0   memory length: 100188   epsilon: 0.9996257800000081    steps: 216    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 548   score: 0.0   memory length: 100311   epsilon: 0.9993822400000134    steps: 123    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 549   score: 0.0   memory length: 100433   epsilon: 0.9991406800000187    steps: 122    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 550   score: 2.0   memory length: 100631   epsilon: 0.9987486400000272    steps: 198    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 551   score: 0.0   memory length: 100754   epsilon: 0.9985051000000325    steps: 123    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 552   score: 2.0   memory length: 100972   epsilon: 0.9980734600000418    steps: 218    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 553   score: 2.0   memory length: 101169   epsilon: 0.9976834000000503    steps: 197    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 554   score: 2.0   memory length: 101388   epsilon: 0.9972497800000597    steps: 219    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 555   score: 0.0   memory length: 101511   epsilon: 0.997006240000065    steps: 123    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 556   score: 1.0   memory length: 101680   epsilon: 0.9966716200000723    steps: 169    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 557   score: 2.0   memory length: 101901   epsilon: 0.9962340400000818    steps: 221    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 558   score: 0.0   memory length: 102023   epsilon: 0.995992480000087    steps: 122    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 559   score: 1.0   memory length: 102192   epsilon: 0.9956578600000943    steps: 169    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 560   score: 4.0   memory length: 102469   epsilon: 0.9951094000001062    steps: 277    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 561   score: 0.0   memory length: 102592   epsilon: 0.9948658600001115    steps: 123    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 562   score: 1.0   memory length: 102742   epsilon: 0.9945688600001179    steps: 150    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 563   score: 1.0   memory length: 102894   epsilon: 0.9942679000001244    steps: 152    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 564   score: 2.0   memory length: 103092   epsilon: 0.993875860000133    steps: 198    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 565   score: 2.0   memory length: 103308   epsilon: 0.9934481800001422    steps: 216    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 566   score: 2.0   memory length: 103505   epsilon: 0.9930581200001507    steps: 197    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 567   score: 6.0   memory length: 103870   epsilon: 0.9923354200001664    steps: 365    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 568   score: 0.0   memory length: 103993   epsilon: 0.9920918800001717    steps: 123    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 569   score: 2.0   memory length: 104191   epsilon: 0.9916998400001802    steps: 198    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 570   score: 3.0   memory length: 104419   epsilon: 0.99124840000019    steps: 228    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 571   score: 1.0   memory length: 104588   epsilon: 0.9909137800001973    steps: 169    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 572   score: 0.0   memory length: 104711   epsilon: 0.9906702400002025    steps: 123    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 573   score: 0.0   memory length: 104834   epsilon: 0.9904267000002078    steps: 123    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 574   score: 2.0   memory length: 105051   epsilon: 0.9899970400002172    steps: 217    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 575   score: 1.0   memory length: 105221   epsilon: 0.9896604400002245    steps: 170    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 576   score: 1.0   memory length: 105390   epsilon: 0.9893258200002317    steps: 169    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 577   score: 0.0   memory length: 105513   epsilon: 0.989082280000237    steps: 123    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 578   score: 2.0   memory length: 105729   epsilon: 0.9886546000002463    steps: 216    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 579   score: 0.0   memory length: 105851   epsilon: 0.9884130400002515    steps: 122    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 580   score: 1.0   memory length: 106022   epsilon: 0.9880744600002589    steps: 171    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 581   score: 1.0   memory length: 106191   epsilon: 0.9877398400002662    steps: 169    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 582   score: 2.0   memory length: 106409   epsilon: 0.9873082000002755    steps: 218    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 583   score: 1.0   memory length: 106560   epsilon: 0.987009220000282    steps: 151    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 584   score: 6.0   memory length: 106917   epsilon: 0.9863023600002974    steps: 357    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 585   score: 0.0   memory length: 107039   epsilon: 0.9860608000003026    steps: 122    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 586   score: 2.0   memory length: 107257   epsilon: 0.985629160000312    steps: 218    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 587   score: 2.0   memory length: 107455   epsilon: 0.9852371200003205    steps: 198    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 588   score: 2.0   memory length: 107671   epsilon: 0.9848094400003298    steps: 216    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 589   score: 0.0   memory length: 107794   epsilon: 0.9845659000003351    steps: 123    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 590   score: 3.0   memory length: 108040   epsilon: 0.9840788200003456    steps: 246    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 591   score: 0.0   memory length: 108163   epsilon: 0.9838352800003509    steps: 123    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 592   score: 2.0   memory length: 108383   epsilon: 0.9833996800003604    steps: 220    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 593   score: 2.0   memory length: 108581   epsilon: 0.9830076400003689    steps: 198    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 594   score: 1.0   memory length: 108750   epsilon: 0.9826730200003762    steps: 169    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 595   score: 2.0   memory length: 108947   epsilon: 0.9822829600003846    steps: 197    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 596   score: 1.0   memory length: 109115   epsilon: 0.9819503200003918    steps: 168    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 597   score: 3.0   memory length: 109363   epsilon: 0.9814592800004025    steps: 248    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 598   score: 1.0   memory length: 109533   epsilon: 0.9811226800004098    steps: 170    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 599   score: 2.0   memory length: 109731   epsilon: 0.9807306400004183    steps: 198    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 600   score: 0.0   memory length: 109854   epsilon: 0.9804871000004236    steps: 123    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 601   score: 0.0   memory length: 109977   epsilon: 0.9802435600004289    steps: 123    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 602   score: 2.0   memory length: 110195   epsilon: 0.9798119200004383    steps: 218    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 603   score: 1.0   memory length: 110346   epsilon: 0.9795129400004448    steps: 151    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 604   score: 0.0   memory length: 110468   epsilon: 0.97927138000045    steps: 122    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 605   score: 3.0   memory length: 110693   epsilon: 0.9788258800004597    steps: 225    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 606   score: 0.0   memory length: 110816   epsilon: 0.978582340000465    steps: 123    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 607   score: 0.0   memory length: 110938   epsilon: 0.9783407800004702    steps: 122    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 608   score: 3.0   memory length: 111184   epsilon: 0.9778537000004808    steps: 246    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 609   score: 0.0   memory length: 111307   epsilon: 0.9776101600004861    steps: 123    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 610   score: 2.0   memory length: 111504   epsilon: 0.9772201000004945    steps: 197    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 611   score: 6.0   memory length: 111843   epsilon: 0.9765488800005091    steps: 339    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 612   score: 3.0   memory length: 112071   epsilon: 0.9760974400005189    steps: 228    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 613   score: 3.0   memory length: 112317   epsilon: 0.9756103600005295    steps: 246    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 614   score: 3.0   memory length: 112566   epsilon: 0.9751173400005402    steps: 249    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 615   score: 2.0   memory length: 112763   epsilon: 0.9747272800005486    steps: 197    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 616   score: 1.0   memory length: 112915   epsilon: 0.9744263200005552    steps: 152    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 617   score: 2.0   memory length: 113113   epsilon: 0.9740342800005637    steps: 198    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 618   score: 3.0   memory length: 113362   epsilon: 0.9735412600005744    steps: 249    lr: 1e-05     evaluation reward: 1.59\n",
            "episode: 619   score: 3.0   memory length: 113608   epsilon: 0.973054180000585    steps: 246    lr: 1e-05     evaluation reward: 1.6\n",
            "episode: 620   score: 2.0   memory length: 113809   epsilon: 0.9726562000005936    steps: 201    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 621   score: 2.0   memory length: 114007   epsilon: 0.9722641600006021    steps: 198    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 622   score: 1.0   memory length: 114158   epsilon: 0.9719651800006086    steps: 151    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 623   score: 0.0   memory length: 114281   epsilon: 0.9717216400006139    steps: 123    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 624   score: 2.0   memory length: 114478   epsilon: 0.9713315800006224    steps: 197    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 625   score: 4.0   memory length: 114775   epsilon: 0.9707435200006351    steps: 297    lr: 1e-05     evaluation reward: 1.66\n",
            "episode: 626   score: 2.0   memory length: 114993   epsilon: 0.9703118800006445    steps: 218    lr: 1e-05     evaluation reward: 1.67\n",
            "episode: 627   score: 1.0   memory length: 115163   epsilon: 0.9699752800006518    steps: 170    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 628   score: 0.0   memory length: 115286   epsilon: 0.9697317400006571    steps: 123    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 629   score: 0.0   memory length: 115409   epsilon: 0.9694882000006624    steps: 123    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 630   score: 2.0   memory length: 115627   epsilon: 0.9690565600006718    steps: 218    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 631   score: 4.0   memory length: 115920   epsilon: 0.9684764200006843    steps: 293    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 632   score: 1.0   memory length: 116088   epsilon: 0.9681437800006916    steps: 168    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 633   score: 0.0   memory length: 116211   epsilon: 0.9679002400006969    steps: 123    lr: 1e-05     evaluation reward: 1.59\n",
            "episode: 634   score: 2.0   memory length: 116426   epsilon: 0.9674745400007061    steps: 215    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 635   score: 2.0   memory length: 116641   epsilon: 0.9670488400007153    steps: 215    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 636   score: 0.0   memory length: 116764   epsilon: 0.9668053000007206    steps: 123    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 637   score: 0.0   memory length: 116887   epsilon: 0.9665617600007259    steps: 123    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 638   score: 3.0   memory length: 117113   epsilon: 0.9661142800007356    steps: 226    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 639   score: 4.0   memory length: 117410   epsilon: 0.9655262200007484    steps: 297    lr: 1e-05     evaluation reward: 1.6\n",
            "episode: 640   score: 2.0   memory length: 117629   epsilon: 0.9650926000007578    steps: 219    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 641   score: 3.0   memory length: 117857   epsilon: 0.9646411600007676    steps: 228    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 642   score: 2.0   memory length: 118054   epsilon: 0.9642511000007761    steps: 197    lr: 1e-05     evaluation reward: 1.61\n",
            "episode: 643   score: 3.0   memory length: 118280   epsilon: 0.9638036200007858    steps: 226    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 644   score: 0.0   memory length: 118403   epsilon: 0.9635600800007911    steps: 123    lr: 1e-05     evaluation reward: 1.6\n",
            "episode: 645   score: 0.0   memory length: 118526   epsilon: 0.9633165400007964    steps: 123    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 646   score: 1.0   memory length: 118677   epsilon: 0.9630175600008029    steps: 151    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 647   score: 2.0   memory length: 118895   epsilon: 0.9625859200008122    steps: 218    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 648   score: 2.0   memory length: 119116   epsilon: 0.9621483400008217    steps: 221    lr: 1e-05     evaluation reward: 1.6\n",
            "episode: 649   score: 2.0   memory length: 119314   epsilon: 0.9617563000008302    steps: 198    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 650   score: 1.0   memory length: 119483   epsilon: 0.9614216800008375    steps: 169    lr: 1e-05     evaluation reward: 1.61\n",
            "episode: 651   score: 0.0   memory length: 119605   epsilon: 0.9611801200008427    steps: 122    lr: 1e-05     evaluation reward: 1.61\n",
            "episode: 652   score: 0.0   memory length: 119727   epsilon: 0.960938560000848    steps: 122    lr: 1e-05     evaluation reward: 1.59\n",
            "episode: 653   score: 2.0   memory length: 119944   epsilon: 0.9605089000008573    steps: 217    lr: 1e-05     evaluation reward: 1.59\n",
            "episode: 654   score: 0.0   memory length: 120067   epsilon: 0.9602653600008626    steps: 123    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 655   score: 2.0   memory length: 120285   epsilon: 0.959833720000872    steps: 218    lr: 1e-05     evaluation reward: 1.59\n",
            "episode: 656   score: 3.0   memory length: 120532   epsilon: 0.9593446600008826    steps: 247    lr: 1e-05     evaluation reward: 1.61\n",
            "episode: 657   score: 3.0   memory length: 120760   epsilon: 0.9588932200008924    steps: 228    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 658   score: 1.0   memory length: 120911   epsilon: 0.9585942400008989    steps: 151    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 659   score: 0.0   memory length: 121034   epsilon: 0.9583507000009042    steps: 123    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 660   score: 2.0   memory length: 121252   epsilon: 0.9579190600009135    steps: 218    lr: 1e-05     evaluation reward: 1.6\n",
            "episode: 661   score: 2.0   memory length: 121472   epsilon: 0.957483460000923    steps: 220    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 662   score: 3.0   memory length: 121701   epsilon: 0.9570300400009328    steps: 229    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 663   score: 2.0   memory length: 121881   epsilon: 0.9566736400009406    steps: 180    lr: 1e-05     evaluation reward: 1.65\n",
            "episode: 664   score: 4.0   memory length: 122157   epsilon: 0.9561271600009524    steps: 276    lr: 1e-05     evaluation reward: 1.67\n",
            "episode: 665   score: 2.0   memory length: 122377   epsilon: 0.9556915600009619    steps: 220    lr: 1e-05     evaluation reward: 1.67\n",
            "episode: 666   score: 2.0   memory length: 122596   epsilon: 0.9552579400009713    steps: 219    lr: 1e-05     evaluation reward: 1.67\n",
            "episode: 667   score: 1.0   memory length: 122764   epsilon: 0.9549253000009785    steps: 168    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 668   score: 3.0   memory length: 123010   epsilon: 0.9544382200009891    steps: 246    lr: 1e-05     evaluation reward: 1.65\n",
            "episode: 669   score: 1.0   memory length: 123160   epsilon: 0.9541412200009955    steps: 150    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 670   score: 1.0   memory length: 123329   epsilon: 0.9538066000010028    steps: 169    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 671   score: 3.0   memory length: 123597   epsilon: 0.9532759600010143    steps: 268    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 672   score: 1.0   memory length: 123748   epsilon: 0.9529769800010208    steps: 151    lr: 1e-05     evaluation reward: 1.65\n",
            "episode: 673   score: 3.0   memory length: 123976   epsilon: 0.9525255400010306    steps: 228    lr: 1e-05     evaluation reward: 1.68\n",
            "episode: 674   score: 5.0   memory length: 124290   epsilon: 0.9519038200010441    steps: 314    lr: 1e-05     evaluation reward: 1.71\n",
            "episode: 675   score: 5.0   memory length: 124635   epsilon: 0.951220720001059    steps: 345    lr: 1e-05     evaluation reward: 1.75\n",
            "episode: 676   score: 3.0   memory length: 124903   epsilon: 0.9506900800010705    steps: 268    lr: 1e-05     evaluation reward: 1.77\n",
            "episode: 677   score: 0.0   memory length: 125026   epsilon: 0.9504465400010758    steps: 123    lr: 1e-05     evaluation reward: 1.77\n",
            "episode: 678   score: 3.0   memory length: 125273   epsilon: 0.9499574800010864    steps: 247    lr: 1e-05     evaluation reward: 1.78\n",
            "episode: 679   score: 1.0   memory length: 125441   epsilon: 0.9496248400010936    steps: 168    lr: 1e-05     evaluation reward: 1.79\n",
            "episode: 680   score: 0.0   memory length: 125564   epsilon: 0.9493813000010989    steps: 123    lr: 1e-05     evaluation reward: 1.78\n",
            "episode: 681   score: 1.0   memory length: 125715   epsilon: 0.9490823200011054    steps: 151    lr: 1e-05     evaluation reward: 1.78\n",
            "episode: 682   score: 2.0   memory length: 125896   epsilon: 0.9487239400011132    steps: 181    lr: 1e-05     evaluation reward: 1.78\n",
            "episode: 683   score: 4.0   memory length: 126211   epsilon: 0.9481002400011267    steps: 315    lr: 1e-05     evaluation reward: 1.81\n",
            "episode: 684   score: 0.0   memory length: 126334   epsilon: 0.947856700001132    steps: 123    lr: 1e-05     evaluation reward: 1.75\n",
            "episode: 685   score: 5.0   memory length: 126661   epsilon: 0.947209240001146    steps: 327    lr: 1e-05     evaluation reward: 1.8\n",
            "episode: 686   score: 4.0   memory length: 126954   epsilon: 0.9466291000011586    steps: 293    lr: 1e-05     evaluation reward: 1.82\n",
            "episode: 687   score: 0.0   memory length: 127077   epsilon: 0.9463855600011639    steps: 123    lr: 1e-05     evaluation reward: 1.8\n",
            "episode: 688   score: 3.0   memory length: 127323   epsilon: 0.9458984800011745    steps: 246    lr: 1e-05     evaluation reward: 1.81\n",
            "episode: 689   score: 3.0   memory length: 127590   epsilon: 0.945369820001186    steps: 267    lr: 1e-05     evaluation reward: 1.84\n",
            "episode: 690   score: 3.0   memory length: 127855   epsilon: 0.9448451200011974    steps: 265    lr: 1e-05     evaluation reward: 1.84\n",
            "episode: 691   score: 4.0   memory length: 128152   epsilon: 0.9442570600012101    steps: 297    lr: 1e-05     evaluation reward: 1.88\n",
            "episode: 692   score: 0.0   memory length: 128274   epsilon: 0.9440155000012154    steps: 122    lr: 1e-05     evaluation reward: 1.86\n",
            "episode: 693   score: 0.0   memory length: 128397   epsilon: 0.9437719600012207    steps: 123    lr: 1e-05     evaluation reward: 1.84\n",
            "episode: 694   score: 0.0   memory length: 128520   epsilon: 0.9435284200012259    steps: 123    lr: 1e-05     evaluation reward: 1.83\n",
            "episode: 695   score: 4.0   memory length: 128795   epsilon: 0.9429839200012378    steps: 275    lr: 1e-05     evaluation reward: 1.85\n",
            "episode: 696   score: 1.0   memory length: 128963   epsilon: 0.942651280001245    steps: 168    lr: 1e-05     evaluation reward: 1.85\n",
            "episode: 697   score: 2.0   memory length: 129161   epsilon: 0.9422592400012535    steps: 198    lr: 1e-05     evaluation reward: 1.84\n",
            "episode: 698   score: 5.0   memory length: 129469   epsilon: 0.9416494000012667    steps: 308    lr: 1e-05     evaluation reward: 1.88\n",
            "episode: 699   score: 1.0   memory length: 129640   epsilon: 0.9413108200012741    steps: 171    lr: 1e-05     evaluation reward: 1.87\n",
            "episode: 700   score: 2.0   memory length: 129839   epsilon: 0.9409168000012826    steps: 199    lr: 1e-05     evaluation reward: 1.89\n",
            "episode: 701   score: 1.0   memory length: 130008   epsilon: 0.9405821800012899    steps: 169    lr: 1e-05     evaluation reward: 1.9\n",
            "episode: 702   score: 0.0   memory length: 130131   epsilon: 0.9403386400012952    steps: 123    lr: 1e-05     evaluation reward: 1.88\n",
            "episode: 703   score: 1.0   memory length: 130282   epsilon: 0.9400396600013017    steps: 151    lr: 1e-05     evaluation reward: 1.88\n",
            "episode: 704   score: 0.0   memory length: 130405   epsilon: 0.939796120001307    steps: 123    lr: 1e-05     evaluation reward: 1.88\n",
            "episode: 705   score: 2.0   memory length: 130603   epsilon: 0.9394040800013155    steps: 198    lr: 1e-05     evaluation reward: 1.87\n",
            "episode: 706   score: 0.0   memory length: 130725   epsilon: 0.9391625200013207    steps: 122    lr: 1e-05     evaluation reward: 1.87\n",
            "episode: 707   score: 2.0   memory length: 130945   epsilon: 0.9387269200013302    steps: 220    lr: 1e-05     evaluation reward: 1.89\n",
            "episode: 708   score: 3.0   memory length: 131190   epsilon: 0.9382418200013407    steps: 245    lr: 1e-05     evaluation reward: 1.89\n",
            "episode: 709   score: 0.0   memory length: 131313   epsilon: 0.937998280001346    steps: 123    lr: 1e-05     evaluation reward: 1.89\n",
            "episode: 710   score: 0.0   memory length: 131435   epsilon: 0.9377567200013512    steps: 122    lr: 1e-05     evaluation reward: 1.87\n",
            "episode: 711   score: 2.0   memory length: 131633   epsilon: 0.9373646800013598    steps: 198    lr: 1e-05     evaluation reward: 1.83\n",
            "episode: 712   score: 1.0   memory length: 131804   epsilon: 0.9370261000013671    steps: 171    lr: 1e-05     evaluation reward: 1.81\n",
            "episode: 713   score: 2.0   memory length: 132001   epsilon: 0.9366360400013756    steps: 197    lr: 1e-05     evaluation reward: 1.8\n",
            "episode: 714   score: 0.0   memory length: 132123   epsilon: 0.9363944800013808    steps: 122    lr: 1e-05     evaluation reward: 1.77\n",
            "episode: 715   score: 0.0   memory length: 132246   epsilon: 0.9361509400013861    steps: 123    lr: 1e-05     evaluation reward: 1.75\n",
            "episode: 716   score: 0.0   memory length: 132369   epsilon: 0.9359074000013914    steps: 123    lr: 1e-05     evaluation reward: 1.74\n",
            "episode: 717   score: 1.0   memory length: 132538   epsilon: 0.9355727800013987    steps: 169    lr: 1e-05     evaluation reward: 1.73\n",
            "episode: 718   score: 2.0   memory length: 132756   epsilon: 0.935141140001408    steps: 218    lr: 1e-05     evaluation reward: 1.72\n",
            "episode: 719   score: 3.0   memory length: 133004   epsilon: 0.9346501000014187    steps: 248    lr: 1e-05     evaluation reward: 1.72\n",
            "episode: 720   score: 1.0   memory length: 133173   epsilon: 0.934315480001426    steps: 169    lr: 1e-05     evaluation reward: 1.71\n",
            "episode: 721   score: 3.0   memory length: 133438   epsilon: 0.9337907800014373    steps: 265    lr: 1e-05     evaluation reward: 1.72\n",
            "episode: 722   score: 0.0   memory length: 133561   epsilon: 0.9335472400014426    steps: 123    lr: 1e-05     evaluation reward: 1.71\n",
            "episode: 723   score: 2.0   memory length: 133758   epsilon: 0.9331571800014511    steps: 197    lr: 1e-05     evaluation reward: 1.73\n",
            "episode: 724   score: 0.0   memory length: 133880   epsilon: 0.9329156200014563    steps: 122    lr: 1e-05     evaluation reward: 1.71\n",
            "episode: 725   score: 3.0   memory length: 134127   epsilon: 0.932426560001467    steps: 247    lr: 1e-05     evaluation reward: 1.7\n",
            "episode: 726   score: 3.0   memory length: 134398   epsilon: 0.9318899800014786    steps: 271    lr: 1e-05     evaluation reward: 1.71\n",
            "episode: 727   score: 4.0   memory length: 134660   epsilon: 0.9313712200014899    steps: 262    lr: 1e-05     evaluation reward: 1.74\n",
            "episode: 728   score: 1.0   memory length: 134830   epsilon: 0.9310346200014972    steps: 170    lr: 1e-05     evaluation reward: 1.75\n",
            "episode: 729   score: 0.0   memory length: 134952   epsilon: 0.9307930600015024    steps: 122    lr: 1e-05     evaluation reward: 1.75\n",
            "episode: 730   score: 7.0   memory length: 135388   epsilon: 0.9299297800015212    steps: 436    lr: 1e-05     evaluation reward: 1.8\n",
            "episode: 731   score: 4.0   memory length: 135667   epsilon: 0.9293773600015331    steps: 279    lr: 1e-05     evaluation reward: 1.8\n",
            "episode: 732   score: 2.0   memory length: 135866   epsilon: 0.9289833400015417    steps: 199    lr: 1e-05     evaluation reward: 1.81\n",
            "episode: 733   score: 1.0   memory length: 136017   epsilon: 0.9286843600015482    steps: 151    lr: 1e-05     evaluation reward: 1.82\n",
            "episode: 734   score: 1.0   memory length: 136170   epsilon: 0.9283814200015548    steps: 153    lr: 1e-05     evaluation reward: 1.81\n",
            "episode: 735   score: 0.0   memory length: 136293   epsilon: 0.9281378800015601    steps: 123    lr: 1e-05     evaluation reward: 1.79\n",
            "episode: 736   score: 2.0   memory length: 136510   epsilon: 0.9277082200015694    steps: 217    lr: 1e-05     evaluation reward: 1.81\n",
            "episode: 737   score: 0.0   memory length: 136633   epsilon: 0.9274646800015747    steps: 123    lr: 1e-05     evaluation reward: 1.81\n",
            "episode: 738   score: 2.0   memory length: 136831   epsilon: 0.9270726400015832    steps: 198    lr: 1e-05     evaluation reward: 1.8\n",
            "episode: 739   score: 2.0   memory length: 137047   epsilon: 0.9266449600015925    steps: 216    lr: 1e-05     evaluation reward: 1.78\n",
            "episode: 740   score: 2.0   memory length: 137245   epsilon: 0.926252920001601    steps: 198    lr: 1e-05     evaluation reward: 1.78\n",
            "episode: 741   score: 2.0   memory length: 137443   epsilon: 0.9258608800016095    steps: 198    lr: 1e-05     evaluation reward: 1.77\n",
            "episode: 742   score: 2.0   memory length: 137658   epsilon: 0.9254351800016187    steps: 215    lr: 1e-05     evaluation reward: 1.77\n",
            "episode: 743   score: 5.0   memory length: 138003   epsilon: 0.9247520800016336    steps: 345    lr: 1e-05     evaluation reward: 1.79\n",
            "episode: 744   score: 1.0   memory length: 138172   epsilon: 0.9244174600016408    steps: 169    lr: 1e-05     evaluation reward: 1.8\n",
            "episode: 745   score: 1.0   memory length: 138341   epsilon: 0.9240828400016481    steps: 169    lr: 1e-05     evaluation reward: 1.81\n",
            "episode: 746   score: 0.0   memory length: 138464   epsilon: 0.9238393000016534    steps: 123    lr: 1e-05     evaluation reward: 1.8\n",
            "episode: 747   score: 0.0   memory length: 138586   epsilon: 0.9235977400016586    steps: 122    lr: 1e-05     evaluation reward: 1.78\n",
            "episode: 748   score: 1.0   memory length: 138758   epsilon: 0.923257180001666    steps: 172    lr: 1e-05     evaluation reward: 1.77\n",
            "episode: 749   score: 2.0   memory length: 138975   epsilon: 0.9228275200016753    steps: 217    lr: 1e-05     evaluation reward: 1.77\n",
            "episode: 750   score: 3.0   memory length: 139203   epsilon: 0.9223760800016851    steps: 228    lr: 1e-05     evaluation reward: 1.79\n",
            "episode: 751   score: 2.0   memory length: 139401   epsilon: 0.9219840400016936    steps: 198    lr: 1e-05     evaluation reward: 1.81\n",
            "episode: 752   score: 2.0   memory length: 139618   epsilon: 0.921554380001703    steps: 217    lr: 1e-05     evaluation reward: 1.83\n",
            "episode: 753   score: 1.0   memory length: 139771   epsilon: 0.9212514400017096    steps: 153    lr: 1e-05     evaluation reward: 1.82\n",
            "episode: 754   score: 0.0   memory length: 139894   epsilon: 0.9210079000017148    steps: 123    lr: 1e-05     evaluation reward: 1.82\n",
            "episode: 755   score: 0.0   memory length: 140017   epsilon: 0.9207643600017201    steps: 123    lr: 1e-05     evaluation reward: 1.8\n",
            "episode: 756   score: 2.0   memory length: 140235   epsilon: 0.9203327200017295    steps: 218    lr: 1e-05     evaluation reward: 1.79\n",
            "episode: 757   score: 2.0   memory length: 140432   epsilon: 0.919942660001738    steps: 197    lr: 1e-05     evaluation reward: 1.78\n",
            "episode: 758   score: 3.0   memory length: 140677   epsilon: 0.9194575600017485    steps: 245    lr: 1e-05     evaluation reward: 1.8\n",
            "episode: 759   score: 1.0   memory length: 140827   epsilon: 0.919160560001755    steps: 150    lr: 1e-05     evaluation reward: 1.81\n",
            "episode: 760   score: 3.0   memory length: 141073   epsilon: 0.9186734800017655    steps: 246    lr: 1e-05     evaluation reward: 1.82\n",
            "episode: 761   score: 2.0   memory length: 141292   epsilon: 0.9182398600017749    steps: 219    lr: 1e-05     evaluation reward: 1.82\n",
            "episode: 762   score: 2.0   memory length: 141490   epsilon: 0.9178478200017834    steps: 198    lr: 1e-05     evaluation reward: 1.81\n",
            "episode: 763   score: 3.0   memory length: 141759   epsilon: 0.917315200001795    steps: 269    lr: 1e-05     evaluation reward: 1.82\n",
            "episode: 764   score: 1.0   memory length: 141928   epsilon: 0.9169805800018023    steps: 169    lr: 1e-05     evaluation reward: 1.79\n",
            "episode: 765   score: 2.0   memory length: 142130   epsilon: 0.916580620001811    steps: 202    lr: 1e-05     evaluation reward: 1.79\n",
            "episode: 766   score: 2.0   memory length: 142347   epsilon: 0.9161509600018203    steps: 217    lr: 1e-05     evaluation reward: 1.79\n",
            "episode: 767   score: 1.0   memory length: 142519   epsilon: 0.9158104000018277    steps: 172    lr: 1e-05     evaluation reward: 1.79\n",
            "episode: 768   score: 0.0   memory length: 142641   epsilon: 0.9155688400018329    steps: 122    lr: 1e-05     evaluation reward: 1.76\n",
            "episode: 769   score: 2.0   memory length: 142859   epsilon: 0.9151372000018423    steps: 218    lr: 1e-05     evaluation reward: 1.77\n",
            "episode: 770   score: 0.0   memory length: 142982   epsilon: 0.9148936600018476    steps: 123    lr: 1e-05     evaluation reward: 1.76\n",
            "episode: 771   score: 0.0   memory length: 143104   epsilon: 0.9146521000018528    steps: 122    lr: 1e-05     evaluation reward: 1.73\n",
            "episode: 772   score: 0.0   memory length: 143227   epsilon: 0.9144085600018581    steps: 123    lr: 1e-05     evaluation reward: 1.72\n",
            "episode: 773   score: 0.0   memory length: 143349   epsilon: 0.9141670000018634    steps: 122    lr: 1e-05     evaluation reward: 1.69\n",
            "episode: 774   score: 2.0   memory length: 143550   epsilon: 0.913769020001872    steps: 201    lr: 1e-05     evaluation reward: 1.66\n",
            "episode: 775   score: 3.0   memory length: 143811   epsilon: 0.9132522400018832    steps: 261    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 776   score: 2.0   memory length: 144009   epsilon: 0.9128602000018917    steps: 198    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 777   score: 1.0   memory length: 144180   epsilon: 0.9125216200018991    steps: 171    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 778   score: 2.0   memory length: 144377   epsilon: 0.9121315600019075    steps: 197    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 779   score: 1.0   memory length: 144545   epsilon: 0.9117989200019148    steps: 168    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 780   score: 0.0   memory length: 144668   epsilon: 0.91155538000192    steps: 123    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 781   score: 1.0   memory length: 144837   epsilon: 0.9112207600019273    steps: 169    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 782   score: 2.0   memory length: 145055   epsilon: 0.9107891200019367    steps: 218    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 783   score: 1.0   memory length: 145206   epsilon: 0.9104901400019432    steps: 151    lr: 1e-05     evaluation reward: 1.6\n",
            "episode: 784   score: 1.0   memory length: 145375   epsilon: 0.9101555200019504    steps: 169    lr: 1e-05     evaluation reward: 1.61\n",
            "episode: 785   score: 2.0   memory length: 145557   epsilon: 0.9097951600019583    steps: 182    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 786   score: 1.0   memory length: 145707   epsilon: 0.9094981600019647    steps: 150    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 787   score: 2.0   memory length: 145904   epsilon: 0.9091081000019732    steps: 197    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 788   score: 0.0   memory length: 146026   epsilon: 0.9088665400019784    steps: 122    lr: 1e-05     evaluation reward: 1.54\n",
            "episode: 789   score: 1.0   memory length: 146177   epsilon: 0.9085675600019849    steps: 151    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 790   score: 2.0   memory length: 146375   epsilon: 0.9081755200019934    steps: 198    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 791   score: 2.0   memory length: 146596   epsilon: 0.9077379400020029    steps: 221    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 792   score: 1.0   memory length: 146766   epsilon: 0.9074013400020102    steps: 170    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 793   score: 2.0   memory length: 146964   epsilon: 0.9070093000020187    steps: 198    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 794   score: 0.0   memory length: 147086   epsilon: 0.906767740002024    steps: 122    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 795   score: 2.0   memory length: 147306   epsilon: 0.9063321400020334    steps: 220    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 796   score: 1.0   memory length: 147477   epsilon: 0.9059935600020408    steps: 171    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 797   score: 0.0   memory length: 147600   epsilon: 0.9057500200020461    steps: 123    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 798   score: 3.0   memory length: 147848   epsilon: 0.9052589800020567    steps: 248    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 799   score: 1.0   memory length: 148019   epsilon: 0.9049204000020641    steps: 171    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 800   score: 3.0   memory length: 148248   epsilon: 0.9044669800020739    steps: 229    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 801   score: 1.0   memory length: 148417   epsilon: 0.9041323600020812    steps: 169    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 802   score: 0.0   memory length: 148539   epsilon: 0.9038908000020864    steps: 122    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 803   score: 0.0   memory length: 148661   epsilon: 0.9036492400020917    steps: 122    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 804   score: 3.0   memory length: 148905   epsilon: 0.9031661200021022    steps: 244    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 805   score: 2.0   memory length: 149103   epsilon: 0.9027740800021107    steps: 198    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 806   score: 0.0   memory length: 149226   epsilon: 0.902530540002116    steps: 123    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 807   score: 2.0   memory length: 149424   epsilon: 0.9021385000021245    steps: 198    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 808   score: 1.0   memory length: 149575   epsilon: 0.901839520002131    steps: 151    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 809   score: 2.0   memory length: 149793   epsilon: 0.9014078800021403    steps: 218    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 810   score: 0.0   memory length: 149916   epsilon: 0.9011643400021456    steps: 123    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 811   score: 0.0   memory length: 150039   epsilon: 0.9009208000021509    steps: 123    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 812   score: 0.0   memory length: 150162   epsilon: 0.9006772600021562    steps: 123    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 813   score: 2.0   memory length: 150379   epsilon: 0.9002476000021655    steps: 217    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 814   score: 1.0   memory length: 150531   epsilon: 0.8999466400021721    steps: 152    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 815   score: 0.0   memory length: 150653   epsilon: 0.8997050800021773    steps: 122    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 816   score: 2.0   memory length: 150850   epsilon: 0.8993150200021858    steps: 197    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 817   score: 1.0   memory length: 151001   epsilon: 0.8990160400021923    steps: 151    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 818   score: 1.0   memory length: 151172   epsilon: 0.8986774600021996    steps: 171    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 819   score: 0.0   memory length: 151294   epsilon: 0.8984359000022049    steps: 122    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 820   score: 2.0   memory length: 151513   epsilon: 0.8980022800022143    steps: 219    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 821   score: 0.0   memory length: 151635   epsilon: 0.8977607200022195    steps: 122    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 822   score: 2.0   memory length: 151832   epsilon: 0.897370660002228    steps: 197    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 823   score: 0.0   memory length: 151955   epsilon: 0.8971271200022333    steps: 123    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 824   score: 1.0   memory length: 152124   epsilon: 0.8967925000022405    steps: 169    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 825   score: 0.0   memory length: 152247   epsilon: 0.8965489600022458    steps: 123    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 826   score: 2.0   memory length: 152444   epsilon: 0.8961589000022543    steps: 197    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 827   score: 3.0   memory length: 152670   epsilon: 0.895711420002264    steps: 226    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 828   score: 1.0   memory length: 152839   epsilon: 0.8953768000022713    steps: 169    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 829   score: 4.0   memory length: 153135   epsilon: 0.894790720002284    steps: 296    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 830   score: 2.0   memory length: 153333   epsilon: 0.8943986800022925    steps: 198    lr: 1e-05     evaluation reward: 1.38\n",
            "episode: 831   score: 0.0   memory length: 153456   epsilon: 0.8941551400022978    steps: 123    lr: 1e-05     evaluation reward: 1.34\n",
            "episode: 832   score: 0.0   memory length: 153579   epsilon: 0.8939116000023031    steps: 123    lr: 1e-05     evaluation reward: 1.32\n",
            "episode: 833   score: 4.0   memory length: 153892   epsilon: 0.8932918600023165    steps: 313    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 834   score: 1.0   memory length: 154042   epsilon: 0.892994860002323    steps: 150    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 835   score: 0.0   memory length: 154164   epsilon: 0.8927533000023282    steps: 122    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 836   score: 3.0   memory length: 154377   epsilon: 0.8923315600023374    steps: 213    lr: 1e-05     evaluation reward: 1.36\n",
            "episode: 837   score: 7.0   memory length: 154675   epsilon: 0.8917415200023502    steps: 298    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 838   score: 2.0   memory length: 154892   epsilon: 0.8913118600023595    steps: 217    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 839   score: 1.0   memory length: 155064   epsilon: 0.8909713000023669    steps: 172    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 840   score: 2.0   memory length: 155282   epsilon: 0.8905396600023763    steps: 218    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 841   score: 1.0   memory length: 155451   epsilon: 0.8902050400023835    steps: 169    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 842   score: 0.0   memory length: 155574   epsilon: 0.8899615000023888    steps: 123    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 843   score: 1.0   memory length: 155746   epsilon: 0.8896209400023962    steps: 172    lr: 1e-05     evaluation reward: 1.35\n",
            "episode: 844   score: 3.0   memory length: 155992   epsilon: 0.8891338600024068    steps: 246    lr: 1e-05     evaluation reward: 1.37\n",
            "episode: 845   score: 1.0   memory length: 156142   epsilon: 0.8888368600024132    steps: 150    lr: 1e-05     evaluation reward: 1.37\n",
            "episode: 846   score: 2.0   memory length: 156340   epsilon: 0.8884448200024218    steps: 198    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 847   score: 2.0   memory length: 156560   epsilon: 0.8880092200024312    steps: 220    lr: 1e-05     evaluation reward: 1.41\n",
            "episode: 848   score: 3.0   memory length: 156825   epsilon: 0.8874845200024426    steps: 265    lr: 1e-05     evaluation reward: 1.43\n",
            "episode: 849   score: 1.0   memory length: 156993   epsilon: 0.8871518800024498    steps: 168    lr: 1e-05     evaluation reward: 1.42\n",
            "episode: 850   score: 0.0   memory length: 157116   epsilon: 0.8869083400024551    steps: 123    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 851   score: 2.0   memory length: 157332   epsilon: 0.8864806600024644    steps: 216    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 852   score: 2.0   memory length: 157550   epsilon: 0.8860490200024738    steps: 218    lr: 1e-05     evaluation reward: 1.39\n",
            "episode: 853   score: 2.0   memory length: 157747   epsilon: 0.8856589600024822    steps: 197    lr: 1e-05     evaluation reward: 1.4\n",
            "episode: 854   score: 4.0   memory length: 158064   epsilon: 0.8850313000024959    steps: 317    lr: 1e-05     evaluation reward: 1.44\n",
            "episode: 855   score: 2.0   memory length: 158261   epsilon: 0.8846412400025043    steps: 197    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 856   score: 2.0   memory length: 158476   epsilon: 0.8842155400025136    steps: 215    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 857   score: 2.0   memory length: 158691   epsilon: 0.8837898400025228    steps: 215    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 858   score: 4.0   memory length: 158987   epsilon: 0.8832037600025355    steps: 296    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 859   score: 2.0   memory length: 159184   epsilon: 0.882813700002544    steps: 197    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 860   score: 3.0   memory length: 159410   epsilon: 0.8823662200025537    steps: 226    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 861   score: 2.0   memory length: 159607   epsilon: 0.8819761600025622    steps: 197    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 862   score: 4.0   memory length: 159898   epsilon: 0.8813999800025747    steps: 291    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 863   score: 2.0   memory length: 160115   epsilon: 0.880970320002584    steps: 217    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 864   score: 0.0   memory length: 160237   epsilon: 0.8807287600025893    steps: 122    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 865   score: 5.0   memory length: 160526   epsilon: 0.8801565400026017    steps: 289    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 866   score: 3.0   memory length: 160754   epsilon: 0.8797051000026115    steps: 228    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 867   score: 1.0   memory length: 160923   epsilon: 0.8793704800026187    steps: 169    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 868   score: 3.0   memory length: 161173   epsilon: 0.8788754800026295    steps: 250    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 869   score: 1.0   memory length: 161323   epsilon: 0.8785784800026359    steps: 150    lr: 1e-05     evaluation reward: 1.54\n",
            "episode: 870   score: 2.0   memory length: 161520   epsilon: 0.8781884200026444    steps: 197    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 871   score: 2.0   memory length: 161737   epsilon: 0.8777587600026537    steps: 217    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 872   score: 1.0   memory length: 161905   epsilon: 0.877426120002661    steps: 168    lr: 1e-05     evaluation reward: 1.59\n",
            "episode: 873   score: 1.0   memory length: 162074   epsilon: 0.8770915000026682    steps: 169    lr: 1e-05     evaluation reward: 1.6\n",
            "episode: 874   score: 0.0   memory length: 162197   epsilon: 0.8768479600026735    steps: 123    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 875   score: 2.0   memory length: 162415   epsilon: 0.8764163200026829    steps: 218    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 876   score: 2.0   memory length: 162633   epsilon: 0.8759846800026923    steps: 218    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 877   score: 1.0   memory length: 162784   epsilon: 0.8756857000026987    steps: 151    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 878   score: 2.0   memory length: 163002   epsilon: 0.8752540600027081    steps: 218    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 879   score: 0.0   memory length: 163124   epsilon: 0.8750125000027134    steps: 122    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 880   score: 0.0   memory length: 163247   epsilon: 0.8747689600027186    steps: 123    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 881   score: 1.0   memory length: 163419   epsilon: 0.874428400002726    steps: 172    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 882   score: 0.0   memory length: 163542   epsilon: 0.8741848600027313    steps: 123    lr: 1e-05     evaluation reward: 1.54\n",
            "episode: 883   score: 2.0   memory length: 163740   epsilon: 0.8737928200027398    steps: 198    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 884   score: 2.0   memory length: 163956   epsilon: 0.8733651400027491    steps: 216    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 885   score: 1.0   memory length: 164125   epsilon: 0.8730305200027564    steps: 169    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 886   score: 0.0   memory length: 164247   epsilon: 0.8727889600027616    steps: 122    lr: 1e-05     evaluation reward: 1.54\n",
            "episode: 887   score: 1.0   memory length: 164416   epsilon: 0.8724543400027689    steps: 169    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 888   score: 3.0   memory length: 164644   epsilon: 0.8720029000027787    steps: 228    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 889   score: 3.0   memory length: 164891   epsilon: 0.8715138400027893    steps: 247    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 890   score: 0.0   memory length: 165014   epsilon: 0.8712703000027946    steps: 123    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 891   score: 3.0   memory length: 165282   epsilon: 0.8707396600028061    steps: 268    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 892   score: 0.0   memory length: 165405   epsilon: 0.8704961200028114    steps: 123    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 893   score: 1.0   memory length: 165555   epsilon: 0.8701991200028178    steps: 150    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 894   score: 1.0   memory length: 165724   epsilon: 0.8698645000028251    steps: 169    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 895   score: 0.0   memory length: 165847   epsilon: 0.8696209600028304    steps: 123    lr: 1e-05     evaluation reward: 1.54\n",
            "episode: 896   score: 2.0   memory length: 166045   epsilon: 0.8692289200028389    steps: 198    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 897   score: 1.0   memory length: 166196   epsilon: 0.8689299400028454    steps: 151    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 898   score: 2.0   memory length: 166394   epsilon: 0.8685379000028539    steps: 198    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 899   score: 1.0   memory length: 166546   epsilon: 0.8682369400028604    steps: 152    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 900   score: 4.0   memory length: 166846   epsilon: 0.8676429400028733    steps: 300    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 901   score: 1.0   memory length: 166996   epsilon: 0.8673459400028798    steps: 150    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 902   score: 0.0   memory length: 167119   epsilon: 0.8671024000028851    steps: 123    lr: 1e-05     evaluation reward: 1.56\n",
            "episode: 903   score: 3.0   memory length: 167363   epsilon: 0.8666192800028956    steps: 244    lr: 1e-05     evaluation reward: 1.59\n",
            "episode: 904   score: 2.0   memory length: 167561   epsilon: 0.8662272400029041    steps: 198    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 905   score: 2.0   memory length: 167777   epsilon: 0.8657995600029134    steps: 216    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 906   score: 2.0   memory length: 167978   epsilon: 0.865401580002922    steps: 201    lr: 1e-05     evaluation reward: 1.6\n",
            "episode: 907   score: 1.0   memory length: 168148   epsilon: 0.8650649800029293    steps: 170    lr: 1e-05     evaluation reward: 1.59\n",
            "episode: 908   score: 5.0   memory length: 168474   epsilon: 0.8644195000029433    steps: 326    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 909   score: 1.0   memory length: 168643   epsilon: 0.8640848800029506    steps: 169    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 910   score: 1.0   memory length: 168811   epsilon: 0.8637522400029578    steps: 168    lr: 1e-05     evaluation reward: 1.63\n",
            "episode: 911   score: 2.0   memory length: 169028   epsilon: 0.8633225800029671    steps: 217    lr: 1e-05     evaluation reward: 1.65\n",
            "episode: 912   score: 1.0   memory length: 169197   epsilon: 0.8629879600029744    steps: 169    lr: 1e-05     evaluation reward: 1.66\n",
            "episode: 913   score: 1.0   memory length: 169348   epsilon: 0.8626889800029809    steps: 151    lr: 1e-05     evaluation reward: 1.65\n",
            "episode: 914   score: 2.0   memory length: 169546   epsilon: 0.8622969400029894    steps: 198    lr: 1e-05     evaluation reward: 1.66\n",
            "episode: 915   score: 2.0   memory length: 169764   epsilon: 0.8618653000029988    steps: 218    lr: 1e-05     evaluation reward: 1.68\n",
            "episode: 916   score: 1.0   memory length: 169933   epsilon: 0.861530680003006    steps: 169    lr: 1e-05     evaluation reward: 1.67\n",
            "episode: 917   score: 1.0   memory length: 170102   epsilon: 0.8611960600030133    steps: 169    lr: 1e-05     evaluation reward: 1.67\n",
            "episode: 918   score: 1.0   memory length: 170274   epsilon: 0.8608555000030207    steps: 172    lr: 1e-05     evaluation reward: 1.67\n",
            "episode: 919   score: 4.0   memory length: 170533   epsilon: 0.8603426800030318    steps: 259    lr: 1e-05     evaluation reward: 1.71\n",
            "episode: 920   score: 2.0   memory length: 170731   epsilon: 0.8599506400030403    steps: 198    lr: 1e-05     evaluation reward: 1.71\n",
            "episode: 921   score: 5.0   memory length: 171071   epsilon: 0.859277440003055    steps: 340    lr: 1e-05     evaluation reward: 1.76\n",
            "episode: 922   score: 0.0   memory length: 171193   epsilon: 0.8590358800030602    steps: 122    lr: 1e-05     evaluation reward: 1.74\n",
            "episode: 923   score: 3.0   memory length: 171462   epsilon: 0.8585032600030718    steps: 269    lr: 1e-05     evaluation reward: 1.77\n",
            "episode: 924   score: 0.0   memory length: 171585   epsilon: 0.858259720003077    steps: 123    lr: 1e-05     evaluation reward: 1.76\n",
            "episode: 925   score: 0.0   memory length: 171708   epsilon: 0.8580161800030823    steps: 123    lr: 1e-05     evaluation reward: 1.76\n",
            "episode: 926   score: 3.0   memory length: 171971   epsilon: 0.8574954400030936    steps: 263    lr: 1e-05     evaluation reward: 1.77\n",
            "episode: 927   score: 2.0   memory length: 172169   epsilon: 0.8571034000031021    steps: 198    lr: 1e-05     evaluation reward: 1.76\n",
            "episode: 928   score: 2.0   memory length: 172367   epsilon: 0.8567113600031107    steps: 198    lr: 1e-05     evaluation reward: 1.77\n",
            "episode: 929   score: 2.0   memory length: 172565   epsilon: 0.8563193200031192    steps: 198    lr: 1e-05     evaluation reward: 1.75\n",
            "episode: 930   score: 1.0   memory length: 172735   epsilon: 0.8559827200031265    steps: 170    lr: 1e-05     evaluation reward: 1.74\n",
            "episode: 931   score: 4.0   memory length: 173027   epsilon: 0.855404560003139    steps: 292    lr: 1e-05     evaluation reward: 1.78\n",
            "episode: 932   score: 2.0   memory length: 173243   epsilon: 0.8549768800031483    steps: 216    lr: 1e-05     evaluation reward: 1.8\n",
            "episode: 933   score: 3.0   memory length: 173508   epsilon: 0.8544521800031597    steps: 265    lr: 1e-05     evaluation reward: 1.79\n",
            "episode: 934   score: 0.0   memory length: 173631   epsilon: 0.854208640003165    steps: 123    lr: 1e-05     evaluation reward: 1.78\n",
            "episode: 935   score: 2.0   memory length: 173833   epsilon: 0.8538086800031737    steps: 202    lr: 1e-05     evaluation reward: 1.8\n",
            "episode: 936   score: 2.0   memory length: 174032   epsilon: 0.8534146600031822    steps: 199    lr: 1e-05     evaluation reward: 1.79\n",
            "episode: 937   score: 3.0   memory length: 174280   epsilon: 0.8529236200031929    steps: 248    lr: 1e-05     evaluation reward: 1.75\n",
            "episode: 938   score: 2.0   memory length: 174499   epsilon: 0.8524900000032023    steps: 219    lr: 1e-05     evaluation reward: 1.75\n",
            "episode: 939   score: 1.0   memory length: 174668   epsilon: 0.8521553800032096    steps: 169    lr: 1e-05     evaluation reward: 1.75\n",
            "episode: 940   score: 1.0   memory length: 174818   epsilon: 0.851858380003216    steps: 150    lr: 1e-05     evaluation reward: 1.74\n",
            "episode: 941   score: 1.0   memory length: 174986   epsilon: 0.8515257400032232    steps: 168    lr: 1e-05     evaluation reward: 1.74\n",
            "episode: 942   score: 2.0   memory length: 175204   epsilon: 0.8510941000032326    steps: 218    lr: 1e-05     evaluation reward: 1.76\n",
            "episode: 943   score: 0.0   memory length: 175327   epsilon: 0.8508505600032379    steps: 123    lr: 1e-05     evaluation reward: 1.75\n",
            "episode: 944   score: 1.0   memory length: 175497   epsilon: 0.8505139600032452    steps: 170    lr: 1e-05     evaluation reward: 1.73\n",
            "episode: 945   score: 0.0   memory length: 175619   epsilon: 0.8502724000032504    steps: 122    lr: 1e-05     evaluation reward: 1.72\n",
            "episode: 946   score: 1.0   memory length: 175791   epsilon: 0.8499318400032578    steps: 172    lr: 1e-05     evaluation reward: 1.71\n",
            "episode: 947   score: 1.0   memory length: 175960   epsilon: 0.8495972200032651    steps: 169    lr: 1e-05     evaluation reward: 1.7\n",
            "episode: 948   score: 3.0   memory length: 176230   epsilon: 0.8490626200032767    steps: 270    lr: 1e-05     evaluation reward: 1.7\n",
            "episode: 949   score: 3.0   memory length: 176458   epsilon: 0.8486111800032865    steps: 228    lr: 1e-05     evaluation reward: 1.72\n",
            "episode: 950   score: 3.0   memory length: 176686   epsilon: 0.8481597400032963    steps: 228    lr: 1e-05     evaluation reward: 1.75\n",
            "episode: 951   score: 0.0   memory length: 176808   epsilon: 0.8479181800033015    steps: 122    lr: 1e-05     evaluation reward: 1.73\n",
            "episode: 952   score: 3.0   memory length: 177056   epsilon: 0.8474271400033122    steps: 248    lr: 1e-05     evaluation reward: 1.74\n",
            "episode: 953   score: 1.0   memory length: 177225   epsilon: 0.8470925200033195    steps: 169    lr: 1e-05     evaluation reward: 1.73\n",
            "episode: 954   score: 0.0   memory length: 177348   epsilon: 0.8468489800033248    steps: 123    lr: 1e-05     evaluation reward: 1.69\n",
            "episode: 955   score: 1.0   memory length: 177517   epsilon: 0.846514360003332    steps: 169    lr: 1e-05     evaluation reward: 1.68\n",
            "episode: 956   score: 3.0   memory length: 177761   epsilon: 0.8460312400033425    steps: 244    lr: 1e-05     evaluation reward: 1.69\n",
            "episode: 957   score: 1.0   memory length: 177912   epsilon: 0.845732260003349    steps: 151    lr: 1e-05     evaluation reward: 1.68\n",
            "episode: 958   score: 3.0   memory length: 178155   epsilon: 0.8452511200033594    steps: 243    lr: 1e-05     evaluation reward: 1.67\n",
            "episode: 959   score: 0.0   memory length: 178278   epsilon: 0.8450075800033647    steps: 123    lr: 1e-05     evaluation reward: 1.65\n",
            "episode: 960   score: 2.0   memory length: 178476   epsilon: 0.8446155400033732    steps: 198    lr: 1e-05     evaluation reward: 1.64\n",
            "episode: 961   score: 0.0   memory length: 178599   epsilon: 0.8443720000033785    steps: 123    lr: 1e-05     evaluation reward: 1.62\n",
            "episode: 962   score: 0.0   memory length: 178722   epsilon: 0.8441284600033838    steps: 123    lr: 1e-05     evaluation reward: 1.58\n",
            "episode: 963   score: 1.0   memory length: 178873   epsilon: 0.8438294800033903    steps: 151    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 964   score: 0.0   memory length: 178995   epsilon: 0.8435879200033956    steps: 122    lr: 1e-05     evaluation reward: 1.57\n",
            "episode: 965   score: 3.0   memory length: 179240   epsilon: 0.8431028200034061    steps: 245    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 966   score: 1.0   memory length: 179412   epsilon: 0.8427622600034135    steps: 172    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 967   score: 3.0   memory length: 179657   epsilon: 0.842277160003424    steps: 245    lr: 1e-05     evaluation reward: 1.55\n",
            "episode: 968   score: 0.0   memory length: 179779   epsilon: 0.8420356000034293    steps: 122    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 969   score: 2.0   memory length: 179997   epsilon: 0.8416039600034386    steps: 218    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 970   score: 2.0   memory length: 180215   epsilon: 0.841172320003448    steps: 218    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 971   score: 0.0   memory length: 180338   epsilon: 0.8409287800034533    steps: 123    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 972   score: 0.0   memory length: 180461   epsilon: 0.8406852400034586    steps: 123    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 973   score: 0.0   memory length: 180584   epsilon: 0.8404417000034639    steps: 123    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 974   score: 0.0   memory length: 180707   epsilon: 0.8401981600034691    steps: 123    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 975   score: 0.0   memory length: 180830   epsilon: 0.8399546200034744    steps: 123    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 976   score: 0.0   memory length: 180953   epsilon: 0.8397110800034797    steps: 123    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 977   score: 2.0   memory length: 181153   epsilon: 0.8393150800034883    steps: 200    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 978   score: 1.0   memory length: 181324   epsilon: 0.8389765000034957    steps: 171    lr: 1e-05     evaluation reward: 1.45\n",
            "episode: 979   score: 1.0   memory length: 181492   epsilon: 0.8386438600035029    steps: 168    lr: 1e-05     evaluation reward: 1.46\n",
            "episode: 980   score: 1.0   memory length: 181663   epsilon: 0.8383052800035102    steps: 171    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 981   score: 1.0   memory length: 181832   epsilon: 0.8379706600035175    steps: 169    lr: 1e-05     evaluation reward: 1.47\n",
            "episode: 982   score: 3.0   memory length: 182061   epsilon: 0.8375172400035273    steps: 229    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 983   score: 1.0   memory length: 182229   epsilon: 0.8371846000035346    steps: 168    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 984   score: 1.0   memory length: 182399   epsilon: 0.8368480000035419    steps: 170    lr: 1e-05     evaluation reward: 1.48\n",
            "episode: 985   score: 2.0   memory length: 182617   epsilon: 0.8364163600035512    steps: 218    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 986   score: 3.0   memory length: 182845   epsilon: 0.835964920003561    steps: 228    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 987   score: 3.0   memory length: 183093   epsilon: 0.8354738800035717    steps: 248    lr: 1e-05     evaluation reward: 1.54\n",
            "episode: 988   score: 2.0   memory length: 183290   epsilon: 0.8350838200035802    steps: 197    lr: 1e-05     evaluation reward: 1.53\n",
            "episode: 989   score: 0.0   memory length: 183412   epsilon: 0.8348422600035854    steps: 122    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 990   score: 2.0   memory length: 183632   epsilon: 0.8344066600035949    steps: 220    lr: 1e-05     evaluation reward: 1.52\n",
            "episode: 991   score: 0.0   memory length: 183755   epsilon: 0.8341631200036002    steps: 123    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 992   score: 2.0   memory length: 183953   epsilon: 0.8337710800036087    steps: 198    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 993   score: 0.0   memory length: 184076   epsilon: 0.833527540003614    steps: 123    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 994   score: 2.0   memory length: 184294   epsilon: 0.8330959000036233    steps: 218    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 995   score: 0.0   memory length: 184417   epsilon: 0.8328523600036286    steps: 123    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 996   score: 2.0   memory length: 184636   epsilon: 0.832418740003638    steps: 219    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 997   score: 1.0   memory length: 184808   epsilon: 0.8320781800036454    steps: 172    lr: 1e-05     evaluation reward: 1.51\n",
            "episode: 998   score: 1.0   memory length: 184977   epsilon: 0.8317435600036527    steps: 169    lr: 1e-05     evaluation reward: 1.5\n",
            "episode: 999   score: 0.0   memory length: 185099   epsilon: 0.8315020000036579    steps: 122    lr: 1e-05     evaluation reward: 1.49\n",
            "episode: 1000   score: 2.0   memory length: 185296   epsilon: 0.8311119400036664    steps: 197    lr: 1e-05     evaluation reward: 1.47\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-291524992219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Start training after random sample generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_policy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;31m# Update the target network only for Double DQN only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdouble_dqn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mupdate_target_network_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/agent.py\u001b[0m in \u001b[0;36mtrain_policy_net\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/memory.py\u001b[0m in \u001b[0;36msample_mini_batch\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mmini_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_ndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m     \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0mexpanded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e8WsQh42OsT"
      },
      "source": [
        "# Visualize Agent Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20vLFGcp2OsT"
      },
      "source": [
        "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
        "\n",
        "Please save your model before running this portion of the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXQttKal2OsT"
      },
      "source": [
        "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ibe_QZ8c3J_c",
        "outputId": "993a0c93-5c78-4498-b173-f0cc91932e73"
      },
      "source": [
        "pip install pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/8a/643043cc70791367bee2d19eb20e00ed1a246ac48e5dbe57bbbcc8be40a9/PyVirtualDisplay-1.3.2-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-1.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZqvZeoJ2OsU"
      },
      "source": [
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Displaying the game live\n",
        "def show_state(env, step=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
        "    plt.axis('off')\n",
        "\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "    \n",
        "# Recording the game and replaying the game afterwards\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoMFyPg82OsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef599d99-9952-4838-eb29-80ba797087b7"
      },
      "source": [
        "!sudo apt-get install -y xvfb python-opengl ffmpeg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl xvfb\n",
            "0 upgraded, 2 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 1,280 kB of archives.\n",
            "After this operation, 7,686 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 1,280 kB in 1s (940 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "id": "uThODipO2OsU",
        "outputId": "db271949-d8d9-48f1-d0af-3e440b30e55c"
      },
      "source": [
        "display = Display(visible=0, size=(300, 200))\n",
        "display.start()\n",
        "\n",
        "# Load agent\n",
        "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
        "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
        "\n",
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "env = wrap_env(env)\n",
        "\n",
        "done = False\n",
        "score = 0\n",
        "step = 0\n",
        "state = env.reset()\n",
        "next_state = state\n",
        "life = number_lives\n",
        "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "get_init_state(history, state)\n",
        "\n",
        "while not done:\n",
        "    \n",
        "    # Render breakout\n",
        "    env.render()\n",
        "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
        "\n",
        "    step += 1\n",
        "    frame += 1\n",
        "\n",
        "    # Perform a fire action if ball is no longer on screen\n",
        "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "        action = 0\n",
        "    else:\n",
        "        action = torch.from_numpy(agent.get_action(torch.from_numpy(np.float32(history[:4, :, :]) / 255.)))\n",
        "    state = next_state\n",
        "    \n",
        "    next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "    frame_next_state = get_frame(next_state)\n",
        "    history[4, :, :] = frame_next_state\n",
        "    terminal_state = check_live(life, info['ale.lives'])\n",
        "        \n",
        "    life = info['ale.lives']\n",
        "    r = np.clip(reward, -1, 1) \n",
        "    r = reward\n",
        "\n",
        "    # Store the transition in memory \n",
        "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "    # Start training after random sample generation\n",
        "    score += reward\n",
        "    \n",
        "    history[:4, :, :] = history[1:, :, :]\n",
        "env.close()\n",
        "show_video()\n",
        "display.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e027c8ab025b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/agent.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Choose the best action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR1t810C60Ye"
      },
      "source": [
        "double_dqn = True # set to True if using double DQN agent\n",
        "\n",
        "if double_dqn:\n",
        "    from agent_double import Agent\n",
        "else:\n",
        "    from agent import Agent\n",
        "\n",
        "agent = Agent(action_size)\n",
        "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
        "frame = 0\n",
        "memory_size = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7415Fkuj62je",
        "outputId": "48595952-c820-4fca-c9ee-686c490485f2"
      },
      "source": [
        "rewards, episodes = [], []\n",
        "best_eval_reward = 0\n",
        "for e in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "    step = 0\n",
        "    d = False\n",
        "    state = env.reset()\n",
        "    next_state = state\n",
        "    life = number_lives\n",
        "\n",
        "    get_init_state(history, state)\n",
        "\n",
        "    while not done:\n",
        "        step += 1\n",
        "        frame += 1\n",
        "\n",
        "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
        "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "            action = 0\n",
        "        else:\n",
        "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "        state = next_state\n",
        "        next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "        frame_next_state = get_frame(next_state)\n",
        "        history[4, :, :] = frame_next_state\n",
        "        terminal_state = check_live(life, info['ale.lives'])\n",
        "\n",
        "        life = info['ale.lives']\n",
        "        r = np.clip(reward, -1, 1) \n",
        "        r = reward\n",
        "\n",
        "        # Store the transition in memory \n",
        "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "        # Start training after random sample generation\n",
        "        if(frame >= train_frame):\n",
        "            agent.train_policy_net(frame)\n",
        "            # Update the target network only for Double DQN only\n",
        "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
        "                agent.update_target_net()\n",
        "        score += reward\n",
        "        history[:4, :, :] = history[1:, :, :]\n",
        "            \n",
        "        if done:\n",
        "            evaluation_reward.append(score)\n",
        "            rewards.append(np.mean(evaluation_reward))\n",
        "            episodes.append(e)\n",
        "            pylab.plot(episodes, rewards, 'b')\n",
        "            pylab.xlabel('Episodes')\n",
        "            pylab.ylabel('Rewards') \n",
        "            pylab.title('Episodes vs Reward')\n",
        "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
        "            \n",
        "            # every episode, plot the play time\n",
        "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
        "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
        "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
        "\n",
        "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
        "            ### Change this save condition to whatever you prefer ###\n",
        "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
        "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
        "                best_eval_reward = np.mean(evaluation_reward)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0   score: 4.0   memory length: 296   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 4.0\n",
            "episode: 1   score: 2.0   memory length: 494   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 3.0\n",
            "episode: 2   score: 2.0   memory length: 692   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 2.6666666666666665\n",
            "episode: 3   score: 0.0   memory length: 815   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 4   score: 3.0   memory length: 1084   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 2.2\n",
            "episode: 5   score: 2.0   memory length: 1300   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 2.1666666666666665\n",
            "episode: 6   score: 2.0   memory length: 1518   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 2.142857142857143\n",
            "episode: 7   score: 1.0   memory length: 1689   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 8   score: 3.0   memory length: 1915   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 2.111111111111111\n",
            "episode: 9   score: 0.0   memory length: 2038   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 10   score: 1.0   memory length: 2189   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.8181818181818181\n",
            "episode: 11   score: 2.0   memory length: 2368   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.8333333333333333\n",
            "episode: 12   score: 2.0   memory length: 2586   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.8461538461538463\n",
            "episode: 13   score: 2.0   memory length: 2784   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.8571428571428572\n",
            "episode: 14   score: 4.0   memory length: 3062   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 15   score: 0.0   memory length: 3185   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.875\n",
            "episode: 16   score: 3.0   memory length: 3416   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.9411764705882353\n",
            "episode: 17   score: 4.0   memory length: 3712   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 2.0555555555555554\n",
            "episode: 18   score: 2.0   memory length: 3910   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 2.0526315789473686\n",
            "episode: 19   score: 1.0   memory length: 4078   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 20   score: 0.0   memory length: 4200   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.9047619047619047\n",
            "episode: 21   score: 1.0   memory length: 4369   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.8636363636363635\n",
            "episode: 22   score: 1.0   memory length: 4538   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.826086956521739\n",
            "episode: 23   score: 1.0   memory length: 4709   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.7916666666666667\n",
            "episode: 24   score: 3.0   memory length: 4919   epsilon: 1.0    steps: 210    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 25   score: 1.0   memory length: 5087   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.8076923076923077\n",
            "episode: 26   score: 4.0   memory length: 5350   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.8888888888888888\n",
            "episode: 27   score: 2.0   memory length: 5568   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.8928571428571428\n",
            "episode: 28   score: 0.0   memory length: 5690   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.8275862068965518\n",
            "episode: 29   score: 2.0   memory length: 5887   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.8333333333333333\n",
            "episode: 30   score: 0.0   memory length: 6010   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7741935483870968\n",
            "episode: 31   score: 1.0   memory length: 6161   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 32   score: 2.0   memory length: 6340   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.7575757575757576\n",
            "episode: 33   score: 1.0   memory length: 6509   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.7352941176470589\n",
            "episode: 34   score: 1.0   memory length: 6680   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.7142857142857142\n",
            "episode: 35   score: 1.0   memory length: 6831   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6944444444444444\n",
            "episode: 36   score: 0.0   memory length: 6953   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6486486486486487\n",
            "episode: 37   score: 0.0   memory length: 7076   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.605263157894737\n",
            "episode: 38   score: 1.0   memory length: 7245   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5897435897435896\n",
            "episode: 39   score: 0.0   memory length: 7368   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 40   score: 0.0   memory length: 7490   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5121951219512195\n",
            "episode: 41   score: 1.0   memory length: 7658   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 42   score: 5.0   memory length: 7983   epsilon: 1.0    steps: 325    lr: 0.0001     evaluation reward: 1.5813953488372092\n",
            "episode: 43   score: 1.0   memory length: 8152   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5681818181818181\n",
            "episode: 44   score: 2.0   memory length: 8350   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5777777777777777\n",
            "episode: 45   score: 2.0   memory length: 8529   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.5869565217391304\n",
            "episode: 46   score: 1.0   memory length: 8679   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.574468085106383\n",
            "episode: 47   score: 3.0   memory length: 8929   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.6041666666666667\n",
            "episode: 48   score: 3.0   memory length: 9176   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.6326530612244898\n",
            "episode: 49   score: 2.0   memory length: 9374   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 50   score: 4.0   memory length: 9649   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.6862745098039216\n",
            "episode: 51   score: 2.0   memory length: 9867   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6923076923076923\n",
            "episode: 52   score: 2.0   memory length: 10065   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6981132075471699\n",
            "episode: 53   score: 3.0   memory length: 10296   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.7222222222222223\n",
            "episode: 54   score: 0.0   memory length: 10419   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.690909090909091\n",
            "episode: 55   score: 2.0   memory length: 10619   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.6964285714285714\n",
            "episode: 56   score: 0.0   memory length: 10742   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
            "episode: 57   score: 2.0   memory length: 10960   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6724137931034482\n",
            "episode: 58   score: 1.0   memory length: 11129   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6610169491525424\n",
            "episode: 59   score: 0.0   memory length: 11251   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6333333333333333\n",
            "episode: 60   score: 1.0   memory length: 11422   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6229508196721312\n",
            "episode: 61   score: 4.0   memory length: 11718   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.6612903225806452\n",
            "episode: 62   score: 0.0   memory length: 11840   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6349206349206349\n",
            "episode: 63   score: 3.0   memory length: 12086   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.65625\n",
            "episode: 64   score: 1.0   memory length: 12255   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.646153846153846\n",
            "episode: 65   score: 1.0   memory length: 12423   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
            "episode: 66   score: 1.0   memory length: 12573   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.626865671641791\n",
            "episode: 67   score: 1.0   memory length: 12743   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6176470588235294\n",
            "episode: 68   score: 3.0   memory length: 12986   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.6376811594202898\n",
            "episode: 69   score: 2.0   memory length: 13207   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.6428571428571428\n",
            "episode: 70   score: 1.0   memory length: 13376   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6338028169014085\n",
            "episode: 71   score: 0.0   memory length: 13499   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6111111111111112\n",
            "episode: 72   score: 3.0   memory length: 13744   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.63013698630137\n",
            "episode: 73   score: 0.0   memory length: 13867   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6081081081081081\n",
            "episode: 74   score: 3.0   memory length: 14133   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.6266666666666667\n",
            "episode: 75   score: 0.0   memory length: 14255   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.605263157894737\n",
            "episode: 76   score: 2.0   memory length: 14453   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6103896103896105\n",
            "episode: 77   score: 2.0   memory length: 14671   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6153846153846154\n",
            "episode: 78   score: 2.0   memory length: 14868   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.620253164556962\n",
            "episode: 79   score: 1.0   memory length: 15039   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6125\n",
            "episode: 80   score: 1.0   memory length: 15210   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6049382716049383\n",
            "episode: 81   score: 0.0   memory length: 15333   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5853658536585367\n",
            "episode: 82   score: 5.0   memory length: 15610   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.6265060240963856\n",
            "episode: 83   score: 0.0   memory length: 15732   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6071428571428572\n",
            "episode: 84   score: 0.0   memory length: 15855   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.588235294117647\n",
            "episode: 85   score: 2.0   memory length: 16078   epsilon: 1.0    steps: 223    lr: 0.0001     evaluation reward: 1.5930232558139534\n",
            "episode: 86   score: 2.0   memory length: 16258   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.5977011494252873\n",
            "episode: 87   score: 1.0   memory length: 16409   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5909090909090908\n",
            "episode: 88   score: 1.0   memory length: 16578   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5842696629213484\n",
            "episode: 89   score: 2.0   memory length: 16775   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5888888888888888\n",
            "episode: 90   score: 0.0   memory length: 16898   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5714285714285714\n",
            "episode: 91   score: 2.0   memory length: 17114   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.576086956521739\n",
            "episode: 92   score: 2.0   memory length: 17331   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.5806451612903225\n",
            "episode: 93   score: 0.0   memory length: 17454   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5638297872340425\n",
            "episode: 94   score: 1.0   memory length: 17623   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5578947368421052\n",
            "episode: 95   score: 1.0   memory length: 17792   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5520833333333333\n",
            "episode: 96   score: 3.0   memory length: 18018   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.5670103092783505\n",
            "episode: 97   score: 1.0   memory length: 18186   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.5612244897959184\n",
            "episode: 98   score: 0.0   memory length: 18309   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5454545454545454\n",
            "episode: 99   score: 0.0   memory length: 18432   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 100   score: 0.0   memory length: 18555   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 101   score: 1.0   memory length: 18726   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 102   score: 2.0   memory length: 18945   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 103   score: 2.0   memory length: 19143   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 104   score: 2.0   memory length: 19343   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 105   score: 1.0   memory length: 19512   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 106   score: 1.0   memory length: 19682   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 107   score: 2.0   memory length: 19880   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 108   score: 3.0   memory length: 20126   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 109   score: 1.0   memory length: 20277   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 110   score: 3.0   memory length: 20522   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 111   score: 2.0   memory length: 20740   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 112   score: 0.0   memory length: 20863   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 113   score: 2.0   memory length: 21079   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 114   score: 1.0   memory length: 21248   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 115   score: 0.0   memory length: 21371   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 116   score: 0.0   memory length: 21494   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 117   score: 1.0   memory length: 21645   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 118   score: 3.0   memory length: 21891   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 119   score: 2.0   memory length: 22089   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 120   score: 4.0   memory length: 22390   epsilon: 1.0    steps: 301    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 121   score: 2.0   memory length: 22588   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 122   score: 2.0   memory length: 22786   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 123   score: 2.0   memory length: 22984   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 124   score: 1.0   memory length: 23135   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 125   score: 2.0   memory length: 23332   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 126   score: 0.0   memory length: 23455   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 127   score: 3.0   memory length: 23687   epsilon: 1.0    steps: 232    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 128   score: 1.0   memory length: 23856   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 129   score: 2.0   memory length: 24074   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 130   score: 2.0   memory length: 24272   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 131   score: 3.0   memory length: 24540   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 132   score: 5.0   memory length: 24848   epsilon: 1.0    steps: 308    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 133   score: 0.0   memory length: 24971   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 134   score: 2.0   memory length: 25169   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 135   score: 0.0   memory length: 25292   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 136   score: 1.0   memory length: 25443   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 137   score: 1.0   memory length: 25614   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 138   score: 1.0   memory length: 25785   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 139   score: 3.0   memory length: 26037   epsilon: 1.0    steps: 252    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 140   score: 1.0   memory length: 26187   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 141   score: 2.0   memory length: 26409   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 142   score: 1.0   memory length: 26580   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 143   score: 2.0   memory length: 26797   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 144   score: 0.0   memory length: 26919   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 145   score: 1.0   memory length: 27089   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 146   score: 1.0   memory length: 27258   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 147   score: 0.0   memory length: 27381   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 148   score: 0.0   memory length: 27504   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 149   score: 0.0   memory length: 27627   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 150   score: 0.0   memory length: 27750   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 151   score: 1.0   memory length: 27919   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 152   score: 2.0   memory length: 28117   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 153   score: 0.0   memory length: 28239   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 154   score: 1.0   memory length: 28390   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 155   score: 0.0   memory length: 28513   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 156   score: 1.0   memory length: 28665   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 157   score: 2.0   memory length: 28866   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 158   score: 0.0   memory length: 28988   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 159   score: 0.0   memory length: 29110   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 160   score: 1.0   memory length: 29280   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 161   score: 1.0   memory length: 29430   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 162   score: 1.0   memory length: 29581   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 163   score: 2.0   memory length: 29799   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 164   score: 2.0   memory length: 30017   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 165   score: 2.0   memory length: 30235   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 166   score: 2.0   memory length: 30455   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 167   score: 0.0   memory length: 30577   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 168   score: 2.0   memory length: 30774   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 169   score: 2.0   memory length: 30972   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 170   score: 0.0   memory length: 31095   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 171   score: 0.0   memory length: 31217   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 172   score: 2.0   memory length: 31415   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 173   score: 1.0   memory length: 31587   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 174   score: 1.0   memory length: 31757   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 175   score: 2.0   memory length: 31955   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 176   score: 2.0   memory length: 32152   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 177   score: 3.0   memory length: 32395   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 178   score: 4.0   memory length: 32669   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 179   score: 3.0   memory length: 32914   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 180   score: 2.0   memory length: 33130   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 181   score: 2.0   memory length: 33328   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 182   score: 2.0   memory length: 33526   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 183   score: 3.0   memory length: 33773   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 184   score: 1.0   memory length: 33924   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 185   score: 1.0   memory length: 34095   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 186   score: 0.0   memory length: 34217   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 187   score: 2.0   memory length: 34435   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 188   score: 2.0   memory length: 34637   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 189   score: 2.0   memory length: 34855   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 190   score: 0.0   memory length: 34978   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 191   score: 4.0   memory length: 35255   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 192   score: 1.0   memory length: 35406   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 193   score: 0.0   memory length: 35529   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 194   score: 0.0   memory length: 35652   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 195   score: 1.0   memory length: 35821   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 196   score: 3.0   memory length: 36048   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 197   score: 2.0   memory length: 36264   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 198   score: 1.0   memory length: 36433   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 199   score: 0.0   memory length: 36556   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 200   score: 2.0   memory length: 36756   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 201   score: 2.0   memory length: 36957   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 202   score: 2.0   memory length: 37139   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 203   score: 2.0   memory length: 37357   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 204   score: 1.0   memory length: 37525   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 205   score: 1.0   memory length: 37693   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 206   score: 1.0   memory length: 37862   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 207   score: 0.0   memory length: 37985   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 208   score: 3.0   memory length: 38228   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 209   score: 2.0   memory length: 38448   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 210   score: 0.0   memory length: 38571   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 211   score: 3.0   memory length: 38799   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 212   score: 3.0   memory length: 39046   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 213   score: 2.0   memory length: 39268   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 214   score: 0.0   memory length: 39390   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 215   score: 2.0   memory length: 39610   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 216   score: 2.0   memory length: 39829   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 217   score: 2.0   memory length: 40030   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 218   score: 1.0   memory length: 40200   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 219   score: 2.0   memory length: 40398   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 220   score: 3.0   memory length: 40644   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 221   score: 2.0   memory length: 40862   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 222   score: 5.0   memory length: 41170   epsilon: 1.0    steps: 308    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 223   score: 1.0   memory length: 41339   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 224   score: 0.0   memory length: 41461   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 225   score: 4.0   memory length: 41735   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 226   score: 3.0   memory length: 41980   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 227   score: 7.0   memory length: 42373   epsilon: 1.0    steps: 393    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 228   score: 0.0   memory length: 42495   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 229   score: 0.0   memory length: 42618   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 230   score: 1.0   memory length: 42771   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 231   score: 3.0   memory length: 43023   epsilon: 1.0    steps: 252    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 232   score: 1.0   memory length: 43192   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 233   score: 1.0   memory length: 43361   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 234   score: 1.0   memory length: 43512   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 235   score: 4.0   memory length: 43788   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 236   score: 1.0   memory length: 43938   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 237   score: 1.0   memory length: 44089   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 238   score: 1.0   memory length: 44261   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 239   score: 0.0   memory length: 44383   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 240   score: 2.0   memory length: 44566   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 241   score: 0.0   memory length: 44689   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 242   score: 0.0   memory length: 44812   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 243   score: 0.0   memory length: 44935   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 244   score: 3.0   memory length: 45183   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 245   score: 0.0   memory length: 45305   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 246   score: 2.0   memory length: 45503   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 247   score: 2.0   memory length: 45719   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 248   score: 3.0   memory length: 45983   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 249   score: 1.0   memory length: 46155   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 250   score: 2.0   memory length: 46353   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 251   score: 2.0   memory length: 46551   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 252   score: 0.0   memory length: 46674   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 253   score: 3.0   memory length: 46941   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 254   score: 0.0   memory length: 47064   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 255   score: 0.0   memory length: 47186   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 256   score: 0.0   memory length: 47308   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 257   score: 1.0   memory length: 47459   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 258   score: 0.0   memory length: 47582   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 259   score: 0.0   memory length: 47704   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 260   score: 1.0   memory length: 47854   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 261   score: 2.0   memory length: 48071   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 262   score: 3.0   memory length: 48337   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 263   score: 3.0   memory length: 48602   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 264   score: 1.0   memory length: 48753   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 265   score: 2.0   memory length: 48951   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 266   score: 4.0   memory length: 49209   epsilon: 1.0    steps: 258    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 267   score: 0.0   memory length: 49331   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 268   score: 0.0   memory length: 49454   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 269   score: 0.0   memory length: 49576   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 270   score: 6.0   memory length: 49969   epsilon: 1.0    steps: 393    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 271   score: 0.0   memory length: 50092   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 272   score: 4.0   memory length: 50370   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 273   score: 1.0   memory length: 50520   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 274   score: 2.0   memory length: 50738   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 275   score: 0.0   memory length: 50861   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 276   score: 5.0   memory length: 51148   epsilon: 1.0    steps: 287    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 277   score: 2.0   memory length: 51368   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 278   score: 3.0   memory length: 51614   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 279   score: 4.0   memory length: 51911   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 280   score: 3.0   memory length: 52155   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 281   score: 2.0   memory length: 52352   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 282   score: 1.0   memory length: 52523   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 283   score: 1.0   memory length: 52674   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 284   score: 0.0   memory length: 52797   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 285   score: 0.0   memory length: 52919   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 286   score: 3.0   memory length: 53183   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 287   score: 2.0   memory length: 53401   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 288   score: 2.0   memory length: 53616   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 289   score: 2.0   memory length: 53834   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 290   score: 0.0   memory length: 53956   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 291   score: 3.0   memory length: 54182   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 292   score: 2.0   memory length: 54380   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 293   score: 1.0   memory length: 54548   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 294   score: 0.0   memory length: 54670   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 295   score: 0.0   memory length: 54793   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 296   score: 2.0   memory length: 54990   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 297   score: 2.0   memory length: 55208   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 298   score: 1.0   memory length: 55358   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 299   score: 0.0   memory length: 55480   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 300   score: 0.0   memory length: 55603   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 301   score: 2.0   memory length: 55801   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 302   score: 4.0   memory length: 56097   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 303   score: 0.0   memory length: 56219   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 304   score: 0.0   memory length: 56342   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 305   score: 4.0   memory length: 56633   epsilon: 1.0    steps: 291    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 306   score: 2.0   memory length: 56851   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 307   score: 3.0   memory length: 57103   epsilon: 1.0    steps: 252    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 308   score: 1.0   memory length: 57254   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 309   score: 0.0   memory length: 57377   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 310   score: 1.0   memory length: 57546   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 311   score: 3.0   memory length: 57790   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 312   score: 0.0   memory length: 57912   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 313   score: 0.0   memory length: 58034   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 314   score: 3.0   memory length: 58268   epsilon: 1.0    steps: 234    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 315   score: 2.0   memory length: 58486   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 316   score: 0.0   memory length: 58609   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 317   score: 1.0   memory length: 58781   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 318   score: 0.0   memory length: 58904   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 319   score: 4.0   memory length: 59219   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 320   score: 0.0   memory length: 59341   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 321   score: 2.0   memory length: 59559   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 322   score: 2.0   memory length: 59777   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 323   score: 0.0   memory length: 59899   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 324   score: 3.0   memory length: 60145   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 325   score: 1.0   memory length: 60296   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 326   score: 0.0   memory length: 60419   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 327   score: 1.0   memory length: 60588   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 328   score: 1.0   memory length: 60757   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 329   score: 1.0   memory length: 60908   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 330   score: 1.0   memory length: 61078   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 331   score: 2.0   memory length: 61260   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 332   score: 2.0   memory length: 61475   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 333   score: 1.0   memory length: 61625   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 334   score: 3.0   memory length: 61874   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 335   score: 3.0   memory length: 62123   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 336   score: 5.0   memory length: 62464   epsilon: 1.0    steps: 341    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 337   score: 2.0   memory length: 62679   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 338   score: 0.0   memory length: 62802   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 339   score: 1.0   memory length: 62972   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 340   score: 1.0   memory length: 63123   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 341   score: 1.0   memory length: 63274   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 342   score: 0.0   memory length: 63397   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 343   score: 0.0   memory length: 63520   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 344   score: 1.0   memory length: 63690   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 345   score: 2.0   memory length: 63888   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 346   score: 1.0   memory length: 64057   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 347   score: 0.0   memory length: 64180   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 348   score: 3.0   memory length: 64407   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 349   score: 0.0   memory length: 64529   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 350   score: 1.0   memory length: 64699   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 351   score: 2.0   memory length: 64897   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 352   score: 3.0   memory length: 65143   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 353   score: 2.0   memory length: 65360   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 354   score: 3.0   memory length: 65607   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 355   score: 1.0   memory length: 65777   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 356   score: 0.0   memory length: 65900   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 357   score: 2.0   memory length: 66097   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 358   score: 5.0   memory length: 66404   epsilon: 1.0    steps: 307    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 359   score: 4.0   memory length: 66676   epsilon: 1.0    steps: 272    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 360   score: 2.0   memory length: 66891   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 361   score: 0.0   memory length: 67013   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 362   score: 2.0   memory length: 67230   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 363   score: 2.0   memory length: 67447   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 364   score: 1.0   memory length: 67616   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 365   score: 2.0   memory length: 67814   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 366   score: 1.0   memory length: 67966   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 367   score: 0.0   memory length: 68088   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 368   score: 1.0   memory length: 68259   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 369   score: 0.0   memory length: 68382   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 370   score: 1.0   memory length: 68552   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 371   score: 2.0   memory length: 68750   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 372   score: 1.0   memory length: 68901   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 373   score: 0.0   memory length: 69024   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 374   score: 0.0   memory length: 69146   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 375   score: 6.0   memory length: 69522   epsilon: 1.0    steps: 376    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 376   score: 1.0   memory length: 69691   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 377   score: 3.0   memory length: 69940   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 378   score: 0.0   memory length: 70062   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 379   score: 0.0   memory length: 70185   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 380   score: 0.0   memory length: 70307   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 381   score: 4.0   memory length: 70583   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 382   score: 0.0   memory length: 70706   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 383   score: 0.0   memory length: 70829   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 384   score: 1.0   memory length: 70998   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 385   score: 1.0   memory length: 71166   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 386   score: 1.0   memory length: 71336   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 387   score: 1.0   memory length: 71507   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 388   score: 0.0   memory length: 71629   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 389   score: 0.0   memory length: 71751   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 390   score: 0.0   memory length: 71873   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 391   score: 2.0   memory length: 72089   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 392   score: 2.0   memory length: 72307   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 393   score: 2.0   memory length: 72525   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 394   score: 2.0   memory length: 72747   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 395   score: 1.0   memory length: 72917   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 396   score: 0.0   memory length: 73040   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 397   score: 1.0   memory length: 73209   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 398   score: 6.0   memory length: 73588   epsilon: 1.0    steps: 379    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 399   score: 2.0   memory length: 73807   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 400   score: 0.0   memory length: 73930   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 401   score: 4.0   memory length: 74227   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 402   score: 0.0   memory length: 74349   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 403   score: 3.0   memory length: 74596   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 404   score: 1.0   memory length: 74746   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 405   score: 1.0   memory length: 74915   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 406   score: 0.0   memory length: 75038   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 407   score: 4.0   memory length: 75334   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 408   score: 1.0   memory length: 75503   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 409   score: 2.0   memory length: 75722   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 410   score: 3.0   memory length: 75950   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 411   score: 1.0   memory length: 76101   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 412   score: 3.0   memory length: 76328   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 413   score: 2.0   memory length: 76526   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 414   score: 1.0   memory length: 76695   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 415   score: 4.0   memory length: 76955   epsilon: 1.0    steps: 260    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 416   score: 2.0   memory length: 77153   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 417   score: 2.0   memory length: 77369   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 418   score: 1.0   memory length: 77539   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 419   score: 2.0   memory length: 77737   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 420   score: 2.0   memory length: 77919   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 421   score: 2.0   memory length: 78116   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 422   score: 0.0   memory length: 78238   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 423   score: 0.0   memory length: 78361   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 424   score: 1.0   memory length: 78512   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 425   score: 0.0   memory length: 78635   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 426   score: 0.0   memory length: 78758   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 427   score: 0.0   memory length: 78881   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 428   score: 1.0   memory length: 79051   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 429   score: 1.0   memory length: 79203   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 430   score: 2.0   memory length: 79401   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 431   score: 3.0   memory length: 79626   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 432   score: 0.0   memory length: 79749   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 433   score: 3.0   memory length: 79977   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 434   score: 3.0   memory length: 80221   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 435   score: 0.0   memory length: 80344   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 436   score: 2.0   memory length: 80542   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 437   score: 1.0   memory length: 80713   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 438   score: 2.0   memory length: 80929   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 439   score: 1.0   memory length: 81100   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 440   score: 1.0   memory length: 81268   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 441   score: 0.0   memory length: 81391   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 442   score: 1.0   memory length: 81560   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 443   score: 2.0   memory length: 81758   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 444   score: 1.0   memory length: 81927   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 445   score: 0.0   memory length: 82049   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 446   score: 1.0   memory length: 82219   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 447   score: 2.0   memory length: 82420   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 448   score: 2.0   memory length: 82620   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 449   score: 0.0   memory length: 82742   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 450   score: 0.0   memory length: 82865   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 451   score: 2.0   memory length: 83084   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 452   score: 2.0   memory length: 83284   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 453   score: 0.0   memory length: 83407   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 454   score: 3.0   memory length: 83655   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 455   score: 0.0   memory length: 83778   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 456   score: 1.0   memory length: 83929   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 457   score: 2.0   memory length: 84126   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 458   score: 0.0   memory length: 84249   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 459   score: 0.0   memory length: 84371   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 460   score: 0.0   memory length: 84494   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 461   score: 1.0   memory length: 84662   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 462   score: 4.0   memory length: 84955   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 463   score: 1.0   memory length: 85123   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 464   score: 1.0   memory length: 85291   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 465   score: 0.0   memory length: 85414   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 466   score: 0.0   memory length: 85536   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 467   score: 3.0   memory length: 85765   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 468   score: 1.0   memory length: 85933   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 469   score: 1.0   memory length: 86083   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 470   score: 1.0   memory length: 86234   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 471   score: 0.0   memory length: 86357   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 472   score: 4.0   memory length: 86653   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 473   score: 1.0   memory length: 86824   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 474   score: 2.0   memory length: 87023   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 475   score: 0.0   memory length: 87146   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 476   score: 3.0   memory length: 87393   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 477   score: 2.0   memory length: 87577   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 478   score: 3.0   memory length: 87822   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 479   score: 0.0   memory length: 87945   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 480   score: 2.0   memory length: 88143   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 481   score: 2.0   memory length: 88342   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 482   score: 2.0   memory length: 88540   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 483   score: 4.0   memory length: 88832   epsilon: 1.0    steps: 292    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 484   score: 1.0   memory length: 88982   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 485   score: 4.0   memory length: 89238   epsilon: 1.0    steps: 256    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 486   score: 3.0   memory length: 89484   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 487   score: 4.0   memory length: 89800   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 488   score: 0.0   memory length: 89923   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 489   score: 1.0   memory length: 90092   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 490   score: 2.0   memory length: 90290   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 491   score: 0.0   memory length: 90412   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 492   score: 1.0   memory length: 90581   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 493   score: 0.0   memory length: 90704   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 494   score: 3.0   memory length: 90947   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 495   score: 0.0   memory length: 91070   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 496   score: 4.0   memory length: 91332   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 497   score: 0.0   memory length: 91455   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 498   score: 0.0   memory length: 91577   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 499   score: 2.0   memory length: 91760   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 500   score: 0.0   memory length: 91883   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 501   score: 2.0   memory length: 92081   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 502   score: 2.0   memory length: 92300   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 503   score: 3.0   memory length: 92526   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 504   score: 0.0   memory length: 92649   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 505   score: 2.0   memory length: 92846   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 506   score: 2.0   memory length: 93043   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 507   score: 2.0   memory length: 93241   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 508   score: 1.0   memory length: 93391   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 509   score: 1.0   memory length: 93542   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 510   score: 2.0   memory length: 93759   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 511   score: 6.0   memory length: 94108   epsilon: 1.0    steps: 349    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 512   score: 1.0   memory length: 94276   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 513   score: 2.0   memory length: 94474   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 514   score: 1.0   memory length: 94644   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 515   score: 3.0   memory length: 94891   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 516   score: 1.0   memory length: 95042   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 517   score: 0.0   memory length: 95165   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 518   score: 3.0   memory length: 95412   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 519   score: 3.0   memory length: 95677   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 520   score: 2.0   memory length: 95876   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 521   score: 0.0   memory length: 95999   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 522   score: 2.0   memory length: 96217   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 523   score: 1.0   memory length: 96389   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 524   score: 5.0   memory length: 96698   epsilon: 1.0    steps: 309    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 525   score: 2.0   memory length: 96896   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 526   score: 3.0   memory length: 97160   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 527   score: 0.0   memory length: 97283   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 528   score: 0.0   memory length: 97406   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 529   score: 8.0   memory length: 97720   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 530   score: 1.0   memory length: 97871   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 531   score: 1.0   memory length: 98039   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 532   score: 3.0   memory length: 98286   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 533   score: 1.0   memory length: 98436   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 534   score: 0.0   memory length: 98559   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 535   score: 2.0   memory length: 98774   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 536   score: 0.0   memory length: 98897   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 537   score: 1.0   memory length: 99048   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 538   score: 1.0   memory length: 99217   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 539   score: 2.0   memory length: 99416   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 540   score: 0.0   memory length: 99539   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 541   score: 0.0   memory length: 99662   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-291524992219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Start training after random sample generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_policy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;31m# Update the target network only for Double DQN only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdouble_dqn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mupdate_target_network_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/agent_double.py\u001b[0m in \u001b[0;36mtrain_policy_net\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Compute Q(s_t, a), the Q-value of the current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m### CODE ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mstate_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnon_terminal_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Compute Q function of next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: gather_out_cuda(): Expected dtype int64 for index"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q26qCm666wV"
      },
      "source": [
        "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04TJiIjZ6-4F"
      },
      "source": [
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Displaying the game live\n",
        "def show_state(env, step=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
        "    plt.axis('off')\n",
        "\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "    \n",
        "# Recording the game and replaying the game afterwards\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m9lADDH7H_V"
      },
      "source": [
        "display = Display(visible=0, size=(300, 200))\n",
        "display.start()\n",
        "\n",
        "# Load agent\n",
        "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
        "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
        "\n",
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "env = wrap_env(env)\n",
        "\n",
        "done = False\n",
        "score = 0\n",
        "step = 0\n",
        "state = env.reset()\n",
        "next_state = state\n",
        "life = number_lives\n",
        "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "get_init_state(history, state)\n",
        "\n",
        "while not done:\n",
        "    \n",
        "    # Render breakout\n",
        "    env.render()\n",
        "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
        "\n",
        "    step += 1\n",
        "    frame += 1\n",
        "\n",
        "    # Perform a fire action if ball is no longer on screen\n",
        "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "        action = 0\n",
        "    else:\n",
        "        action = torch.from_numpy(agent.get_action(torch.from_numpy(np.float32(history[:4, :, :]) / 255.)))\n",
        "    state = next_state\n",
        "    \n",
        "    next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "    frame_next_state = get_frame(next_state)\n",
        "    history[4, :, :] = frame_next_state\n",
        "    terminal_state = check_live(life, info['ale.lives'])\n",
        "        \n",
        "    life = info['ale.lives']\n",
        "    r = np.clip(reward, -1, 1) \n",
        "    r = reward\n",
        "\n",
        "    # Store the transition in memory \n",
        "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "    # Start training after random sample generation\n",
        "    score += reward\n",
        "    \n",
        "    history[:4, :, :] = history[1:, :, :]\n",
        "env.close()\n",
        "show_video()\n",
        "display.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}